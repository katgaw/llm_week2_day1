{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.11.4.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - 🚧 Activity #1: Augment RAG\n",
        "- Task 6: Visibility Tooling\n",
        "- Task 7: RAG Evaluation Using GPT-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/PvlaIUO.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VEzqziR6yt",
        "outputId": "f873dd3b-55a0-4e00-ecf4-e2a0fe3af327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/Users/katerina/Documents/LLM/.llm_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU numpy matplotlib plotly pandas scipy scikit-learn openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9OrFZRnER6yt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Comprehensive Overview of Large Language Models\n",
            "Humza Naveeda, Asad Ullah Khana,∗, Shi Qiub,∗, Muhammad Saqibc,d,∗, Saeed Anware,f, Muhammad Usmane,f, Naveed Akhtarg,i,\n",
            "Nick Barnesh, Ajmal Miani\n",
            "aUniversity of Engineering and Technology (UET), Lahore, Pakistan\n",
            "bThe Chinese University of Hong Kong (CUHK), HKSAR, China\n",
            "cUniversity of Technology Sydney (UTS), Sydney, Australia\n",
            "dCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\n",
            "eKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\n",
            "fSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\n",
            "gThe University of Melbourne (UoM), Melbourne, Australia\n",
            "hAustralian National University (ANU), Canberra, Australia\n",
            "iThe University of Western Australia (UWA), Perth, Australia\n",
            "Abstract\n",
            "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
            "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
            "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
            "robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\n",
            "LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\n",
            "the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\n",
            "yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature\n",
            "on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background\n",
            "concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only\n",
            "provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from\n",
            "extensive informative summaries of the existing works to advance the LLM research.\n",
            "Keywords:\n",
            "Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\n",
            "1. Introduction\n",
            "Language plays a fundamental role in facilitating commu-\n",
            "nication and self-expression for humans, and their interaction\n",
            "with machines. The need for generalized models stems from\n",
            "the growing demand for machines to handle complex language\n",
            "tasks, including translation, summarization, information re-\n",
            "trieval, conversational interactions, etc. Recently, significant\n",
            "breakthroughs have been witnessed in language models, pri-\n",
            "marily attributed to transformers [1], increased computational\n",
            "capabilities, and the availability of large-scale training data.\n",
            "These developments have brought about a revolutionary trans-\n",
            "formation by enabling the creation of LLMs that can approxi-\n",
            "mate human-level performance on various tasks [2, 3]. Large\n",
            "∗Equal contribution\n",
            "Email addresses: humza_naveed@yahoo.com (Humza Naveed),\n",
            "aukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\n",
            "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
            "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
            "muhammad.usman@kfupm.edu.sa (Muhammad Usman),\n",
            "naveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\n",
            "nick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\n",
            "(Ajmal Mian)\n",
            "Figure 1: The trend of papers released over years containing keywords \"Large\n",
            "Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large Lan-\n",
            "guage Model + Alignment\".\n",
            "Preprint submitted to Elsevier\n",
            "April 11, 2024\n",
            "arXiv:2307.06435v9  [cs.CL]  9 Apr 2024\n",
            "2019\n",
            "T5 (Oct)\n",
            "GPT-3 (May)\n",
            "WebGPT (Dec)\n",
            "OPT-IML\n",
            "TK-Instruct (May)\n",
            "mT0 (Dec)\n",
            "Wizard-LM\n",
            "Vicuna\n",
            "Alpaca (Mar)\n",
            "HuaTuo (Apr)\n",
            "Koala (May)\n",
            "Wizard-Coder (Jun)\n",
            "Goat\n",
            "PanGu-α (Apr)\n",
            "CPM-2 (Jun)\n",
            "GPT-NeoX-20B (Apr)\n",
            "CodeGen (Mar)\n",
            "Galactica (Nov)\n",
            "GLM (Oct)\n",
            "OPT\n",
            "UL2 (May)\n",
            "LLaMA (Feb)\n",
            "LLaMA 2 (Jul)\n",
            "MPT (Jun)\n",
            "CodeT5+\n",
            "Code Llama (Aug)\n",
            "StarCoder\n",
            "Xuan Yuan 2.0 (May)\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "mT5 (Oct)\n",
            "HyperCLOVA (Sep)\n",
            "ERNIE 3.0\n",
            "Codex (Jul)\n",
            "Jurassic-1 (Aug)\n",
            "Yuan 1.0 (Oct)\n",
            "Gopher (Dec)\n",
            "ERNIE 3.0 Titan\n",
            "GLaM\n",
            "LaMDA\n",
            "T0 (Oct)\n",
            "ChatGPT (Nov)\n",
            "Sparrow (Sep)\n",
            "FLAN-U-PaLM (Oct)\n",
            "Bard (Oct)\n",
            "MT-NLG (Jan)\n",
            "AlphaCode (Feb)\n",
            "Chinchilla (Mar)\n",
            "PaLM (Apr)\n",
            "U-PALM (Oct)\n",
            "BLOOM (Nov)\n",
            "AlexaTM (Aug)\n",
            "PaLM2 (May)\n",
            "GPT-4\n",
            "PanGu-Σ (Mar)\n",
            "BloombergGPT\n",
            "Claude\n",
            "Gemini (Dec)\n",
            "Figure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\n",
            "on the upper half signify open-source availability, whereas those on the bottom half are closed-source. The chart illustrates the increasing trend towards instruction-\n",
            "tuned models and open-source models, highlighting the evolving landscape and trends in natural language processing research.\n",
            "Language Models (LLMs) have emerged as cutting-edge arti-\n",
            "ficial intelligence systems that can process and generate text\n",
            "with coherent communication [4], and generalize to multiple\n",
            "tasks [5, 6].\n",
            "The historical progress in natural language processing (NLP)\n",
            "evolved from statistical to neural language modeling and then\n",
            "from pre-trained language models (PLMs) to LLMs.\n",
            "While\n",
            "conventional language modeling (LM) trains task-specific mod-\n",
            "els in supervised settings, PLMs are trained in a self-supervised\n",
            "setting on a large corpus of text [7, 8, 9] with the aim of learning\n",
            "a generic representation that is shareable among various NLP\n",
            "tasks. After fine-tuning for downstream tasks, PLMs surpass\n",
            "the performance gains of traditional language modeling (LM).\n",
            "The larger PLMs bring more performance gains, which has led\n",
            "to the transitioning of PLMs to LLMs by significantly increas-\n",
            "ing model parameters (tens to hundreds of billions) [10] and\n",
            "training dataset (many GBs and TBs) [10, 11]. Following this\n",
            "development, numerous LLMs have been proposed in the lit-\n",
            "erature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\n",
            "number of released LLMs and names of a few significant LLMs\n",
            "proposed over the years are shown in Fig 1 and Fig 2, respec-\n",
            "tively.\n",
            "The early work on LLMs, such as T5 [10] and mT5 [11] em-\n",
            "ployed transfer learning until GPT-3 [6] showed LLMs are\n",
            "zero-shot transferable to downstream tasks without fine-tuning.\n",
            "LLMs accurately respond to task queries when prompted with\n",
            "task descriptions and examples. However, pre-trained LLMs\n",
            "fail to follow user intent and perform worse in zero-shot set-\n",
            "tings than in few-shot.\n",
            "Fine-tuning them with task instruc-\n",
            "tions data [16, 17, 18, 19] and aligning with human prefer-\n",
            "ences [20, 21] enhances generalization to unseen tasks, im-\n",
            "proving zero-shot performance significantly and reducing mis-\n",
            "aligned behavior.\n",
            "In addition to better generalization and domain adaptation,\n",
            "LLMs appear to have emergent abilities, such as reasoning,\n",
            "planning, decision-making, in-context learning, answering in\n",
            "zero-shot settings, etc.\n",
            "These abilities are known to be ac-\n",
            "quired by them due to their gigantic scale even when the pre-\n",
            "trained LLMs are not trained specifically to possess these at-\n",
            "tributes [22, 23, 24]. Such abilities have led LLMs to be widely\n",
            "adopted in diverse settings including, multi-modal, robotics,\n",
            "tool manipulation, question answering, autonomous agents, etc.\n",
            "Various improvements have also been suggested in these areas\n",
            "either by task-specific training [25, 26, 27, 28, 29, 30, 31] or\n",
            "better prompting [32].\n",
            "The LLMs abilities to solve diverse tasks with human-level\n",
            "performance come at a cost of slow training and inference,\n",
            "extensive hardware requirements, and higher running costs.\n",
            "Such requirements have limited their adoption and opened up\n",
            "opportunities to devise better architectures [15, 33, 34, 35]\n",
            "and training strategies [36, 37, 21, 38, 39, 40, 41].\n",
            "Param-\n",
            "eter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-\n",
            "tion [44, 45], knowledge distillation, and context length inter-\n",
            "polation [46, 47, 48, 49] among others are some of the methods\n",
            "widely studied for efficient LLM utilization.\n",
            "Due to the success of LLMs on a wide variety of tasks, the\n",
            "research literature has recently experienced a large influx of\n",
            "LLM-related contributions.\n",
            "Researchers have organized the\n",
            "LLMs literature in surveys [50, 51, 52, 53], and topic-specific\n",
            "surveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\n",
            "contribution focuses on providing a comprehensive yet concise\n",
            "overview of the general direction of LLM research. This arti-\n",
            "cle summarizes architectural and training details of pre-trained\n",
            "LLMs and delves deeper into the details of concepts like fine-\n",
            "tuning, multi-modal LLMs, augmented LLMs, datasets, eval-\n",
            "uation, applications, challenges, and others to provide a self-\n",
            "contained comprehensive overview. Our key contributions are\n",
            "summarized as follows.\n",
            "• We present a survey on the developments in LLM research\n",
            "providing a concise comprehensive overview of the direc-\n",
            "tion.\n",
            "• We present extensive summaries of pre-trained models that\n",
            "include fine-grained details of architecture and training de-\n",
            "tails.\n",
            "• We summarize major findings of the popular contributions\n",
            "and provide a detailed discussion on the key design and\n",
            "development aspects of LLMs to help practitioners effec-\n",
            "tively leverage this technology.\n",
            "• In this self-contained article, we cover a range of con-\n",
            "cepts to present the general direction of LLMs compre-\n",
            "2\n",
            "Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\n",
            "7. Challenges\n",
            "hensively, including background, pre-training, fine-tuning,\n",
            "multi-modal LLMs, augmented LLMs, LLMs-powered\n",
            "agents, datasets, evaluation, etc.\n",
            "We loosely follow the existing terminology to ensure a stan-\n",
            "dardized outlook of this research direction. For instance, fol-\n",
            "lowing [50], our survey discusses pre-trained LLMs with 10B\n",
            "parameters or more. We refer the readers interested in smaller\n",
            "pre-trained models to [51, 52, 53].\n",
            "The organization of this paper is as follows. Section 2 discusses\n",
            "the background of LLMs. Section 3 focuses on LLMs overview,\n",
            "architectures, training pipelines and strategies, fine-tuning, and\n",
            "utilization in different domains. Section 4 highlights the config-\n",
            "uration and parameters that play a crucial role in the function-\n",
            "ing of these models. Summary and discussions are presented\n",
            "in section 3.8. The LLM training and evaluation, datasets, and\n",
            "benchmarks are discussed in section 5, followed by challenges\n",
            "and future directions, and conclusion in sections 7 and 8, re-\n",
            "spectively.\n",
            "3\n",
            "2. Background\n",
            "We provide the relevant background to understand the fun-\n",
            "damentals related to LLMs in this section. We briefly discuss\n",
            "necessary components in LLMs and refer the readers interested\n",
            "in details to the original works.\n",
            "2.1. Tokenization\n",
            "Tokenization [59] is an essential pre-processing step in\n",
            "LLM training that parses the text into non-decomposing units\n",
            "called tokens. Tokens can be characters, subwords [60], sym-\n",
            "bols [61], or words, depending on the tokenization process.\n",
            "Some of the commonly used tokenization schemes in LLMs\n",
            "include wordpiece [62], byte pair encoding (BPE) [61], and un-\n",
            "igramLM [60]. Readers are encouraged to refer to [63] for a\n",
            "detailed survey.\n",
            "2.2. Encoding Positions\n",
            "The transformer processes input sequences in parallel and\n",
            "independently of each other.\n",
            "Moreover, the attention mod-\n",
            "ule in the transformer does not capture positional information.\n",
            "As a result, positional encodings were introduced in trans-\n",
            "former [64], where a positional embedding vector is added to\n",
            "the token embedding. Variants of positional embedding include\n",
            "absolute, relative, or learned positional encodings. Within rel-\n",
            "ative encoding, Alibi and RoPE are two widely used positional\n",
            "embeddings in LLMs.\n",
            "Alibi [65]: It subtracts a scalar bias from the attention score\n",
            "that increases with the distance between token positions. This\n",
            "favors using recent tokens for attention.\n",
            "RoPE [66]: It rotates query and key representations at an an-\n",
            "gle proportional to the token absolute position in the input\n",
            "sequence, resulting in a relative positional encoding scheme\n",
            "which decays with the distance between the tokens.\n",
            "2.3. Attention in LLMs\n",
            "Attention assigns weights to input tokens based on impor-\n",
            "tance so that the model gives more emphasis to relevant tokens.\n",
            "Attention in transformers [64] calculates query, key, and value\n",
            "mappings for input sequences, where the attention score is\n",
            "obtained by multiplying the query and key, and later used to\n",
            "weight values. We discuss different attention strategies used in\n",
            "LLMs below.\n",
            "Self-Attention [64]: Calculates attention using queries, keys,\n",
            "and values from the same block (encoder or decoder).\n",
            "Cross Attention: It is used in encoder-decoder architectures,\n",
            "where encoder outputs are the queries, and key-value pairs\n",
            "come from the decoder.\n",
            "Sparse Attention [67]: Self-attention has O(n2) time complex-\n",
            "ity which becomes infeasible for large sequences. To speed\n",
            "up the computation, sparse attention [67] iteratively calculates\n",
            "attention in sliding windows for speed gains.\n",
            "Flash Attention [68]: Memory access is the major bottleneck\n",
            "in calculating attention using GPUs.\n",
            "To speed up, flash\n",
            "attention employs input tiling to minimize the memory reads\n",
            "and writes between the GPU high bandwidth memory (HBM)\n",
            "and the on-chip SRAM.\n",
            "2.4. Activation Functions\n",
            "The activation functions serve a crucial role in the curve-\n",
            "fitting abilities of neural networks [69]. We discuss activation\n",
            "functions used in LLMs in this section.\n",
            "ReLU [70]: The Rectified linear unit (ReLU) is defined as:\n",
            "ReLU(x) = max(0, x)\n",
            "(1)\n",
            "GeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\n",
            "combination of ReLU, dropout [72] and zoneout [73].\n",
            "GLU variants [74]: The Gated Linear Unit [75] is a neural\n",
            "network layer that is an element-wise product (⊗) of a linear\n",
            "transformation and a sigmoid transformed (σ) linear projection\n",
            "of the input given as:\n",
            "GLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),\n",
            "(2)\n",
            "where X is the input of layer and l, W, b, V and c are learned\n",
            "parameters. Other GLU variants [74] used in LLMs are:\n",
            "ReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\n",
            "GEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\n",
            "S wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).\n",
            "2.5. Layer Normalization\n",
            "Layer normalization leads to faster convergence and is an in-\n",
            "tegrated component of transformers [64]. In addition to Layer-\n",
            "Norm [76] and RMSNorm [77], LLMs use pre-layer normal-\n",
            "ization [78], applying it before multi-head attention (MHA).\n",
            "Pre-norm is shown to provide training stability in LLMs. An-\n",
            "other normalization variant, DeepNorm [79] fixes the issue with\n",
            "larger gradients in pre-norm.\n",
            "2.6. Distributed LLM Training\n",
            "This section describes distributed LLM training approaches\n",
            "briefly. More details are available in [13, 37, 80, 81].\n",
            "Data Parallelism: Data parallelism replicates the model on\n",
            "multiple devices where data in a batch gets divided across de-\n",
            "vices. At the end of each training iteration weights are synchro-\n",
            "nized across all devices.\n",
            "Tensor Parallelism: Tensor parallelism shards a tensor compu-\n",
            "tation across devices. It is also known as horizontal parallelism\n",
            "or intra-layer model parallelism.\n",
            "Pipeline Parallelism: Pipeline parallelism shards model layers\n",
            "across different devices. This is also known as vertical paral-\n",
            "lelism.\n",
            "Model Parallelism: A combination of tensor and pipeline par-\n",
            "allelism is known as model parallelism.\n",
            "3D Parallelism: A combination of data, tensor, and model par-\n",
            "allelism is known as 3D parallelism.\n",
            "Optimizer Parallelism: Optimizer parallelism also known as\n",
            "zero redundancy optimizer [37] implements optimizer state\n",
            "partitioning, gradient partitioning, and parameter partitioning\n",
            "across devices to reduce memory consumption while keeping\n",
            "the communication costs as low as possible.\n",
            "4\n",
            "2.7. Libraries\n",
            "Some commonly used libraries for LLMs training are:\n",
            "Transformers [82]: The library provides access to various pre-\n",
            "trained transformer models with APIs to train, fine-tune, infer,\n",
            "and develop custom models.\n",
            "DeepSpeed [36]: A library for scalable distributed training and\n",
            "inference of deep learning models.\n",
            "Megatron-LM [80]: It provides GPU-optimized techniques for\n",
            "large-scale training of LLMs.\n",
            "JAX [83]: A Python library for high-performance numerical\n",
            "computing and scaleable machine learning. It can differenti-\n",
            "ate native Python and NumPy functions and execute them on\n",
            "GPUs.\n",
            "Colossal-AI [84]: A collection of components to write dis-\n",
            "tributed deep learning models.\n",
            "BMTrain [81]: A library to write efficient stand-alone LLMs\n",
            "training code.\n",
            "FastMoE [85]:\n",
            "Provides API to build mixture-of-experts\n",
            "(MoE) model in PyTorch.\n",
            "MindSpore [86]: A deep learning training and inference frame-\n",
            "work extendable to mobile, edge, and cloud computing.\n",
            "PyTorch [87]: A framework developed by Facebook AI Re-\n",
            "search lab (FAIR) to build deep learning models. The main\n",
            "features of PyTorch include a dynamic computation graph and\n",
            "a pythonic coding style.\n",
            "Tensorflow [88]:\n",
            "A deep learning framework written by\n",
            "Google. The key features of TensorFlow are graph-based com-\n",
            "putation, eager execution, scalability, etc.\n",
            "MXNet [89]: Apache MXNet is a deep learning framework\n",
            "with support to write programs in multiple languages, includ-\n",
            "ing, Python, C++, Scala, R, etc. It also provides support for\n",
            "dynamic and static computation graphs.\n",
            "2.8. Data PreProcessing\n",
            "This section briefly summarizes data preprocessing tech-\n",
            "niques used in LLMs training.\n",
            "Quality Filtering: For better results, training data quality is\n",
            "essential. Some approaches to filtering data are: 1) classifier-\n",
            "based and 2) heuristics-based.\n",
            "Classifier-based approaches\n",
            "train a classifier on high-quality data and predict the quality of\n",
            "text for filtering, whereas heuristics-based employ some rules\n",
            "for filtering like language, metrics, statistics, and keywords.\n",
            "Data Deduplication: Duplicated data can affect model per-\n",
            "formance and increase data memorization; therefore, to train\n",
            "LLMs, data deduplication is one of the preprocessing steps.\n",
            "This can be performed at multiple levels, like sentences,\n",
            "documents, and datasets.\n",
            "Privacy Reduction: Most of the training data for LLMs is\n",
            "collected through web sources.\n",
            "This data contains private\n",
            "information; therefore, many LLMs employ heuristics-based\n",
            "methods to filter information such as names, addresses, and\n",
            "phone numbers to avoid learning personal information.\n",
            "2.9. Architectures\n",
            "Here we discuss the variants of the transformer architectures\n",
            "used in LLMs. The difference arises due to the application of\n",
            "Figure 4: An example of attention patterns in language models, image is taken\n",
            "from [93].\n",
            "Figure 5: An example of language model training objectives, image from [93].\n",
            "the attention and the connection of transformer blocks. An il-\n",
            "lustration of attention patterns of these architectures is shown\n",
            "in Figure 4.\n",
            "Encoder Decoder: This architecture processes inputs through\n",
            "the encoder and passes the intermediate representation to the\n",
            "decoder to generate the output.\n",
            "Here, the encoder sees the\n",
            "complete sequence utilizing self-attention whereas the decoder\n",
            "processes the sequence one after the other with implementing\n",
            "cross-attention.\n",
            "Causal Decoder: A type of architecture that does not have an\n",
            "encoder and processes and generates output using a decoder,\n",
            "where the predicted token depends only on the previous time\n",
            "steps.\n",
            "Prefix Decoder: It is also known as a non-causal decoder,\n",
            "where the attention calculation is not strictly dependent on the\n",
            "past information and the attention is bidirectional. An example\n",
            "of a non-causal attention mask is shown in Figure 4.\n",
            "Mixture-of-Experts: It is a variant of transformer architecture\n",
            "with parallel independent experts and a router to route tokens\n",
            "to experts. These experts are feed-forward layers after the at-\n",
            "tention block [90]. Mixture-of-Experts (MoE) is an efficient\n",
            "sparse architecture that offers comparable performance to dense\n",
            "models and allows increasing the model size without increas-\n",
            "ing the computational cost by activating only a few experts at a\n",
            "time [91, 92].\n",
            "2.10. Pre-Training Objectives\n",
            "This section describes LLMs pre-training objectives.\n",
            "For\n",
            "more details see the paper [93].\n",
            "Full Language Modeling: An autoregressive language model-\n",
            "ing objective where the model is asked to predict future tokens\n",
            "given the previous tokens, an example is shown in Figure 5.\n",
            "Prefix Language Modeling: A non-causal training objective,\n",
            "where a prefix is chosen randomly and only remaining target\n",
            "tokens are used to calculate the loss. An example is shown in\n",
            "Figure 5.\n",
            "5\n",
            "Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\n",
            "different training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\n",
            "“RLHF” represents reinforcement learning with human feedback.\n",
            "Masked Language Modeling: In this training objective, tokens\n",
            "or spans (a sequence of tokens) are masked randomly and the\n",
            "model is asked to predict masked tokens given the past and\n",
            "future context. An example is shown in Figure 5.\n",
            "Unified Language Modeling: Unified language modeling [94]\n",
            "is a combination of causal, non-causal, and masked language\n",
            "training objectives. Here in masked language modeling, the\n",
            "attention is not bidirectional but unidirectional, attending either\n",
            "left-to-right or right-to-left context.\n",
            "2.11. LLMs Scaling Laws\n",
            "Scaling laws study the optimal combination of model param-\n",
            "eters, dataset size, and computational resources that predict the\n",
            "improvement in the model performance. It has been shown\n",
            "that the loss scales according to the power-law with model size,\n",
            "dataset size, and compute resources [95]. This study suggests\n",
            "larger models are more important than big data for better perfor-\n",
            "mance. Another variant of scaling law [96] suggests the model\n",
            "size and the number of training tokens should be scaled equally.\n",
            "2.12. LLMs Adaptation Stages\n",
            "This section discusses the fundamentals of LLMs adaptation\n",
            "stages, from pre-training to fine-tuning for downstream tasks\n",
            "and utilization. An example of different training stages and in-\n",
            "ference in LLMs is shown in Figure 6. In this paper, we refer\n",
            "to alignment-tuning as aligning with human preferences, while\n",
            "occasionally the literature uses the term alignment for different\n",
            "purposes.\n",
            "2.12.1. Pre-Training\n",
            "In the very first stage, the model is trained in a self-\n",
            "supervised manner on a large corpus to predict the next to-\n",
            "kens given the input. The design choices of LLMs vary from\n",
            "encoder-decoder to decoder-only architectures with different\n",
            "building blocks and loss functions in sections 2.5, 2.4, 2.10.\n",
            "2.12.2. Fine-Tuning\n",
            "There are different styles to fine-tune an LLM. This section\n",
            "briefly discusses fine-tuning approaches.\n",
            "Transfer Learning: The pre-trained LLMs perform well for\n",
            "various tasks [6, 15]. However, to improve the performance for\n",
            "6\n",
            "a downstream task, pre-trained models are fine-tuned with the\n",
            "task-specific data [10, 11], known as transfer learning.\n",
            "Instruction-tuning: To enable a model to respond to user\n",
            "queries effectively, the pre-trained model is fine-tuned on in-\n",
            "struction formatted data i.e., instruction and an input-output\n",
            "pair. Instructions generally comprise multi-task data in plain\n",
            "natural language, guiding the model to respond according to the\n",
            "prompt and the input. This type of fine-tuning improves zero-\n",
            "shot generalization and downstream task performance. Details\n",
            "on formatting instruction data and its various styles are avail-\n",
            "able in [16, 50, 97].\n",
            "Alignment-tuning: LLMs are prone to generating false, biased,\n",
            "and harmful text. To make them helpful, honest, and harmless,\n",
            "models are aligned using human feedback. Alignment involves\n",
            "asking LLMs to generate unexpected responses and then updat-\n",
            "ing their parameters to avoid such responses [20, 21, 98].\n",
            "It ensures LLMs operate according to human intentions and\n",
            "values. A model is defined to be an “aligned” model if the\n",
            "model fulfills three criteria of helpful, honest, and harmless or\n",
            "“HHH” [99].\n",
            "Researchers employ reinforcement learning with human feed-\n",
            "back (RLHF) [100] for model alignment. In RLHF, a fine-tuned\n",
            "model on demonstrations is further trained with reward model-\n",
            "ing (RM) and reinforcement learning (RL), shown in Figure 6.\n",
            "Below we briefly discuss RM and RL pipelines in RLHF.\n",
            "Reward modeling: trains a model to rank generated responses\n",
            "according to human preferences using a classification objec-\n",
            "tive. To train the classifier humans annotate LLMs generated\n",
            "responses based on the HHH criteria.\n",
            "Reinforcement learning: in combination with the reward model\n",
            "is used for alignment in the next stage. The previously trained\n",
            "reward model ranks LLM-generated responses into preferred\n",
            "vs. non-preferred, which is used to align the model with proxi-\n",
            "mal policy optimization (PPO). This process repeats iteratively\n",
            "until convergence.\n",
            "2.12.3. Prompting/Utilization\n",
            "Prompting is a method to query trained LLMs for generating\n",
            "responses, as illustrated in Figure 6. LLMs can be prompted in\n",
            "various prompt setups, where they can be adapted to the instruc-\n",
            "tions without fine-tuning and in other cases with fine-tuning on\n",
            "data containing different prompt styles [16, 101, 102]. A good\n",
            "guide on prompt engineering is available at [32]. Below, we\n",
            "will discuss various widely used prompt setups.\n",
            "Zero-Shot Prompting: LLMs are zero-shot learners and ca-\n",
            "pable of answering queries never seen before. This style of\n",
            "prompting requires LLMs to answer user questions without see-\n",
            "ing any examples in the prompt.\n",
            "In-context Learning: Also known as few-shot learning, here,\n",
            "multiple input-output demonstration pairs are shown to the\n",
            "model to generate the desired response. This adaptation style\n",
            "is also called few-shot learning. A discussion on formatting in-\n",
            "context learning (ICL) templates is available in [54, 50, 18, 16].\n",
            "Reasoning in LLMs: LLMs are zero-shot reasoners and can\n",
            "be provoked to generate answers to logical problems, task\n",
            "planning, critical thinking, etc. with reasoning.\n",
            "Generating\n",
            "reasons is possible only by using different prompting styles,\n",
            "whereas to improve LLMs further on reasoning tasks many\n",
            "methods [16, 97] train them on reasoning datasets. We discuss\n",
            "various prompting techniques for reasoning below.\n",
            "Chain-of-Thought (CoT): A special case of prompting where\n",
            "demonstrations contain reasoning information aggregated with\n",
            "inputs and outputs so that the model generates outcomes with\n",
            "step-by-step reasoning. More details on CoT prompts are avail-\n",
            "able in [55, 103, 101].\n",
            "Self-Consistency:\n",
            "Improves CoT performance by generat-\n",
            "ing multiple responses and selecting the most frequent an-\n",
            "swer [104].\n",
            "Tree-of-Thought (ToT): Explores multiple reasoning paths\n",
            "with possibilities to look ahead and backtrack for problem-\n",
            "solving [105].\n",
            "Single-Turn Instructions: In this prompting setup, LLMs are\n",
            "queried only once with all the relevant information in the\n",
            "prompt. LLMs generate responses by understanding the con-\n",
            "text either in a zero-shot or few-shot setting.\n",
            "Multi-Turn Instructions: Solving a complex task requires mul-\n",
            "tiple interactions with LLMs, where feedback and responses\n",
            "from the other tools are given as input to the LLM for the next\n",
            "rounds. This style of using LLMs in the loop is common in\n",
            "autonomous agents.\n",
            "3. Large Language Models\n",
            "This section reviews LLMs, briefly describing their architec-\n",
            "tures, training objectives, pipelines, datasets, and fine-tuning\n",
            "details.\n",
            "3.1. Pre-Trained LLMs\n",
            "Here, we provide summaries of various well-known pre-\n",
            "trained LLMs with significant discoveries, changing the course\n",
            "of research and development in NLP. These LLMs have consid-\n",
            "erably improved the performance in NLU and NLG domains,\n",
            "and are widely fine-tuned for downstream tasks. Moreover, We\n",
            "also identify key findings and insights of pre-trained LLMs in\n",
            "Table 1 and 2 that improve their performance.\n",
            "3.1.1. General Purpose\n",
            "T5 [10]: An encoder-decoder model employing a unified text-\n",
            "to-text training for all NLP problems is shown in Figure 7. T5\n",
            "places layer normalization outside the residual path in a conven-\n",
            "tional transformer model [64]. It uses masked language mod-\n",
            "eling as a pre-training objective where spans (consecutive to-\n",
            "kens) are replaced with a single mask instead of separate masks\n",
            "for each token. This type of masking speeds up the training as\n",
            "it produces shorter sequences. After pre-training, the model is\n",
            "fine-tuned using adapter layers [106] for downstream tasks.\n",
            "GPT-3 [6]: The GPT-3 architecture is the same as the GPT-\n",
            "2 [5] but with dense and sparse attention in transformer layers\n",
            "similar to the Sparse Transformer [67]. It shows that large mod-\n",
            "els can train on larger batch sizes with a lower learning rate to\n",
            "decide the batch size during training, GPT-3 uses the gradient\n",
            "noise scale as in [107]. Overall, GPT-3 increases model param-\n",
            "eters to 175B showing that the performance of large language\n",
            "7\n",
            "Figure 7: Unified text-to-text training example, source image from [10].\n",
            "Figure 8: The image is the article of [108], showing an example of PanGu-α\n",
            "architecture.\n",
            "models improves with the scale and is competitive with the fine-\n",
            "tuned models.\n",
            "mT5 [11]: A multilingual T5 model [10] trained on the mC4\n",
            "dataset with 101 languages. The dataset is extracted from the\n",
            "public common crawl scrape. The model uses a larger vocab-\n",
            "ulary size of 250,000 to cover multiple languages. To avoid\n",
            "over-fitting or under-fitting for a language, mT5 employs a data\n",
            "sampling procedure to select samples from all languages. The\n",
            "paper suggests using a small amount of pre-training datasets,\n",
            "including all languages when fine-tuning for a task using En-\n",
            "glish language data. This allows the model to generate correct\n",
            "non-English outputs.\n",
            "PanGu-α [108]: An autoregressive model that has a query\n",
            "layer at the end of standard transformer layers, example shown\n",
            "in Figure 8, to predict the next token. Its structure is similar to\n",
            "the transformer layer but with an additional embedding for the\n",
            "next position in the attention mechanism, given in Eq. 3.\n",
            "a = pnWq\n",
            "hWk\n",
            "hTHT\n",
            "L\n",
            "(3)\n",
            "CPM-2 [12]: Cost-efficient Pre-trained language Models\n",
            "(CPM-2) pre-trains bilingual (English and Chinese) 11B and\n",
            "198B mixture-of-experts (MoE) models on the WuDaoCor-\n",
            "pus [109] dataset. The tokenization process removes “_” white\n",
            "space tokens in the sentencepiece tokenizer. The models are\n",
            "trained with knowledge inheritance, starting with only the Chi-\n",
            "nese language in the first stage and then adding English and\n",
            "Chinese data. This trained model gets duplicated multiple times\n",
            "to initialize the 198B MoE model. Moreover, to use the model\n",
            "for downstream tasks, CPM-2 experimented with both com-\n",
            "plete fine-tuning and prompt fine-tuning as in [40] where only\n",
            "prompt-related parameters are updated by inserting prompts at\n",
            "various positions, front, middle, and back. CPM-2 also pro-\n",
            "poses the INFMOE, a memory-efficient framework with a strat-\n",
            "egy to dynamically offload parameters to the CPU for inference\n",
            "at a 100B scale. It overlaps data movement with inference com-\n",
            "putation for lower inference time.\n",
            "ERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\n",
            "task learning to build a modular architecture using Transformer-\n",
            "XL [111] as the backbone. The universal representation mod-\n",
            "ule is shared by all the tasks, which serve as the basic block\n",
            "for task-specific representation modules, which are all trained\n",
            "jointly for natural language understanding, natural language\n",
            "generation, and knowledge extraction. This LLM is primar-\n",
            "ily focused on the Chinese language. It claims to train on the\n",
            "largest Chinese text corpora for LLM training, and achieved\n",
            "state-of-the-art in 54 Chinese NLP tasks.\n",
            "Jurassic-1 [112]: A pair of auto-regressive language mod-\n",
            "els, including a 7B-parameter J1-Large model and a 178B-\n",
            "parameter J1-Jumbo model.\n",
            "The training vocabulary of\n",
            "Jurassic-1 comprise word pieces, complete words, and multi-\n",
            "word expressions without any word boundaries, where possible\n",
            "out-of-vocabulary instances are interpreted as Unicode bytes.\n",
            "Compared to the GPT-3 counterparts, the Jurassic-1 models\n",
            "apply a more balanced depth-to-width self-attention architec-\n",
            "ture [113] and an improved tokenizer for a faster prediction\n",
            "based on broader resources, achieving a comparable perfor-\n",
            "mance in zero-shot learning tasks and a superior performance in\n",
            "few-shot learning tasks given the ability to feed more examples\n",
            "as a prompt.\n",
            "HyperCLOVA [114]: A Korean language model with GPT-3\n",
            "architecture.\n",
            "Yuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\n",
            "high-quality text collected from the Internet. A Massive Data\n",
            "Filtering System (MDFS) built on Spark is developed to pro-\n",
            "cess the raw data via coarse and fine filtering techniques. To\n",
            "speed up the training of Yuan 1.0 to save energy expenses and\n",
            "carbon emissions, various factors that improve the performance\n",
            "of distributed training are incorporated in architecture and train-\n",
            "ing: like increasing the hidden state size improves pipeline and\n",
            "tensor parallelism performance, larger micro batches improve\n",
            "pipeline parallelism performance, and larger global batch size\n",
            "improve data parallelism performance. In practice, the Yuan 1.0\n",
            "model performs well on text classification, Winograd Schema,\n",
            "natural language inference, and reading comprehension tasks.\n",
            "Gopher [116]: The Gopher family of models ranges from\n",
            "44M to 280B parameters in size to study the effect of scale\n",
            "on the LLMs performance. The 280B model beats GPT-3 [6],\n",
            "Jurrasic-1 [112], MT-NLG [117], and others on 81% of the\n",
            "evaluated tasks.\n",
            "ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\n",
            "by training a larger model with 26x the number of parameters\n",
            "of the latter. This bigger model outperformed other state-of-the-\n",
            "art models in 68 NLP tasks. LLMs produce text with incorrect\n",
            "facts. In order to have control of the generated text with fac-\n",
            "tual consistency, ERNIE 3.0 Titan adds another task, Credible\n",
            "and Controllable Generations, to its multi-task learning setup.\n",
            "8\n",
            "It introduces additional self-supervised adversarial and control-\n",
            "lable language modeling losses to the pre-training step, which\n",
            "enables ERNIE 3.0 Titan to beat other LLMs in their manually\n",
            "selected Factual QA task set evaluations.\n",
            "GPT-NeoX-20B [118]: An auto-regressive model that largely\n",
            "follows GPT-3 with a few deviations in architecture design,\n",
            "trained on the Pile dataset without any data deduplication. GPT-\n",
            "NeoX has parallel attention and feed-forward layers in a trans-\n",
            "former block, given in Eq. 4, that increases throughput by 15%.\n",
            "It uses rotary positional embedding [66], applying it to only\n",
            "25% of embedding vector dimension as in [119]. This reduces\n",
            "the computation without performance degradation. As opposed\n",
            "to GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\n",
            "uses only dense layers. The hyperparameter tuning at this scale\n",
            "is difficult; therefore, the model chooses hyperparameters from\n",
            "the method [6] and interpolates values between 13B and 175B\n",
            "models for the 20B model. The model training is distributed\n",
            "among GPUs using both tensor and pipeline parallelism.\n",
            "x + Attn(LN1(x)) + FF(LN2(x))\n",
            "(4)\n",
            "OPT [14]: It is a clone of GPT-3, developed to open-source\n",
            "a model that replicates GPT-3 performance. Training of OPT\n",
            "employs dynamic loss scaling [120] and restarts from an earlier\n",
            "checkpoint with a lower learning rate whenever loss divergence\n",
            "is observed. Overall, the performance of OPT-175B models is\n",
            "comparable to the GPT3-175B model.\n",
            "BLOOM [13]: A causal decoder model trained on the ROOTS\n",
            "corpus to open-source an LLM. The architecture of BLOOM is\n",
            "shown in Figure 9, with differences like ALiBi positional em-\n",
            "bedding, an additional normalization layer after the embedding\n",
            "layer as suggested by the bitsandbytes1 library. These changes\n",
            "stabilize training with improved downstream performance.\n",
            "GLaM [91]: Generalist Language Model (GLaM) represents a\n",
            "family of language models using a sparsely activated decoder-\n",
            "only mixture-of-experts (MoE) structure [121, 90].\n",
            "To gain\n",
            "more model capacity while reducing computation, the experts\n",
            "are sparsely activated where only the best two experts are used\n",
            "to process each input token. The largest GLaM model, GLaM\n",
            "(64B/64E), is about 7× larger than GPT-3 [6], while only part of\n",
            "the parameters are activated per input token. The largest GLaM\n",
            "(64B/64E) model achieves better overall results as compared\n",
            "to GPT-3 while consuming only one-third of GPT-3’s training\n",
            "energy.\n",
            "MT-NLG [117]: A 530B causal decoder based on the GPT-\n",
            "2 architecture that has roughly 3× GPT-3 model parameters.\n",
            "MT-NLG is trained on filtered high-quality data collected from\n",
            "various public datasets and blends various types of datasets in a\n",
            "single batch, which beats GPT-3 on several evaluations.\n",
            "Chinchilla [96]: A causal decoder trained on the same dataset\n",
            "as the Gopher [116] but with a little different data sampling\n",
            "distribution (sampled from MassiveText). The model architec-\n",
            "ture is similar to the one used for Gopher, with the exception of\n",
            "AdamW optimizer instead of Adam. Chinchilla identifies the\n",
            "1https://github.com/TimDettmers/bitsandbytes\n",
            "Figure 9: The BLOOM architecture example sourced from [13].\n",
            "relationship that model size should be doubled for every dou-\n",
            "bling of training tokens. Over 400 language models ranging\n",
            "from 70 million to over 16 billion parameters on 5 to 500 bil-\n",
            "lion tokens are trained to get the estimates for compute-optimal\n",
            "training under a given budget. The authors train a 70B model\n",
            "with the same compute budget as Gopher (280B) but with 4\n",
            "times more data. It outperforms Gopher [116], GPT-3 [6], and\n",
            "others on various downstream tasks, after fine-tuning.\n",
            "AlexaTM [122]: An encoder-decoder model, where encoder\n",
            "weights and decoder embeddings are initialized with a pre-\n",
            "trained encoder to speed up training. The encoder stays frozen\n",
            "for the initial 100k steps and is later unfrozen for end-to-end\n",
            "training. The model is trained on a combination of denoising\n",
            "and causal language modeling (CLM) objectives, concatenat-\n",
            "ing a [CLM] token at the beginning for mode switching. Dur-\n",
            "ing training, the CLM task is applied for 20% of the time, which\n",
            "improves the in-context learning performance.\n",
            "PaLM [15]: A causal decoder with parallel attention and\n",
            "feed-forward layers similar to Eq. 4, speeding up training by\n",
            "a factor of 15. Additional changes to the conventional trans-\n",
            "former model include SwiGLU activation, RoPE embeddings,\n",
            "multi-query attention that saves computation cost during decod-\n",
            "ing, and shared input-output embeddings. During training, loss\n",
            "spiking was observed, and to fix it, model training was restarted\n",
            "from a 100-step earlier checkpoint by skipping 200-500 batches\n",
            "around the spike. Moreover, the model was found to memo-\n",
            "rize around 2.4% of the training data at the 540B model scale,\n",
            "whereas this number was lower for smaller models.\n",
            "PaLM-2 [123]: A smaller multi-lingual variant of PaLM,\n",
            "trained for larger iterations on a better quality dataset. PaLM-\n",
            "2 shows significant improvements over PaLM, while reducing\n",
            "training and inference costs due to its smaller size. To lessen\n",
            "toxicity and memorization, it appends special tokens with a\n",
            "fraction of pre-training data, which shows a reduction in gener-\n",
            "ating harmful responses.\n",
            "U-PaLM [124]: This method trains PaLM for 0.1% addi-\n",
            "tional compute with the UL2 (also named as UL2Restore) ob-\n",
            "jective [125], using the same dataset it outperforms the baseline\n",
            "significantly on various NLP tasks, including zero-shot, few-\n",
            "shot, commonsense reasoning, CoT, etc. Training with UL2R\n",
            "involves converting a causal decoder PaLM to a non-causal de-\n",
            "coder PaLM and employing 50% sequential denoising, 25%\n",
            "regular denoising, and 25% extreme denoising loss functions.\n",
            "9\n",
            "UL2 [125]: An encoder-decoder architecture trained using a\n",
            "mixture of denoisers (MoD) objective. Denoisers include 1)\n",
            "R-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\n",
            "rupts consecutive tokens of a large sequence and 3) X-Denoiser:\n",
            "which corrupts a large number of tokens randomly. During pre-\n",
            "training, UL2 includes a denoiser token from R, S, X to rep-\n",
            "resent a denoising setup. It helps improve fine-tuning perfor-\n",
            "mance for downstream tasks that bind the task to one of the up-\n",
            "stream training modes. This MoD style of training outperforms\n",
            "the T5 model on many benchmarks.\n",
            "GLM-130B [33]: GLM-130B is a bilingual (English and Chi-\n",
            "nese) model trained using an auto-regressive mask infilling pre-\n",
            "training objective similar to the GLM [126]. This training style\n",
            "makes the model bidirectional as compared to GPT-3, which is\n",
            "unidirectional. As opposed to GLM, the training of GLM-130B\n",
            "includes a small amount of multi-task instruction pre-training\n",
            "data (5% of the total data) along with self-supervised mask in-\n",
            "filling. To stabilize the training, it applies embedding layer gra-\n",
            "dient shrink.\n",
            "LLaMA [127, 21]: A set of decoder-only language models\n",
            "varying from 7B to 70B parameters. LLaMA models series is\n",
            "the most famous among the community for parameter efficiency\n",
            "and instruction tuning.\n",
            "LLaMA-1 [127]: Implements efficient causal attention [128]\n",
            "by not storing and computing masked attention weights and\n",
            "key/query scores. Another optimization is reducing the number\n",
            "of activations recomputed in the backward pass, as in [129].\n",
            "LLaMA-2 [21]: This work is more focused on fine-tuning a\n",
            "safer and better LLaMA-2-Chat model for dialogue generation.\n",
            "The pre-trained model has 40% more training data with a larger\n",
            "context length and grouped-query attention.\n",
            "PanGu-Σ [92]: An autoregressive model with parameters\n",
            "copied from PanGu-α and extended to a trillion scale with Ran-\n",
            "dom Routed Experts (RRE), the architectural diagram is shown\n",
            "in Figure 10. RRE is similar to the MoE architecture, with\n",
            "distinctions at the second level, where tokens are randomly\n",
            "routed to experts in a domain instead of using a learnable gat-\n",
            "ing method. The model has bottom layers densely activated\n",
            "and shared across all domains, whereas top layers are sparsely\n",
            "activated according to the domain. This training style allows\n",
            "extracting task-specific models and reduces catastrophic forget-\n",
            "ting effects in the case of continual learning.\n",
            "3.1.2. Coding\n",
            "CodeGen [130]:\n",
            "CodeGen has similar architecture to\n",
            "PaLM [15], i.e., parallel attention, MLP layers, and RoPE em-\n",
            "beddings. The model is trained on both natural language and\n",
            "programming language data sequentially (trained on the first\n",
            "dataset, then the second and so on) on the following datasets\n",
            "1) PILE, 2) BIGQUERY and 3) BIGPYTHON. CodeGen pro-\n",
            "posed a multi-step approach to synthesizing code. The purpose\n",
            "is to simplify the generation of long sequences where the previ-\n",
            "ous prompt and generated code are given as input with the next\n",
            "prompt to generate the next code sequence. CodeGen open-\n",
            "source a Multi-Turn Programming Benchmark (MTPB) to eval-\n",
            "uate multi-step program synthesis.\n",
            "Codex [131]: This LLM is trained on a subset of public Python\n",
            "Github repositories to generate code from docstrings. Com-\n",
            "puter programming is an iterative process where the programs\n",
            "are often debugged and updated before fulfilling the require-\n",
            "ments. Similarly to this, Codex generates 100 versions of a\n",
            "program by repetitive sampling for a given description, which\n",
            "produces a working solution for 77.5% of the problems passing\n",
            "unit tests. Its powerful version powers Github Copilot2.\n",
            "AlphaCode [132]: A set of large language models, ranging\n",
            "from 300M to 41B parameters, designed for competition-level\n",
            "code generation tasks. It uses the multi-query attention [133] to\n",
            "reduce memory and cache costs. Since competitive program-\n",
            "ming problems highly require deep reasoning and an under-\n",
            "standing of complex natural language algorithms, the Alpha-\n",
            "Code models are pre-trained on filtered GitHub code in popular\n",
            "languages and then fine-tuned on a new competitive program-\n",
            "ming dataset named CodeContests. The CodeContests dataset\n",
            "mainly contains problems, solutions, and test cases collected\n",
            "from the Codeforces platform3. The pre-training employs stan-\n",
            "dard language modeling objectives, while GOLD [134] with\n",
            "tempering [135] serves as the training objective for the fine-\n",
            "tuning on CodeContests data. To evaluate the performance of\n",
            "AlphaCode, simulated programming competitions are hosted\n",
            "on the Codeforces platform: overall, AlphaCode ranks at the\n",
            "top 54.3% among over 5000 competitors, where its Codeforces\n",
            "rating is within the top 28% of recently participated users.\n",
            "CodeT5+ [34]: CodeT5+ is based on CodeT5 [136], with\n",
            "shallow encoder and deep decoder, trained in multiple stages\n",
            "initially unimodal data (code) and later bimodal data (text-code\n",
            "pairs). Each training stage has different training objectives and\n",
            "activates different model blocks encoder, decoder, or both ac-\n",
            "cording to the task. The unimodal pre-training includes span\n",
            "denoising and CLM objectives, whereas bimodal pre-training\n",
            "objectives contain contrastive learning, matching, and CLM for\n",
            "text-code pairs. CodeT5+ adds special tokens with the text to\n",
            "enable task modes, for example, [CLS ] for contrastive loss,\n",
            "[Match] for text-code matching, etc.\n",
            "StarCoder [137]: A decoder-only model with the SantaCoder\n",
            "architecture, employing Flash attention to scale up the context\n",
            "length to 8k. The StarCoder trains an encoder to filter names,\n",
            "emails, and other personal data from the training data. Its fine-\n",
            "tuned variant outperforms PaLM, LLaMA, and LAMDA on\n",
            "HumanEval and MBPP benchmarks.\n",
            "3.1.3. Scientific Knowledge\n",
            "Galactica [138]: A large curated corpus of human scientific\n",
            "knowledge with 48 million papers, textbooks, lecture notes,\n",
            "millions of compounds and proteins, scientific websites, en-\n",
            "cyclopedias, and more are trained using the metaseq library3,\n",
            "which is built on PyTorch and fairscale [139]. The model wraps\n",
            "reasoning datasets with the < work > token to provide step-by-\n",
            "step reasoning context to the model, which has been shown to\n",
            "improve the performance on reasoning tasks.\n",
            "2https://github.com/features/copilot\n",
            "3https://codeforces.com/\n",
            "10\n",
            "Figure 10: This example illustrates the PanGu-P architecture, as depicted in\n",
            "the image sourced from [92].\n",
            "3.1.4. Dialog\n",
            "LaMDA [140]: A decoder-only model pre-trained on pub-\n",
            "lic dialog data, public dialog utterances, and public web doc-\n",
            "uments, where more than 90% of the pre-training data is in\n",
            "English. LaMDA is trained with the objective of producing re-\n",
            "sponses that exhibit high levels of quality, safety, and grounded-\n",
            "ness. To achieve this, discriminative and generative fine-tuning\n",
            "techniques are incorporated to enhance the model’s safety and\n",
            "quality aspects. As a result, the LaMDA models can be utilized\n",
            "as a general language model performing various tasks.\n",
            "3.1.5. Finance\n",
            "BloombergGPT [141]: A non-causal decoder model trained\n",
            "using both financial (\"FINPILE\" from the Bloomberg archive)\n",
            "and general-purpose datasets. The model’s architecture is sim-\n",
            "ilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\n",
            "eters to different blocks of the model using the approach [113].\n",
            "For effective training, BloombergGPT packs documents to-\n",
            "gether with < |endo ftext| > to use the maximum sequence\n",
            "length, uses warmup batch size starting from 1024 to 2048, and\n",
            "manually reduces the learning rate multiple times during the\n",
            "training.\n",
            "Xuan Yuan 2.0 [142]: A Chinese financial chat model with\n",
            "BLOOM’s [13] architecture trained on a combination of general\n",
            "purpose, financial, general purpose instructions, and financial\n",
            "institutions datasets. Xuan Yuan 2.0 combined the pre-training\n",
            "and fine-tuning stages to avoid catastrophic forgetting.\n",
            "3.2. Fine-Tuned LLMs\n",
            "Pre-trained LLMs have excellent generalization abilities to\n",
            "unseen tasks. However, because they are generally trained with\n",
            "the objective of next token prediction, LLMs have limited ca-\n",
            "pacity to follow user intent and are prone to generate unethical,\n",
            "toxic or inaccurate responses [20]. For their effective utiliza-\n",
            "tion, LLMs are fine-tuned to follow instructions [16, 17, 97] and\n",
            "generate safe responses [20], which also results in increasing\n",
            "zero-shot, few-shot, and cross-task generalization [97, 16, 18],\n",
            "Figure 11: An example image shows an instance of the Flan training paradigm,\n",
            "taken from [16].\n",
            "with minimal compute increment, e.g., 0.2% of the total pre-\n",
            "training for PaLM 540B [16].\n",
            "We review various fine-tuned LLMs and strategies for effective\n",
            "fine-tuning in this section.\n",
            "3.2.1. Instruction-Tuning with Manually Created Datasets\n",
            "Numerous hand-crafted instruction-tuning datasets with\n",
            "different design choices are proposed in the literature to\n",
            "instruction-tune LLMs. The performance of fine-tuned LLMs\n",
            "depends on multiple factors, such as dataset, instruction diver-\n",
            "sity, prompting templates, model size, and training objectives.\n",
            "Keeping this in view, diverse fine-tuned models have emerged\n",
            "in the literature using manually created datasets.\n",
            "The models T0 [17] and mT0 (multi-lingual) [144] employ\n",
            "templates to convert existing datasets into prompt datasets.\n",
            "They have shown improvements in generalization to zero-shot\n",
            "and held-out tasks. Tk-Instruct [18] fine-tuned the T5 model\n",
            "with in-context instructions to study generalization on unseen\n",
            "tasks when given in-context instructions during test time. The\n",
            "model outperformed Instruct-GPT, despite being smaller in\n",
            "size, i.e., 11B parameters as compared to 175B of GPT-3.\n",
            "Increasing Tasks and Prompt Setups: Zero-shot and few-shot\n",
            "performance improves significantly by expanding task collec-\n",
            "tion and prompt styles. OPT-IML [97] and Flan [16] curated\n",
            "larger 2k and 1.8k task datasets, respectively. While increasing\n",
            "task size alone is not enough, OPT-IML and Flan add more\n",
            "prompting setups in their datasets, zero-shot, few-shot, and\n",
            "CoT. In continuation, CoT Collection [101] fine-tunes Flan-T5\n",
            "further on 1.88M CoT samples. Another method [102] uses\n",
            "symbolic tasks with tasks in T0, Flan, etc.\n",
            "3.2.2. Instruction-Tuning with LLMs Generated Datasets\n",
            "Generating an instruction-tuning dataset requires carefully\n",
            "writing instructions and input-output pairs, which are often\n",
            "written by humans, smaller in size, and less diverse.\n",
            "To\n",
            "overcome this, self-instruct [19] proposed an approach to\n",
            "prompt available LLMs to generate instruction-tuning datasets.\n",
            "Self-instruct outperformed models trained on manually created\n",
            "dataset SUPER-NATURALINSTRUCTIONS (a dataset with\n",
            "1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks,\n",
            "1 instruction, and 1 sample per task and iteratively generates\n",
            "11\n",
            "Table 1: Noteworthy findings and insights of pre-trained Large Language Models.\n",
            "Models\n",
            "Findings & Insights\n",
            "T5\n",
            "• Encoder and decoder with shared parameters perform equivalently when parameters are not shared\n",
            "• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only\n",
            "classification layers\n",
            "GPT-3\n",
            "• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-\n",
            "learners\n",
            "mT5\n",
            "• Large multi-lingual models perform equivalently to single language models on downstream tasks.\n",
            "However, smaller multi-lingual models perform worse\n",
            "PanGu-α\n",
            "• LLMs have good few shot capabilities\n",
            "CPM-2\n",
            "• Prompt fine-tuning requires updating very few parameters while achieving performance compara-\n",
            "ble to full model fine-tuning\n",
            "• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\n",
            "• Inserting prompt tokens in-between sentences can allow the model to understand relations between\n",
            "sentences and long sequences\n",
            "• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator\n",
            "(aggregate information with the input text) for the model\n",
            "ERNIE 3.0\n",
            "• A modular LLM architecture with a universal representation module and task-specific representa-\n",
            "tion module helps in the finetuning phase\n",
            "• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is\n",
            "an efficient way to take advantage of the powerful pre-trained model\n",
            "Jurassic-1\n",
            "• The performance of LLM is highly related to the network size\n",
            "• To improve runtime performance, more operations can be performed in parallel (width) rather than\n",
            "sequential (depth)\n",
            "• To efficiently represent and fit more text in the same context length, the model uses a larger vo-\n",
            "cabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further\n",
            "benefits in few-shot learning tasks\n",
            "HyperCLOVA\n",
            "• By employing prompt-based tuning, the performances of models can be improved, often surpassing\n",
            "those of state-of-the-art models when the backward gradients of inputs are accessible\n",
            "Yuan 1.0\n",
            "• The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting\n",
            "behavior in zero-shot and few-shot learning\n",
            "Gopher\n",
            "• Relative encodings enable the model to evaluate for longer sequences than training.\n",
            "ERNIE 3.0 Titan\n",
            "• Additional self-supervised adversarial loss to distinguish between real and generated text improves\n",
            "the model performance as compared to ERNIE 3.0\n",
            "GPT-NeoX-20B\n",
            "• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded\n",
            "layers\n",
            "• Initializing feed-forward output layers before residuals with scheme in [143] avoids activations\n",
            "from growing with increasing depth and width\n",
            "• Training on Pile outperforms GPT-3 on five-shot\n",
            "Table Continued on Next Page\n",
            "12\n",
            "Models\n",
            "Findings & Insights\n",
            "OPT\n",
            "• Restart training from an earlier checkpoint with a lower learning rate if loss diverges\n",
            "• Model is prone to generate repetitive text and stuck in a loop\n",
            "Galactica\n",
            "• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-\n",
            "domain benchmarks, even with multiple repetitions of the corpus, which is superior to existing\n",
            "research on LLMs\n",
            "• A working memory token approach can achieve strong performance over existing methods on\n",
            "mathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream\n",
            "tasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\n",
            "GLaM\n",
            "• The model capacity can be maintained at reduced computation by replacing the feed-forward layer\n",
            "in each transformer layer with a mixture-of-experts (MoE)\n",
            "• The model trained on filtered data shows consistently better performances on both NLG and NLU\n",
            "tasks, where the effect of filtering is more significant on the former tasks\n",
            "• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for\n",
            "the downstream tasks\n",
            "• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in\n",
            "the MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor-\n",
            "mance\n",
            "LaMDA\n",
            "• The model can be fine-tuned to learn to call different external information resources and tools\n",
            "AlphaCode\n",
            "• For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed\n",
            "with a shallower encoder and a deeper decoder\n",
            "• To achieve better performances, it is necessary to employ strategies such as massively scaling\n",
            "upsampling, followed by the filtering and clustering of samples into a compact set\n",
            "• The utilization of novel sampling-efficient transformer architectures designed to facilitate large-\n",
            "scale sampling is crucial\n",
            "• Simplifying problem descriptions can effectively improve the model’s performance\n",
            "Chinchilla\n",
            "• The model size and the number of training tokens should be scaled proportionately: for each dou-\n",
            "bling of the model size, the number of training tokens should be doubled as well\n",
            "PaLM\n",
            "• English-centric models produce better translations when translating to English as compared to non-\n",
            "English\n",
            "• Generalized models can have equivalent performance for language translation to specialized small\n",
            "models\n",
            "• Larger models have a higher percentage of training data memorization\n",
            "• Performance has not yet saturated even at 540B scale, which means larger models are likely to\n",
            "perform better\n",
            "AlexaTM\n",
            "• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the\n",
            "context than decoder-only\n",
            "• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context\n",
            "learning\n",
            "• Placing layer norm at the beginning of each transformer layer improves the training stability\n",
            "Table Continued on Next Page\n",
            "13\n",
            "Models\n",
            "Findings & Insights\n",
            "U-PaLM\n",
            "• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\n",
            "• Training with a mixture of denoisers improves the infilling ability and open-ended text generation\n",
            "diversity\n",
            "UL2\n",
            "• Mode switching training enables better performance on downstream tasks\n",
            "• CoT prompting outperforms standard prompting for UL2\n",
            "GLM-130B\n",
            "• Pre-training data with a small proportion of multi-task instruction data improves the overall model\n",
            "performance\n",
            "CodeGen\n",
            "• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-\n",
            "eration\n",
            "LLaMA\n",
            "• A constant performance improvement is observed when scaling the model\n",
            "• Smaller models can achieve good performances with more training data and computing time\n",
            "PanGu-Σ\n",
            "• Sparse models provide the benefits of large models at a lower computation cost\n",
            "• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for\n",
            "continual learning\n",
            "• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is\n",
            "cost-efficient while maintaining a performance similar to the original\n",
            "BloombergGPT\n",
            "• Pre-training with general-purpose and task-specific data improves task performance without hurt-\n",
            "ing other model capabilities\n",
            "XuanYuan 2.0\n",
            "• Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\n",
            "CodeT5+\n",
            "• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\n",
            "• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other\n",
            "for better performance\n",
            "StarCoder\n",
            "• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\n",
            "LLaMA-2\n",
            "• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after\n",
            "fine-tuning\n",
            "• Model trained on unfiltered data requires fewer samples for safety alignment\n",
            "PaLM-2\n",
            "• Data quality is important to train better models\n",
            "• Model and data size should be scaled with 1:1 proportions\n",
            "• Smaller models trained for larger iterations outperform larger models\n",
            "14\n",
            "Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.\n",
            "Models\n",
            "Findings & Insights\n",
            "T0\n",
            "• Multi-task prompting enables zero-shot generalization and outperforms baselines\n",
            "• Even a single prompt per dataset task is enough to improve performance\n",
            "WebGPT\n",
            "• To aid the model in effectively filtering and utilizing relevant information, human labelers play a\n",
            "crucial role in answering questions regarding the usefulness of the retrieved documents\n",
            "• Interacting a fine-tuned language model with a text-based web-browsing environment can improve\n",
            "end-to-end retrieval and synthesis via imitation learning and reinforcement learning\n",
            "• Generating answers with references can make labelers easily judge the factual accuracy of answers\n",
            "Tk-INSTRUCT\n",
            "• Instruction tuning leads to a stronger generalization of unseen tasks\n",
            "• More tasks improve generalization whereas only increasing task instances does not help\n",
            "• Supervised trained models are better than generalized models\n",
            "• Models pre-trained with instructions and examples perform well for different types of inputs\n",
            "mT0 and BLOOMZ\n",
            "• Instruction tuning enables zero-shot generalization to tasks never seen before\n",
            "• Multi-lingual training leads to even better zero-shot generalization for both English and non-\n",
            "English\n",
            "• Training on machine-translated prompts improves performance for held-out tasks with non-English\n",
            "prompts\n",
            "• English only fine-tuning on multilingual pre-trained language model is enough to generalize to\n",
            "other pre-trained language tasks\n",
            "OPT-IML\n",
            "• Creating a batch with multiple task examples is important for better performance\n",
            "• Only example proportional sampling is not enough, training datasets should also be proportional\n",
            "for better generalization/performance\n",
            "• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories\n",
            "whereas fully supervised tasks have no effect\n",
            "• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\n",
            "• Only 1% reasoning data improves the performance, adding more deteriorates performance\n",
            "• Adding dialogue data makes the performance worse\n",
            "Sparrow\n",
            "• Labelers’ judgment and well-defined alignment rules help the model generate better responses\n",
            "• Good dialogue goals can be broken down into detailed natural language rules for the agent and the\n",
            "raters\n",
            "• The combination of reinforcement learning (RL) with reranking yields optimal performance in\n",
            "terms of preference win rates and resilience against adversarial probing\n",
            "Flan\n",
            "• Finetuning with CoT improves performance on held-out tasks\n",
            "• Fine-tuning along with CoT data improves reasoning abilities\n",
            "• CoT tuning improves zero-shot reasoning\n",
            "• Performance improves with more tasks\n",
            "• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\n",
            "• Improving the model’s performance with instruction tuning is compute-efficient\n",
            "• Multitask prompting enables zero-shot generalization abilities in LLM\n",
            "WizardCoder\n",
            "• Fine-tuning with re-written instruction-tuning data into a complex set improves performance\n",
            "LLaMA-2-Chat\n",
            "• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\n",
            "RLHF step further improves model safety and make it less prone to jailbreak attacks\n",
            "LIMA\n",
            "• Less high quality data is enough for fine-tuned model generalization\n",
            "15\n",
            "new instructions (52k) and instances (82k input-output pairs)\n",
            "using GPT-3 [6]. Contrary to this, Dynosaur [145] uses the\n",
            "meta-data of datasets on Huggingface to prompt LLMs to\n",
            "generate multiple task instruction-tuning datasets.\n",
            "LLaMA Tuned: Various models in the literature instruction-\n",
            "tune LLaMA [146] with GPT-3 [6] or GPT-4 [147] generated\n",
            "datasets.\n",
            "Among these, Alpaca [148], Vicuna [149], and\n",
            "LLaMA-GPT-4 [150] are a few general-purpose fine-tuned\n",
            "models, where Alpaca is trained on 52k samples from text-\n",
            "davinci-003, Vicuna on 70k samples from ShareGPT.com,\n",
            "and LLaMA-GPT-4 by re-creating Alpaca instructions from\n",
            "GPT-4.\n",
            "Goat [151] fine-tunes LLaMA for arithmetic tasks\n",
            "(1 million samples) by generating data from ChatGPT and\n",
            "outperforms GPT-4, PaLM, BLOOM, OPT, etc., attributing its\n",
            "success to the LLaMA’s consistent tokenization of numbers.\n",
            "HuaTuo [152] is a medical knowledge model, fine-tuned with\n",
            "a generated QA dataset of 8k instructions.\n",
            "Complex Instructions:\n",
            "Evol-Instruct [153, 154] prompts\n",
            "LLMs to convert given instructions into a more complex set.\n",
            "The instructions are iteratively evolved with re-writing instruc-\n",
            "tions in complex wording and creating new instructions. With\n",
            "this style of automated instruction generation, WizardLM [153]\n",
            "(fine-tuned LLaMA on 250k instructions), outperforms Vicuna\n",
            "and Alpaca, and WizardCoder [154] (fine-tuned StarCoder)\n",
            "beats Claude-Plus, Bard, and others.\n",
            "3.2.3. Aligning with Human Preferences\n",
            "Incorporating human preferences into LLMs presents a\n",
            "significant advantage in mitigating undesirable behaviors and\n",
            "ensuring accurate outputs. The initial work on alignment, such\n",
            "as InstructGPT [20] aligns GPT-3 using a 3-step approach,\n",
            "instruction-tuning, reward modeling, and fine-tuning with\n",
            "reinforcement learning (RL). The supervised fine-tuned GPT-3\n",
            "on demonstrations is queried to generate responses, which\n",
            "human labelers rank according to human values, and a reward\n",
            "model is trained on the ranked data. Lastly, the GPT-3 is trained\n",
            "with proximal policy optimization (PPO) using rewards on the\n",
            "generated data from the reward model. LLaMA 2-Chat [21]\n",
            "improves alignment by dividing reward modeling into help-\n",
            "fulness and safety rewards and using rejection sampling in\n",
            "addition to PPO. The initial four versions of LLaMA 2-Chat\n",
            "are fine-tuned with rejection sampling and then with PPO on\n",
            "top of rejection sampling.\n",
            "Aligning with Supported Evidence: This style of alignment\n",
            "allows the model to generate responses with proofs and facts,\n",
            "reduces hallucination, and assists humans more effectively,\n",
            "which increases trust in the model’s output.\n",
            "Similar to\n",
            "the RLHF training style, a reward model is trained to rank\n",
            "generated responses containing web citations in answers\n",
            "to questions, which is later used to train the model, as in\n",
            "GopherCite [155], WebGPT [156], and Sparrow [157]. The\n",
            "ranking model in Sparrow [157] is divided into two branches,\n",
            "preference reward and rule reward, where human annotators\n",
            "adversarial probe the model to break a rule. These two rewards\n",
            "together rank a response to train with RL.\n",
            "Aligning Directly with SFT: The PPO in the RLHF pipeline\n",
            "is complex, memory-intensive, and unstable, requiring mul-\n",
            "tiple models, reward, value, policy, and reference models.\n",
            "Avoiding this sophisticated alignment pipeline is possible by\n",
            "incorporating minimal changes in the supervised fine-tuning\n",
            "(SFT) pipeline as in [158, 159, 160], with better or compa-\n",
            "rable performance to PPO. Direct preference optimization\n",
            "(DPO) [158] trains a model directly on the human-preferred\n",
            "responses to maximize the likelihood of preferred against\n",
            "unpreferred responses, with per-sample importance weight.\n",
            "Reward ranked fine-tuning RAFT [159] fine-tunes the model\n",
            "on ranked responses by the reward model. Preference ranking\n",
            "optimization (PRO) [161] and RRHF [160] penalize the model\n",
            "to rank responses with human preferences and supervised loss.\n",
            "On the other hand, chain-of-hindsight (CoH) [162] provides\n",
            "feedback to the model in language rather than reward, to learn\n",
            "good versus bad responses.\n",
            "Aligning with Synthetic Feedback:\n",
            "Aligning LLMs with\n",
            "human feedback is slow and costly. The literature suggests a\n",
            "semi-automated process to align LLMs by prompting LLMs to\n",
            "generate helpful, honest, and ethical responses to the queries,\n",
            "and fine-tuning using the newly created dataset. Constitutional\n",
            "AI [163] replaces human feedback in RLHF with AI, calling\n",
            "it RL from AI feedback (RLAIF). AlpacaFarm [164] designs\n",
            "prompts to imitate human feedback using LLMs APIs. Oppo-\n",
            "site to constitutional AI, AlpacaFarm injects noise in feedback\n",
            "to replicate human mistakes.\n",
            "Self-Align [98] prompts the\n",
            "LLM with ICL examples, instructing the LLM about what the\n",
            "response should contain to be considered useful and ethical.\n",
            "The same LLM is later fine-tuned with the new dataset.\n",
            "Aligning with Prompts: LLMs can be steered with prompts to\n",
            "generate desirable responses without training [165, 166]. The\n",
            "self-correction prompting in [166] concatenates instructions\n",
            "and CoT with questions, guiding the model to answer its\n",
            "instruction following a strategy to ensure moral safety before\n",
            "the actual answer. This strategy is shown to reduce the harm in\n",
            "generated responses significantly.\n",
            "Red-Teaming/Jailbreaking/Adversarial\n",
            "Attacks:\n",
            "LLMs\n",
            "exhibit harmful behaviors, hallucinations, leaking personal in-\n",
            "formation, and other shortcomings through adversarial probing.\n",
            "The models are susceptible to generating harmful responses\n",
            "even though they are aligned for safety [167, 168].\n",
            "Red-\n",
            "teaming is a common approach to address illicit outputs, where\n",
            "the LLMs are prompted to generate harmful outputs [168, 169].\n",
            "The dataset collected through red-teaming is used to fine-tune\n",
            "models for safety. While red-teaming largely relies on human\n",
            "annotators, another work [170] red-team LLMs to find prompts\n",
            "that lead to harmful outputs for other LLMs.\n",
            "3.2.4. Continue Pre-Training\n",
            "Although fine-tuning boosts a model’s performance, it leads\n",
            "to catastrophic forgetting of previously learned information.\n",
            "Concatenating fine-tuning data with a few randomly selected\n",
            "pre-training samples in every iteration avoids network forget-\n",
            "ting [171, 142]. This is also effective in adapting LLMs for\n",
            "cases where fine-tuning data is small and the original capac-\n",
            "16\n",
            "ity is to be maintained. Prompt-based continued pre-training\n",
            "(PCP) [172] trains the model with text and instructions related\n",
            "to tasks and then finally instruction-tunes the model for down-\n",
            "stream tasks.\n",
            "3.2.5. Sample Efficiency\n",
            "While fine-tuning data is generally many-fold smaller than\n",
            "the pre-training data, it still has to be large enough for accept-\n",
            "able performance [16, 97, 18] and requires proportional com-\n",
            "puting resources. Studying the effects on performance with less\n",
            "data, existing literature [173, 174] finds that models trained\n",
            "on less data can outperform models trained with more data.\n",
            "In [173], 25% of the total downstream data is found enough\n",
            "for state-of-the-art performance. Selecting coreset-based 0.5%\n",
            "of the total instruction-tuning data improves the model perfor-\n",
            "mance by 2% in [174], as compared to the complete data tun-\n",
            "ing. Less is more for alignment (LIMA) [175] uses only 1000\n",
            "carefully created demonstrations to fine-tune the model and has\n",
            "achieved comparable performance to GPT-4.\n",
            "3.3. Increasing Context Window\n",
            "LLMs are trained with limited context windows due to ex-\n",
            "pensive attention and high memory requirements.\n",
            "A model\n",
            "trained on limited sequence lengths fails to generalize to unseen\n",
            "lengths at inference time [176, 49]. Alternatively, LLMs with\n",
            "ALiBi [65] positional encodings can perform zero-shot length\n",
            "extrapolation. However, ALiBi has less expressive power [66]\n",
            "and inferior performance on multiple benchmarks [46], and\n",
            "many LLMs use RoPE positional embedding that is unable to\n",
            "perform zero-shot extrapolation. A larger context length has\n",
            "benefits such as a better understanding of longer documents,\n",
            "more samples in in-context learning, execution of bigger rea-\n",
            "soning processes, etc. Expanding context length during fine-\n",
            "tuning is slow, inefficient, and computationally expensive [49].\n",
            "Therefore, researchers employ various context window extrap-\n",
            "olation techniques discussed below.\n",
            "Position Interpolation: Rather than extrapolating, [49] shows\n",
            "that interpolating position encodings within the pre-trained con-\n",
            "text window are more effective. The work demonstrates that\n",
            "only 1000 steps of fine-tuning are enough to achieve better re-\n",
            "sults on larger windows without reducing performance com-\n",
            "pared to the original context size. Giraffe [46] uses power scal-\n",
            "ing in RoPE, and YaRN [47] proposed NTK-aware interpola-\n",
            "tion.\n",
            "Efficient Attention Mechanism:\n",
            "Dense global attention is\n",
            "one of the major constraints in training larger context win-\n",
            "dow LLMs.\n",
            "Using efficient attention variants, such as lo-\n",
            "cal, sparse, and dilated attention, reduces the computation cost\n",
            "significantly.\n",
            "LongT5 [48] proposes transient global atten-\n",
            "tion (TGlobal), applying attention to local and global tokens\n",
            "(windowed token averaging).\n",
            "The model replaces attention\n",
            "in T5 [10] with TGlobal attention, pre-trains the model on\n",
            "4098 sequence length, fine-tunes on larger window sizes, as\n",
            "large as 16k, and improves task performance on longer inputs.\n",
            "This shows the extrapolation ability of TGlobal attention with\n",
            "only fine-tuning. COLT5 [177] uses two branches, one with\n",
            "lightweight and the other with heavyweight attention and feed-\n",
            "forward layers. All tokens are processed from the lightweight\n",
            "branch, and only important tokens are routed to the heavy-\n",
            "weight branch. LongNet [178] replaces standard attention with\n",
            "dilated attention, expanding sequence length to 1 billion tokens.\n",
            "LongLoRA [179] proposes shift-short attention, used during\n",
            "fine-tuning to reduce dense attention costs. However, the model\n",
            "during inference uses dense attention and achieves similar per-\n",
            "formance as full attention fine-tuning.\n",
            "Extrapolation without Training: LM-Infinite [176] and par-\n",
            "allel context windows (PCW) [180] show length extrapolation\n",
            "is possible using pre-trained LLMs. LM-Infinite suggested Λ-\n",
            "shaped attention applied within the original context window\n",
            "limits. Likewise, PCW chunks larger inputs into the pre-trained\n",
            "context lengths and applies the same positional encodings to\n",
            "each chunk.\n",
            "3.4. Augmented LLMs\n",
            "LLMs are capable of learning from the examples concate-\n",
            "nated with the input, known as context augmentation, in-\n",
            "context learning (ICL), or few-shot prompting. They show ex-\n",
            "cellent generalization to unseen tasks with few-shot prompt-\n",
            "ing, enabling LLMs to answer queries beyond the capacity ac-\n",
            "quired during training [6, 55]. These emergent abilities allow\n",
            "for adapting the model without fine-tuning—a costly process.\n",
            "Aside from this, hallucination, producing inaccurate, unsafe,\n",
            "or factually incorrect responses, is common for LLMs, which is\n",
            "avoided by augmenting contextual data. While the user can pro-\n",
            "vide in-context samples in the query [54, 32], here we specifi-\n",
            "cally refer to the methods that access external storage program-\n",
            "matically, calling them augmented LLMs.\n",
            "The literature suggests various external memory designs to aug-\n",
            "ment LLMs, long-term [181, 182, 183, 184], short-term [185],\n",
            "symbolic [186], and non-symbolic [187, 188]. The memory\n",
            "can be maintained in different formats such as documents, vec-\n",
            "tors, or databases. A few systems maintain intermediate mem-\n",
            "ory representations to retain information across multiple iter-\n",
            "ations [184, 182], while others extract important information\n",
            "from the datasets and save it in memory for recall [189]. The\n",
            "memory read and write operations are performed either with\n",
            "or without LLMs cooperation [182, 190, 184, 191], acting as\n",
            "a feedback signal in [185]. We discuss different types of aug-\n",
            "mented LLMs below.\n",
            "3.4.1. Retrieval Augmented LLMs\n",
            "LLMs may have limited memory and outdated information,\n",
            "leading to inaccurate responses. Retrieving relevant informa-\n",
            "tion from external up-to-date storage enables the LLMs to\n",
            "accurately answer with references and utilize more informa-\n",
            "tion. With retrieval augmentation, smaller models have been\n",
            "shown to perform at par with larger models. For instance, the\n",
            "11B model can become competitive to 540B PaLM in [25] and\n",
            "7.5B to 280B Gopher in [183]. Retrieval augmented language\n",
            "modeling (RALM) has two major components, shown in\n",
            "Figure 12, namely: 1) retriever and 2) language model.\n",
            "In\n",
            "RALM, the retriever plays a crucial role in driving LLM\n",
            "17\n",
            "Figure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex-\n",
            "tracts a similar context to the input and forwards it to the LLM either in simple\n",
            "language or encoded through Fusion-in-Decoder (FiD). Depending on the task,\n",
            "retrieval and generation may repeat multiple times.\n",
            "response, where incorrect information can steer LLMs to false\n",
            "behavior. This leads to the development of various methods to\n",
            "retrieve accurate information and fuse with the query for better\n",
            "performance.\n",
            "Zero-Shot Retrieval Augmentation: This kind of augmen-\n",
            "tation keeps the original LLM architecture and weights\n",
            "unchanged and uses BM25 [192], nearest neighbors, or frozen\n",
            "pre-trained models like Bert [7] as a retriever. The retrieved\n",
            "information is provided as input to the model for response\n",
            "generation, shown to improve performance over LLMs without\n",
            "retrieval [188, 193].\n",
            "In some scenarios, multiple retrieval\n",
            "iterations are required to complete the task.\n",
            "The output\n",
            "generated in the first iteration is forwarded to the retriever\n",
            "to fetch similar documents. Forward-looking active retrieval\n",
            "(FLARE) [187] initially generates the response and corrects\n",
            "the output by retrieving relevant documents if the response\n",
            "contains low-confidence tokens. Similarly, RepoCoder [194]\n",
            "fetches code snippets recursively for code completion.\n",
            "Training with Retrieval Augmentation: To reduce failures in\n",
            "retrieval augmentation generation (RAG), researchers train or\n",
            "fine-tune retrievers and LLMs with a retrieval augmentation\n",
            "pipeline. We discuss the literature below based on their focus\n",
            "on the respective training processes of the pipeline.\n",
            "Training LLM: Retrieval-enhanced transformer (RETRO) [183]\n",
            "shows pre-training smaller LLMs with RAG pipeline outper-\n",
            "forms larger LLMs, such as GPT-3 trained without RAG.\n",
            "RETRO uses a 2-trillion token subset of MassiveText as\n",
            "a database.\n",
            "The retrieval pipeline divides the input query\n",
            "into subsets and retrieves relevant chunks from the database\n",
            "for each subset, encoded together with input intermediate\n",
            "representations for generating tokens. It uses cross-chunked\n",
            "attention to attend to previous chunks auto-regressively.\n",
            "A\n",
            "study on RETRO [195] shows models pre-trained without RAG\n",
            "but fine-tuned using RAG lack the performance gains obtained\n",
            "by pre-training with RAG.\n",
            "Training Retriever: Quality of responses generated by LLMs\n",
            "is highly dependent on the in-context examples.\n",
            "There-\n",
            "fore, [196, 197, 198, 199] train retrievers to retrieve accurate\n",
            "few-shot samples while keeping the LLM frozen for gener-\n",
            "ation.\n",
            "Retrieved samples are ranked to build ground-truth\n",
            "data to train retrievers with contrastive learning in [196, 198].\n",
            "RoBERTa is trained for downstream tasks in [197] for ICL\n",
            "samples retrieval.\n",
            "REPLUG [199] trains the retriever with\n",
            "supervised signals from the frozen LLM-generated outputs.\n",
            "Training Retriever and LLM: Further benefits are achieved by\n",
            "training both the retriever and the model in [25, 200, 201]. In\n",
            "this case, the error propagates back to the retriever, updating\n",
            "both the language model and the retriever.\n",
            "While masked\n",
            "language modeling (MLM) is a common pre-training objec-\n",
            "tive [25, 201], retrieval pre-trained transformer (RPT) [200]\n",
            "used document chunk prediction as a pre-training objective for\n",
            "long text modeling.\n",
            "Encoded Context Augmentation:\n",
            "Concatenating retrieved\n",
            "documents with the query becomes infeasible as the sequence\n",
            "length and sample size grow. Encoding the context and fusing\n",
            "it with the decoder (Fusion-in-Decoder) using cross-attention\n",
            "makes it possible to augment more samples without increasing\n",
            "computation costs significantly [202, 183, 200, 25].\n",
            "Web Augmented:\n",
            "Locally stored memory, but external to\n",
            "LLM, has limited information. However, a large amount of\n",
            "information is available on the internet, which gets updated\n",
            "regularly.\n",
            "Rather than storing information locally, various\n",
            "methods retrieve query-related context through a web search\n",
            "and forward it to LLMs [203, 204, 156].\n",
            "3.4.2. Tool Augmented LLMs\n",
            "While RAG relies on the retriever to provide context to the\n",
            "LLM to answer queries, tool augmented LLMs capitalize on the\n",
            "reasoning abilities of LLMs to iteratively plan by dividing tasks\n",
            "into sub-tasks, selecting necessary tools, and taking actions to\n",
            "complete the task [205, 206, 207, 27]. A generic pipeline of\n",
            "tool-augmented LLMs is shown in Figure 13, where different\n",
            "modules in Figure 13 are selected in a loop until the task com-\n",
            "pletion.\n",
            "Zero-Shot Tool Augmentation: LLMs in-context learning\n",
            "and reasoning abilities enable them to interact with tools with-\n",
            "out training. Automatic reasoning and tool-use (ART) [207]\n",
            "builds a task library with demonstrations of reasoning steps and\n",
            "calling external tools. It retrieves similar task examples and\n",
            "provides the context to the LLM for inference. Aside from\n",
            "this, [208] shows tool documentation is enough to teach LLMs\n",
            "to use tools without demonstrations. RestGPT [209] integrates\n",
            "LLMs with RESTful APIs by decomposing tasks into planning\n",
            "and API selection steps. The API selector understands the API\n",
            "documentation to select a suitable API for the task and plan the\n",
            "execution. ToolkenGPT [210] uses tools as tokens by concate-\n",
            "nating tool embeddings with other token embeddings. During\n",
            "inference, the LLM generates the tool tokens representing the\n",
            "tool call, stops text generation, and restarts using the tool exe-\n",
            "cution output.\n",
            "Training with Tool Augmentation: LLMs are trained to inter-\n",
            "act with diverse tools, enhancing planning abilities to overcome\n",
            "the limitations of zero-shot tool augmentation [211, 27, 212,\n",
            "213]. Gorilla [211] instruction-tunes LLaMA with information\n",
            "retrieval from API documentation. It uses the self-instruct [19]\n",
            "18\n",
            "Figure 13: A basic flow diagram of tool augmented LLMs. Given an input and\n",
            "a set of available tools, the model generates a plan to complete the task. The\n",
            "tool augmented LLMs utilize different modules iteratively, such as retriever,\n",
            "tool execution, read-write to memory, feedback, etc., depending on the task.\n",
            "data generation pipeline with GPT-4 by providing in-context\n",
            "examples retrieved from API documentation. Tool augmented\n",
            "language model (TALM) [27] fine-tunes T5 [10] for tool use\n",
            "with a self-play approach, where it iteratively completes tool\n",
            "manipulation tasks and includes them back in the training set.\n",
            "ToolLLM [213] collects 16k APIs from RapidAPI. It samples\n",
            "APIs from the list to generate an instruction-tuning dataset us-\n",
            "ing ChatGPT in single-tool and multi-tool scenarios. For high-\n",
            "quality datasets, ToolLLM suggested a depth-first search-based\n",
            "decision tree (DFSDT) method to generate ground-truths with\n",
            "diverse reasoning and planning.\n",
            "Multimodal Tool Augmentation: The compositional reasoning\n",
            "capacity of LLMs allows them to manipulate tools in multi-\n",
            "modal settings [205, 206, 214]. Following the pipeline shown\n",
            "in Figure 13, the LLM outlines a plan, generally executing in a\n",
            "sequence: Plan →Tool selection →Execute →Inspect →\n",
            "Generate, to respond to the user query. Here, the database of\n",
            "tools is rich in modalities, including text, images, etc. Many of\n",
            "the multimodal tool augmentation systems employ multimodal\n",
            "LLMs [31, 215, 214, 206], while others utilize single modality\n",
            "LLMs and generate a plan on using different modality tools to\n",
            "solve multimodal queries [216].\n",
            "3.5. LLMs-Powered Agents\n",
            "AI agents are autonomous entities, capable of planning,\n",
            "decision-making, and performing actions to achieve complex\n",
            "goals.\n",
            "In the early days, AI agents were rule-based, de-\n",
            "signed for narrow tasks, and had limited capabilities, such\n",
            "as Clippy [217] and Deep Blue [218].\n",
            "In contrast to this,\n",
            "LLMs abilities to respond to dynamic scenarios have made it\n",
            "possible to incorporate them in diverse applications, includ-\n",
            "ing LLMs-powered agents [214, 206], where LLMs behave\n",
            "as the brain of agents. LLMs have been incorporated in web\n",
            "agents [156, 157], coding agents [219], tool agents [27, 213],\n",
            "embodied agents [26], and conversational agents [185], requir-\n",
            "ing minimal to no fine-tuning\". Below we summarize the re-\n",
            "search in LLMs-based autonomous agents. For a more detailed\n",
            "discussion, please refer to [220, 221].\n",
            "LLMs Steering Autonomous Agents: LLMs are the cognitive\n",
            "controllers of the autonomous agents. They generate plans, rea-\n",
            "son about tasks, incorporate memory to complete tasks, and\n",
            "adapt the outline depending on the feedback from the environ-\n",
            "ment. Depending on the acquired capabilities of LLMs, many\n",
            "methods fine-tune, propose a better prompting approach, or uti-\n",
            "lize different modules to enhance agents’ performance. Mod-\n",
            "ules and strategies employed in autonomous agents are briefly\n",
            "discussed below.\n",
            "Planning and Reasoning: Completing a complex task requires\n",
            "human-like logical thinking, planning necessary steps, and\n",
            "reasoning current and future directions. Prompting methods\n",
            "like chain-of-thoughts [103], tree-of-thoughts [105], and self-\n",
            "consistency [104] are central to agents, eliciting LLMs to rea-\n",
            "son its actions and choose among different paths for task com-\n",
            "pletion. When LLMs are prompted with a task description and\n",
            "a sequence of actions, they can accurately generate plan ac-\n",
            "tions without any fine-tuning [222]. Reasoning via planning\n",
            "(RAP) [223] incorporates a re-purposed LLM as a world model\n",
            "to reason about future outcomes and explore alternative paths\n",
            "for task completion. Retroformer [224] uses a retrospective\n",
            "LLM to improve main LLM planning and reasoning capabil-\n",
            "ities by providing helpful task cues.\n",
            "Feedback: LLMs in open-loop systems generate plans and as-\n",
            "sume that the agent will complete them successfully. However,\n",
            "the actual scenario is different with failures and variable re-\n",
            "sponses from the environment. To correctly complete tasks,\n",
            "many methods use LLMs in a closed-loop where the action re-\n",
            "sponse is provided as feedback to the LLMs to re-assess and\n",
            "update the plan as required [225, 226, 227, 185]. Another di-\n",
            "rection of research exploits LLMs as reward functions to train\n",
            "reinforcement learning (RL) policies instead of humans [228].\n",
            "Memory: LLMs can learn from the context provided in the\n",
            "prompt. In addition to internal memory, various systems em-\n",
            "ploy external memory to save the response history.\n",
            "Reflex-\n",
            "ion [185] maintains an episodic memory to use previous re-\n",
            "sponses as feedback to improve future decision-making. Retro-\n",
            "former [224] improves its responses by employing short-term\n",
            "and long-term memory, where short-term memory contains re-\n",
            "cent responses and long-term memory keeps summarized failed\n",
            "attempts to add in the prompt as reflection.\n",
            "Multi-Agents Systems: LLMs can play user-defined roles and\n",
            "behave like a specific domain expert. In multi-agent systems,\n",
            "each LLM is assigned a unique role, simulating human behav-\n",
            "ior and collaborating with other agents to complete a complex\n",
            "task [219, 229].\n",
            "LLMs in Physical Environment:\n",
            "LLMs are good at\n",
            "instruction-following, however, utilizing them for physically\n",
            "grounded tasks requires adaptation, as they lack real-world\n",
            "knowledge. This could lead to generating illogical responses\n",
            "for a particular physical situation [230, 26].\n",
            "SayCan [230]\n",
            "19\n",
            "make LLMs aware of the available low-level task operations.\n",
            "LLM (Say) builds a high-level plan to complete the task and\n",
            "a learned affordance function (Can) explores the possibility of\n",
            "executing the plan in the real world. SayCan uses RL to train\n",
            "the language-conditioned affordance function. PaLM-E enables\n",
            "the LLM to solve grounded tasks by training multi-modal LLM\n",
            "feeding inputs directly from the sensors.\n",
            "Manipulation: In the area of manipulation [226, 231], LLMs\n",
            "enhance a robot’s dexterity and adaptability, excelling in tasks\n",
            "like object recognition, grasping, and collaboration. They ana-\n",
            "lyze visual and spatial information to determine the most effec-\n",
            "tive approach to interact with objects.\n",
            "Navigation: LLMs enhance a robot’s ability to navigate com-\n",
            "plex environments with precision and adaptability [232, 233,\n",
            "234, 235]. They generate feasible paths and trajectories for\n",
            "robots, accounting for intricate environmental details [236].\n",
            "This ability is valuable in scenarios requiring precise and\n",
            "dynamically adaptable navigation in environments like ware-\n",
            "houses, transport, healthcare facilities, and residences.\n",
            "3.6. Efficient LLMs\n",
            "Deploying LLMs in production is expensive. Reducing their\n",
            "running costs while preserving performance is an appealing\n",
            "area of research. This section summarizes the approaches sug-\n",
            "gested to enhance LLMs’ efficiency.\n",
            "3.6.1. Parameter Efficient Fine-Tuning\n",
            "Fine-tuning LLMs with tens or hundreds of billions of pa-\n",
            "rameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG\n",
            "(540B), etc., is computationally intensive and time-consuming.\n",
            "To avoid complete model fine-tuning, numerous parameter-\n",
            "efficient fine-tuning (PEFT) techniques [40, 237, 41, 38, 39] try\n",
            "to achieve acceptable model fine-tuning performance at reduced\n",
            "costs. As compared to full fine-tuning [238], PEFT performs\n",
            "better in low-resource setups, achieves comparable perfor-\n",
            "mance on medium-resource scenarios, and performs worse than\n",
            "full fine-tuning under high-resource availability. An overview\n",
            "of different PEFT approaches is shown in Figure 14.\n",
            "Adapter Tuning: Adds a few trainable parameters within the\n",
            "transformer block. The adapter layer is a sequence of feature\n",
            "downscaling, non-linearity, and upscaling [106]. Variants of\n",
            "adapter tuning inject adapter layers sequentially [106] and in\n",
            "parallel [38], whereas the mixture of adapter (AdaMix) [239]\n",
            "employs multiple adapter modules in a single layer. AdaMix\n",
            "routes input instances randomly to one of the multiple down-\n",
            "scale and upscale modules. The mixture of adapters is averaged\n",
            "out for inference to avoid additional latency. Low-Rank Adap-\n",
            "tation (LoRA) [240] learns low-rank decomposed matrices to\n",
            "freeze original weights. The learned weights are fused with the\n",
            "original weights for inference, avoiding latency.\n",
            "Prompt Tuning: Prompting is an effective way to adapt a\n",
            "pre-trained LLM for the downstream task. However, manual\n",
            "prompts bring uncertainty in the model’s prediction, where a\n",
            "change in a single word drops the performance [237]. Prompt\n",
            "tuning alleviates this problem by fine-tuning only 0.001%-3%\n",
            "additional parameters [241]. It concatenates trainable prompt\n",
            "parameters with the model embeddings [237, 40, 241]. Task-\n",
            "specific fixed discrete prompts are concatenated with input em-\n",
            "beddings in [40]. As discrete prompts bring instability, prompts\n",
            "are encoded through a learnable mapping in P-Tuning [237],\n",
            "naming continuous prompts, which are appended with the dis-\n",
            "crete prompts.\n",
            "Only the prompt encoder is trainable in the\n",
            "model. In an extension of P-Tuning, continuous prompts are\n",
            "concatenated with each layer of the network in [241]. Progres-\n",
            "sive prompts [242] avoid catastrophic forgetting and transfer\n",
            "previously learned knowledge by sequentially adding trainable\n",
            "prompt embeddings to the previously frozen task embeddings.\n",
            "Prefix Tuning: A set of trainable task-specific prefix vectors\n",
            "are appended to the frozen transformer layers in prefix tun-\n",
            "ing [41]. The prefix vectors are virtual tokens attended by the\n",
            "context tokens on the right. In addition, adaptive prefix tun-\n",
            "ing [243] applies a gating mechanism to control the information\n",
            "from the prefix and actual tokens.\n",
            "Bias Tuning: Fine-tuning only bias terms in small to medium\n",
            "training data has been found effective in BitFit [244].\n",
            "This\n",
            "method achieves full fine-tuning performance for tasks with less\n",
            "training data and comparable performance with more training\n",
            "data.\n",
            "3.6.2. Quantization\n",
            "LLMs require extensive computing and memory for infer-\n",
            "ence.\n",
            "Deploying a 175B parameter GPT-3 model needs at\n",
            "least 5x80GB A100 GPUs and 350GB of memory to store in\n",
            "FP16 format [44]. Such demanding requirements for deploying\n",
            "LLMs make it harder for smaller organizations to utilize them.\n",
            "Model compression is an effective solution but comes at the cost\n",
            "of degraded performance, especially at large scales greater than\n",
            "6B. These models exhibit very large magnitude outliers that do\n",
            "not exist in smaller models [245], making it challenging and re-\n",
            "quiring specialized methods for quantizing LLMs [44, 246].\n",
            "Post-Training Quantization: Minimal or no training is re-\n",
            "quired in this type of quantization, without significantly com-\n",
            "promising the model performance. LLM-8-bit [245] uses full-\n",
            "precision matrix multiplication for weights associated with out-\n",
            "lier features and 8-bit for remaining features. The lower pre-\n",
            "cision multiplication outputs are converted to FP-16 and con-\n",
            "catenated with others. The quantized models have homogenous\n",
            "word embeddings, which may degrade their performance. To\n",
            "fix this, token-level knowledge distillation is employed in [45]\n",
            "along with independent quantization scaling factors for each\n",
            "module due to varying weight distribution. Feature distribu-\n",
            "tions are asymmetric and appear in different channels; outlier\n",
            "suppression [247] shifts and scales per-channel activation dis-\n",
            "tributions for effective quantization. SmoothQuant [44] quan-\n",
            "tizes activations and weights to INT8 format by smoothing\n",
            "activations and migrating the quantization difficulty toward\n",
            "weights. It multiplies the inverse of the smoothing factor with\n",
            "weights, which introduces a few outliers in the weights but is\n",
            "easier to quantify than unsmoothed activations. OPTQ [246]\n",
            "uses the optimal brain compression (OBC) [248] algorithm to\n",
            "quantize the model layer-by-layer and update weights to com-\n",
            "pensate for quantization error.\n",
            "To improve speed and per-\n",
            "formance, OPTQ updates weights in arbitrary order, employs\n",
            "20\n",
            "Figure 14: Illustration of parameter-efficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in\n",
            "the adapter tuning category.\n",
            "lazy updates, and uses better Cholesky kernels. Outlier-aware\n",
            "weight quantization (OWQ) [249] uses the OPTQ algorithm for\n",
            "quantization but assigns higher precision to vulnerable weights,\n",
            "causing outliers and lower precision for others.\n",
            "Quantization-Aware Training:\n",
            "To compensate for perfor-\n",
            "mance degradation,\n",
            "a quantized model is fine-tuned in\n",
            "quantization-aware training (QAT) [250, 251, 252].\n",
            "Al-\n",
            "pha Tuning quantizes the model using binary coding quan-\n",
            "tization (BCQ) [253] and fine-tunes only quantization scal-\n",
            "ing factors.\n",
            "This approach improves performance over\n",
            "parameter-efficient fine-tuning of the pre-trained model. Sim-\n",
            "ilarly, parameter-efficient and quantization-aware adaptation\n",
            "(PEQA) [254] reduces the precision of fully-connected layers\n",
            "and fine-tunes only quantization scaling parameters.\n",
            "LLM-\n",
            "QAT [252] generates training data from the pre-trained network\n",
            "and trains a quantized student model with knowledge distilla-\n",
            "tion. QLoRA [251] fine-tunes 4-bit quantized pre-trained LLM\n",
            "with LoRA [240] using a 4-bit normal float, which shows better\n",
            "performance over a 4-bit integer and float.\n",
            "3.6.3. Pruning\n",
            "Pruning is an alternative approach to quantization to com-\n",
            "press model size, thereby reducing LLMs deployment costs\n",
            "significantly. Compared to task-agnostic pruning, task-specific\n",
            "pruning is easily achievable with good performance, where a\n",
            "model is fine-tuned on the downstream task and pruned for\n",
            "faster inference. It is possible to prune LLMs for individual\n",
            "tasks, but the cost of pruning and deploying task-specific mod-\n",
            "els is high. To overcome this, many structured and unstructured\n",
            "pruning methods for LLMs have been proposed to maintain rea-\n",
            "sonable performance across all tasks while shrinking the model\n",
            "size [255, 42, 256].\n",
            "Unstructured Pruning: This kind of pruning removes less im-\n",
            "portant weights without maintaining any structure.\n",
            "Existing\n",
            "LLM pruning methods take advantage of the unique charac-\n",
            "teristics of LLMs, uncommon for smaller models, where a\n",
            "small subset of hidden states are activated with large magni-\n",
            "tude [245]. Pruning by weights and activations (Wanda) [255]\n",
            "prunes weights in every row based on importance, calculated\n",
            "by multiplying the weights with the norm of input. The pruned\n",
            "model does not require fine-tuning, thereby saving computa-\n",
            "tional costs. Outlier weighed layerwise sparsity (OWL) [257]\n",
            "extends Wanda with non-uniform layer pruning. It shows that\n",
            "the number of outliers varies for different layers; therefore, the\n",
            "model should have variable pruning ratios for better perfor-\n",
            "mance for every layer. Contrastive pruning (CAP) [43] itera-\n",
            "tively prunes the model by training the sparse model using con-\n",
            "trastive loss between pre-trained, fine-tuned, and snapshots of\n",
            "previous sparse models to learn task-specific and task-agnostic\n",
            "knowledge.\n",
            "Structured Pruning: Here, the parameters are removed in\n",
            "groups, rows, columns, or matrices, which speeds up the\n",
            "inference because of effective hardware tensor core utiliza-\n",
            "tion [255].\n",
            "LLM-Pruner [42] employs a 3-stage structured\n",
            "pruning strategy, identifying the groups of hidden states caus-\n",
            "ing each other to activate during the forward-pass, keeping im-\n",
            "portant groups and removing less important ones, and fine-\n",
            "tuning the pruned model with LoRA. Sparsity-induced mask\n",
            "learning (SIMPLE) [258] prunes the network using learnable\n",
            "masks. Similarly, another method prunes LLMs by learning\n",
            "masks and removing unimportant rank-1 components of the\n",
            "factorized weight matrix [256].\n",
            "3.7. Multimodal LLMs\n",
            "Inspired by the success of LLMs in natural language process-\n",
            "ing applications, an increasing number of research works are\n",
            "now facilitating LLMs to perceive different modalities of infor-\n",
            "mation like image [259, 260, 261], video [262, 263, 264], au-\n",
            "dio [265, 264, 266], etc. Multimodal LLMs (MLLMs) present\n",
            "substantial benefits compared to standard LLMs that process\n",
            "only text. By incorporating information from various modal-\n",
            "ities, MLLMs can achieve a deeper understanding of context,\n",
            "leading to more intelligent responses infused with a variety of\n",
            "expressions. Importantly, MLLMs align closely with human\n",
            "perceptual experiences, leveraging the synergistic nature of our\n",
            "multisensory inputs to form a comprehensive understanding of\n",
            "the world [266, 26]. Coupled with a user-friendly interface,\n",
            "MLLMs can offer intuitive, flexible, and adaptable interactions,\n",
            "allowing users to engage with intelligent assistants through a\n",
            "spectrum of input methods. According to the ways of construct-\n",
            "21\n",
            "ing models, current MLLMs can be generally divided into three\n",
            "streams: pre-training, fine-tuning, and prompting. In this sec-\n",
            "tion, we will discuss more details of these main streams, as well\n",
            "as the important application of MLLMs in visual reasoning.\n",
            "Pre-training: This stream of MLLMs intends to support differ-\n",
            "ent modalities using unified end-to-end models. For instance,\n",
            "Flamingo [259] applies gated cross-attention to fuse vision and\n",
            "language modalities, which are collected from pre-trained and\n",
            "frozen visual encoder and LLM, respectively. Moreover, BLIP-\n",
            "2 [260] proposes a two-stage strategy to pre-train a Querying\n",
            "Transformer (Q-Former) for the alignment between vision and\n",
            "language modalities: in the first stage, vision-language repre-\n",
            "sentation learning is bootstrapped from a frozen visual encoder;\n",
            "and in the second stage, a frozen LLM bootstraps vision-to-\n",
            "language generative learning for zero-shot image-to-text gen-\n",
            "eration. Similarly, MiniGPT-4 [267] deploys pre-trained and\n",
            "frozen ViT [268], Q-Former and Vicuna LLM [149], only train-\n",
            "ing the linear projection layer for vision and language modali-\n",
            "ties alignment.\n",
            "Fine-tuning: Derived from instruction tuning [16] for NLP\n",
            "tasks [20, 16, 97], researchers are fine-tune pre-trained LLMs\n",
            "using multimodal instructions. Following this method, LLMs\n",
            "can be easily and effectively extended as multimodal chat-\n",
            "bots [267, 261, 29] and multimodal task solvers [269, 30, 270].\n",
            "The key issue of this stream of MLLMs is to collect multi-\n",
            "modal instruction-following data for fine-tuning [58]. To ad-\n",
            "dress this issue, the solutions of benchmark adaptation [269,\n",
            "271, 272], self-instruction [19, 31, 273], and hybrid composi-\n",
            "tion [274, 270] are employed, respectively. To mitigate the gap\n",
            "between the original language modality and additional modal-\n",
            "ities, the learnable interface is introduced to connect differ-\n",
            "ent modalities from frozen pre-trained models.\n",
            "Particularly,\n",
            "the learnable interface is expected to work in a parameter-\n",
            "efficient tuning manner: e.g., LLaMA-Adapter [275] applies\n",
            "an efficient transformer-based adapter module for training,\n",
            "and LaVIN [274] dynamically learns the multimodal feature\n",
            "weights using a mixture-of-modality adapter. Different from\n",
            "the learnable interface, the expert models can directly convert\n",
            "multimodalities into language: e.g., VideoChat-Text [262] in-\n",
            "corporates Whisper [276], a speech recognition expert model,\n",
            "to generate the captions of given videos for the understanding\n",
            "of following LLMs.\n",
            "Prompting:\n",
            "Different from the fine-tuning technique that\n",
            "directly updates the model parameters given task-specific\n",
            "datasets, the prompting technique provides certain context, ex-\n",
            "amples, or instructions to the model, fulfilling specialized tasks\n",
            "without changing the model parameters. Since prompting can\n",
            "significantly reduce the need for large-scale multimodal data,\n",
            "this technique is widely used to construct MLLMs. Particularly,\n",
            "to solve multimodal Chain of Thought (CoT) problems [103],\n",
            "LLMs are prompted to generate both the reasoning process and\n",
            "the answer given multimodal inputs [277]. On this front, differ-\n",
            "ent learning paradigms are exploited in practice: for example,\n",
            "Multimodal-CoT [277] involves two stages of rationale genera-\n",
            "tion and answer inference, where the input of the second stage\n",
            "is a combination of the original input and the output of the first\n",
            "stage; and CoT-PT [278] applies both prompt tuning and spe-\n",
            "cific visual bias to generate a chain of reasoning implicitly. In\n",
            "addition to CoT problems, LLMs can also be prompted with\n",
            "multimodal descriptions and tools, effectively dividing complex\n",
            "tasks into sub-tasks [279, 280].\n",
            "Visual Reasoning Application: Recent visual reasoning sys-\n",
            "tems [281, 282, 206, 283] tend to apply LLMs for better visual\n",
            "information analysis and visual-language integration. Differ-\n",
            "ent from previous works [284, 285] that rely on limited VQA\n",
            "datasets and small-scale neural networks, current LLM-aided\n",
            "methods offer benefits of stronger generalization ability, emer-\n",
            "gent ability, and interactivity [58]. To realize visual reasoning\n",
            "with the help of LLMs, prompting and fine-tuning techniques\n",
            "can also be utilized: for example, PointClip V2 [282] applies\n",
            "LLMs to generate 3D-specific prompts, which are encoded as\n",
            "textual features and then combined with visual features for\n",
            "3D recognition; and GPT4Tools [31] employs LoRA [240] to\n",
            "fine-tune LLMs following tool-related instructions.\n",
            "Serving\n",
            "as a controller [283], decision maker [286], or semantics re-\n",
            "finer [281, 287], LLMs significantly facilitates the progress of\n",
            "visual reasoning research.\n",
            "3.8. Summary and Discussion\n",
            "3.8.1. Architecture\n",
            "Due to the gigantic scale of LLMs, minor changes in archi-\n",
            "tecture and training strategies have a big impact on performance\n",
            "and stability. Here, we summarize key architectural modules\n",
            "used in various LLMs, leading to better performance, reduced\n",
            "training time and memory, and better training stability.\n",
            "Layer Normalization: The performance and training stability\n",
            "of LLMs are affected significantly by layer normalization. Pre-\n",
            "norm, that is normalizing inputs rather than outputs, is more\n",
            "common among LLMs stabilizing the training [6, 127, 108].\n",
            "BLOOM [13] and AlexaTM [122] utilize an additional layer\n",
            "normalization before embedding layer to stabilize the training\n",
            "of large-scale models, while the model’s zero-shot generaliza-\n",
            "tion ability can be negatively impacted [13]. However, another\n",
            "study [33] finds that pre-norm degrades fine-tuned model per-\n",
            "formance as compared to post-norm, and there are no stability\n",
            "benefits of pre-norm beyond the 100B scale. Therefore, GLM-\n",
            "130B [33] used deep-norm which is a variant of post-norm for\n",
            "better downstream task performance after fine-tuning.\n",
            "Positional Encoding: Like other building blocks of the model,\n",
            "positional encoding also affects the performance and training\n",
            "stability of LLMs.\n",
            "BLOOM [13] finds ALiBi outperforms\n",
            "learned and rotary positional encodings.\n",
            "Contrary to this,\n",
            "GLM-130B [33] identifies rotary positional encoding as being\n",
            "better than ALiBi. So, there is no conclusion in the literature\n",
            "about positional encodings yet.\n",
            "Parallel Attention: In this type of attention, feed-forward and\n",
            "attention layers are parallel to each other rather than sequen-\n",
            "tial in a transformer block. It has been shown to reduce train-\n",
            "ing time by 15%. There is no evidence of performance drop\n",
            "due to this change in the literature and it is used by the models\n",
            "PaLM [15], GPT-NeoX [118], and CodeGen [130].\n",
            "Multi-Query Attention It has shared key and value attention\n",
            "heads in a transformer block while query attention heads are\n",
            "22\n",
            "projected as usual. This reduces memory usage and speeds up\n",
            "sampling in autoregressive decoding. No performance degrada-\n",
            "tion has been observed with this change and it makes the train-\n",
            "ing efficient allowing larger batch sizes. Multi-query attention\n",
            "is used in [15, 132].\n",
            "Mixture of Experts: This type of architecture enables eas-\n",
            "ily scaling models to trillions of parameters [92, 91]. Only a\n",
            "few experts are activated during the computation making them\n",
            "compute-efficient. The performance of MoE models is better\n",
            "than dense models for the same amount of data and requires less\n",
            "computation during fine-tuning to achieve performance similar\n",
            "to dense models as discussed in [91]. MoE architectures are\n",
            "less prone to catastrophic forgetting, therefore are more suited\n",
            "for continual learning [92]. Extracting smaller sub-models for\n",
            "downstream tasks is possible without losing any performance,\n",
            "making MoE architecture hardware-friendly [92].\n",
            "Sparse vs Dense Activated: GPT-3 [6] uses sparse transform-\n",
            "ers [67] whereas GLaM [91] and PanGu-P [92] use MoE [121]\n",
            "architectures to lower computational costs and increase the\n",
            "model size and capacity. According to the literature, sparse\n",
            "modules do not degrade the model’s performance [67]. How-\n",
            "ever, more experiments are required to verify this statement.\n",
            "3.8.2. Training Strategies\n",
            "Training models at a huge scale require tricks to reduce train-\n",
            "ing costs, avoid loss divergence, and achieve better perfor-\n",
            "mance. We summarize and discuss some of these key tricks\n",
            "used in different LLMs.\n",
            "Mixed Precision: It is a famous method for LLMs to reduce\n",
            "memory usage and improve training efficiency. In mixed pre-\n",
            "cision, forward and backward passes are performed in FP16\n",
            "format whereas optimizer states and master weights are kept\n",
            "in FP32 format [120]. A drawback associated with this for-\n",
            "mat change is training instability due to a smaller value range\n",
            "resulting in loss spikes [33]. An alternative to FP16 is BF16\n",
            "which has a comparatively larger range and performs precision-\n",
            "sensitive operations like gradient accumulation and softmax in\n",
            "FP32 [13]. BF16 has better performance and training stability\n",
            "but uses more memory and is supported on specific hardware,\n",
            "for example, A100 GPUs. Therefore, its adoption in LLMs is\n",
            "limited.\n",
            "Training Instability: Loss divergence or spiking is a common\n",
            "issue in LLMs that occurs multiple times during training. This\n",
            "happens in the presence of gradient clipping [15]. To mitigate\n",
            "this problem, many approaches suggest restarting training from\n",
            "an earlier checkpoint [15, 33, 91], skipping 200-500 earlier\n",
            "data batches at the point of divergence in [15] and re-shuffling\n",
            "batches in [91]. The embedding layer gradient shrink proves to\n",
            "further stabilize the training as its gradient norm is significantly\n",
            "larger than the other layers [33]. Another suggestion to improve\n",
            "training stability for larger models is not to use biases in dense\n",
            "and norm layers as in [15].\n",
            "Weight Initialization: It plays a significant role in model con-\n",
            "vergence and training stability.\n",
            "GPT-NeoX [118] initializes\n",
            "feed-forward layers before residuals with\n",
            "2\n",
            "L\n",
            "√\n",
            "d as in [143] and\n",
            "other layers with the small initialization scheme [288]. This\n",
            "avoids activations growing exponentially with increasing depth.\n",
            "MT-NLG [117] found higher variance for weight initialization\n",
            "leads to unstable training, hence validating small initialization\n",
            "scheme [288]. Various models perform random weight initial-\n",
            "ization which can cause bad initialization, Galactica [138] sug-\n",
            "gests a longer warmup to negate the effect.\n",
            "Learning Rate: A suitable learning rate is important for sta-\n",
            "ble training. It is suggested to use a lower value [13, 15, 124]\n",
            "with warmup and decay (cosine or linear). Usually, the learn-\n",
            "ing rate is within the range 1e−4 to 8e−4. Moreover, MT-NLG\n",
            "(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-\n",
            "ing learning rates based on the model size using the GPT-3 [6]\n",
            "models ranging between 13B and 175B. This avoids tuning the\n",
            "learning rate hyperparameter.\n",
            "Training Parallelism: 3D parallelism, a combination of data,\n",
            "pipeline, and tensor parallelism, is the most utilized training\n",
            "parallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].\n",
            "In addition to 3D parallelism, BLOOM [13] uses a zero op-\n",
            "timizer [37] to shard optimizer states.\n",
            "PanGu-α [108] and\n",
            "PanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-\n",
            "lelism which additionally contains optimizer parallelism and\n",
            "rematerialization.\n",
            "Mode Switching: It adds task-related tokens at the beginning\n",
            "of the text during training. These tokens refer to the natural\n",
            "language understanding and natural language generation tasks\n",
            "which are shown to improve downstream task performance\n",
            "in [125, 124, 122]. During fine-tuning and inference, tokens\n",
            "are appended based on the downstream tasks.\n",
            "Controllable Text Generation: Generating credible and con-\n",
            "trolled text from a pre-trained model is challenging. GPT-3 [6]\n",
            "and other LLMs use in-context learning to control generated\n",
            "text. While in-context learning helps in controlling the gener-\n",
            "ated text, ERNIE 3.0 Titan [35] suggests using adversarial loss\n",
            "to rank its generated text for credibility and soft prompts such as\n",
            "genre, topic, keywords, sentiment, and length for better control\n",
            "on generated text.\n",
            "3.8.3. Supervised Models vs Generalized Models\n",
            "Although generalized models are capable of performing di-\n",
            "verse tasks with good performance they have not yet outper-\n",
            "formed models trained in supervised settings. The supervised\n",
            "trained models are still state-of-the-art in various NLP tasks by\n",
            "a large margin as shown in [6, 15, 18].\n",
            "3.8.4. Zero-Shot vs Few-Shot\n",
            "LLMs perform well in zero-shot and few-shot settings. But\n",
            "the performance difference between zero-shot and few-shot is\n",
            "large for pre-trained models [6, 15], naming LLMs as meta-\n",
            "learners [6]. LLMs zero-shot evaluations underperform unsu-\n",
            "pervised methods in neural machine translation [6]. The liter-\n",
            "ature shows pre-training is not enough for good zero-shot per-\n",
            "formance [15, 16]. To improve the zero-shot performance the\n",
            "literature suggests using instruction fine-tuning that improves\n",
            "the zero-shot performance significantly and outperforms base-\n",
            "lines. Instruction fine-tuning has also been shown to improve\n",
            "zero-shot generalization to unseen tasks. Another model, Flan-\n",
            "PaLM [16], unlocks zero-shot reasoning with CoT training.\n",
            "23\n",
            "3.8.5. Encoder vs Decoder vs Encoder-Decoder\n",
            "Traditionally, these architectures perform well for different\n",
            "tasks, for example, encoder-only for NLU tasks, decoder-only\n",
            "for NLG, and encoder-decoder for sequence2sequence model-\n",
            "ing. Encoder-only models are famous for smaller models such\n",
            "as Bert [7], RoBERTa [289], etc., whereas LLMs are either\n",
            "decoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].\n",
            "While decoder-only models are good at NLG tasks, various\n",
            "LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\n",
            "LLaMA [146], are decoder-only models with significant per-\n",
            "formance gains on both NLU and NLG tasks. In contradic-\n",
            "tion to this, T5 [10] and UL2 [125] identify encoder-decoder\n",
            "models out-performing decoder-only models. In another study,\n",
            "PaLM [15] finds increasing the size of decoder-only models\n",
            "can reduce the performance gap between decoder-only and\n",
            "encoder-decoder architectures.\n",
            "Although decoder-only architectures have become a trend for\n",
            "LLMs, many recently proposed approaches [125, 122] use\n",
            "mode-switching tokens in text with encoder-decoder architec-\n",
            "tures to enable task-specific modes. Similarly, CodeT5+ [34]\n",
            "uses an encoder-decoder architecture with multiple training ob-\n",
            "jectives for different tasks, activating the encoder, decoder, or\n",
            "both according to the tasks. These variations in architecture\n",
            "and training objectives allow a model to perform well in differ-\n",
            "ent settings. Because of this dynamic configuration, the future\n",
            "of LLMs can be attributed to encoder-decoder architectures.\n",
            "4. Model Configurations\n",
            "We provide different statistics of pre-trained and instruction-\n",
            "tuned models in this section. This includes information such as\n",
            "publication venue, license type, model creators, steps trained,\n",
            "parallelism, etc in Table 3 and Table 4. Architecture details\n",
            "of pre-trained LLMs are available in Table 5. Providing these\n",
            "details for instruction-tuned models is unnecessary because it\n",
            "fine-tunes pre-trained models for instruction datasets. Hence,\n",
            "architectural details are the same as the baselines. Moreover,\n",
            "optimization settings for various LLMs are available in Table 6\n",
            "and Table 7. We do not include details on precision, warmup,\n",
            "and weight decay in Table 7. These details are not as important\n",
            "as others to mention for instruction-tuned models, and are not\n",
            "provided by the papers.\n",
            "5. Datasets and Evaluation\n",
            "Generating training and evaluation datasets is expensive be-\n",
            "cause of the large-scale data demand of LLMs. Hence, datasets\n",
            "for training and benchmarking these models are topics of key\n",
            "importance. A summary of datasets commonly used by LLMs\n",
            "is provided next.\n",
            "5.1. Training Datasets\n",
            "The performance of LLMs largely depends on the training\n",
            "data’s quality, size, and diversity. Preparing training datasets\n",
            "of high quality at a large scale is laborious. Researchers have\n",
            "suggested various pre-training and fine-tuning datasets to en-\n",
            "hance LLMs capabilities. We summarize these efforts in Ta-\n",
            "ble 8. While numerous training datasets are available in the\n",
            "literature, we cover the most widely used ones in our summary.\n",
            "5.2. Evaluation Datasets and Tasks\n",
            "The evaluation of LLMs is important in gauging their profi-\n",
            "ciency and limitations. This process measures the model’s abil-\n",
            "ity to comprehend, generate, and interact with human language\n",
            "across a spectrum of tasks. Evaluating a language model (LM)\n",
            "is divided into two broader categories: 1) natural language un-\n",
            "derstanding (NLU) and 2) natural language generation (NLG).\n",
            "It is emphasized that tasks in NLU and NLG are softly catego-\n",
            "rized and are often used interchangeably in the literature.\n",
            "Natural Language Understanding: This task measures the lan-\n",
            "guage understanding capacity of LMs. It encompasses multiple\n",
            "tasks, including sentiment analysis, text classification, natural\n",
            "language inference (NLI), question answering (QA), common-\n",
            "sense reasoning (CR), mathematical reasoning (MR), reading\n",
            "comprehension (RC), etc.\n",
            "Natural Language Generation: This task assesses the language\n",
            "generation capabilities of LLMs by understanding the provided\n",
            "input context. It includes tasks such as summarization, sen-\n",
            "tence completion, machine translation (MT), dialogue genera-\n",
            "tion, etc.\n",
            "Numerous datasets are proposed for each task, evaluating\n",
            "LLMs against different characteristics. To provide an overview\n",
            "of evaluation datasets, we briefly discuss a few famous datasets\n",
            "within each category and offer a comprehensive list of datasets\n",
            "in Table 9. Moreover, we show a detailed overview of the train-\n",
            "ing datasets and evaluation tasks and benchmarks used by vari-\n",
            "ous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-\n",
            "ble 11. We also compare the top-performing LLMs in various\n",
            "NLP tasks in Table 12.\n",
            "5.2.1. Multi-task\n",
            "MMLU [297]: A benchmark that measures the knowledge\n",
            "acquired by models during pretraining and evaluates models in\n",
            "zero-shot and few-shot settings across 57 subjects, testing both\n",
            "world knowledge and problem-solving ability.\n",
            "SuperGLUE [2]: A more challenging and diverse successor\n",
            "to the GLUE [299] benchmark, SuperGLUE includes a variety\n",
            "of language understanding tasks, such as question answering,\n",
            "natural language inference, and co-reference resolution. It is\n",
            "designed to provide a rigorous test of language understanding\n",
            "and requires significant progress in areas like sample-efficient,\n",
            "transfer, multi-task, and unsupervised or self-supervised learn-\n",
            "ing.\n",
            "BIG-bench [298]: The BIG-bench (Behavior of Intelligent\n",
            "Generative Models Benchmark) is a large-scale benchmark de-\n",
            "signed to test the abilities of LLMs across a wide range of\n",
            "tasks, including reasoning, creativity, ethics, and understanding\n",
            "of specific domains.\n",
            "GLUE [299]: The General Language Understanding Evalua-\n",
            "tion (GLUE) benchmark is a collection of resources for train-\n",
            "ing, evaluating, and analyzing natural language understanding\n",
            "24\n",
            "Table 3: Summary of pre-trained LLMs (>10B). Only the LLMs discussed individually in the previous sections are summarized. “Data/Tokens” is the model’s\n",
            "pre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics\n",
            "(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs\n",
            "hourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,\n",
            "re-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism\n",
            "(T), pipeline parallelism (P), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column, “DS” is a short form for\n",
            "Deep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.\n",
            "Models\n",
            "Publication\n",
            "Venue\n",
            "License\n",
            "Type\n",
            "Model\n",
            "Creators Purpose\n",
            "No. of\n",
            "Params\n",
            "Commercial\n",
            "Use\n",
            "Steps\n",
            "Trained\n",
            "Data/\n",
            "Tokens\n",
            "Data\n",
            "Cleaning\n",
            "No. of\n",
            "Processing Units\n",
            "Processing\n",
            "Unit Type\n",
            "Training\n",
            "Time\n",
            "Calculated\n",
            "Train. Cost\n",
            "Training\n",
            "Parallelism\n",
            "Library\n",
            "T5 [10]\n",
            "JMLR'20\n",
            "Apache-2.0\n",
            "Google\n",
            "General\n",
            "11B\n",
            "✓\n",
            "1M\n",
            "1T\n",
            "Heur+Dedup\n",
            "1024\n",
            "TPU v3\n",
            "-\n",
            "-\n",
            "D+M\n",
            "Mesh TensorFlow\n",
            "GPT-3 [6]\n",
            "NeurIPS'20\n",
            "-\n",
            "OpenAI\n",
            "General\n",
            "175B\n",
            "×\n",
            "-\n",
            "300B\n",
            "Dedup+QF\n",
            "-\n",
            "V100\n",
            "-\n",
            "-\n",
            "M\n",
            "-\n",
            "mT5 [11]\n",
            "NAACL'21\n",
            "Apache-2.0\n",
            "Google\n",
            "General\n",
            "13B\n",
            "✓\n",
            "1M\n",
            "1T\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "PanGu-α [108]\n",
            "arXiv'21\n",
            "Apache-2.0\n",
            "Huawei\n",
            "General\n",
            "200B\n",
            "✓\n",
            "260k\n",
            "1.1TB\n",
            "Heur+Dedup\n",
            "2048\n",
            "Ascend 910\n",
            "-\n",
            "-\n",
            "D+OP+P+O+R\n",
            "MindSpore\n",
            "CPM-2 [12]\n",
            "AI Open'21\n",
            "MIT\n",
            "Tsinghua\n",
            "General\n",
            "198B\n",
            "✓\n",
            "1M\n",
            "2.6TB\n",
            "Dedup\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "D+M\n",
            "JAXFormer\n",
            "Codex [131]\n",
            "arXiv'21\n",
            "-\n",
            "OpenAI\n",
            "Coding\n",
            "12B\n",
            "×\n",
            "-\n",
            "100B\n",
            "Heur\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "ERNIE 3.0 [110]\n",
            "arXiv'21\n",
            "-\n",
            "Baidu\n",
            "General\n",
            "10B\n",
            "×\n",
            "120k∗\n",
            "375B\n",
            "Heur+Dedup\n",
            "384\n",
            "V100\n",
            "-\n",
            "-\n",
            "M∗\n",
            "PaddlePaddle\n",
            "Jurassic-1 [112]\n",
            "White-Paper'21 Apache-2.0\n",
            "AI21\n",
            "General\n",
            "178B\n",
            "✓\n",
            "-\n",
            "300B\n",
            "-\n",
            "800\n",
            "GPU\n",
            "-\n",
            "-\n",
            "D+M+P\n",
            "Megatron+DS\n",
            "HyperCLOVA [114]\n",
            "EMNLP'21\n",
            "-\n",
            "Naver\n",
            "General\n",
            "82B\n",
            "×\n",
            "-\n",
            "300B\n",
            "Clf+Dedup+PF\n",
            "1024\n",
            "A100\n",
            "321h\n",
            "1.32 Mil\n",
            "M\n",
            "Megatron\n",
            "Yuan 1.0 [115]\n",
            "arXiv'21\n",
            "Apache-2.0\n",
            "-\n",
            "General\n",
            "245B\n",
            "✓\n",
            "26k∗\n",
            "180B Heur+Clf+Dedup\n",
            "2128\n",
            "GPU\n",
            "-\n",
            "-\n",
            "D+T+P\n",
            "-\n",
            "Gopher [116]\n",
            "arXiv'21\n",
            "-\n",
            "Google\n",
            "General\n",
            "280B\n",
            "×\n",
            "-\n",
            "300B\n",
            "QF+Dedup\n",
            "4096\n",
            "TPU v3\n",
            "920h\n",
            "13.19 Mil\n",
            "D+M\n",
            "JAX+Haiku\n",
            "ERNIE 3.0 Titan [35] arXiv'21\n",
            "-\n",
            "Baidu\n",
            "General\n",
            "260B\n",
            "×\n",
            "-\n",
            "300B\n",
            "Heur+Dedup\n",
            "-\n",
            "Ascend 910\n",
            "-\n",
            "-\n",
            "D+M+P+D*\n",
            "PaddlePaddle\n",
            "GPT-NeoX-20B [118] BigScience'22\n",
            "Apache-2.0\n",
            "EleutherAI\n",
            "General\n",
            "20B\n",
            "✓\n",
            "150k\n",
            "825GB\n",
            "None\n",
            "96\n",
            "40G A100\n",
            "-\n",
            "-\n",
            "M\n",
            "Megatron+DS+PyTorch\n",
            "OPT [14]\n",
            "arXiv'22\n",
            "MIT\n",
            "Meta\n",
            "General\n",
            "175B\n",
            "✓\n",
            "150k\n",
            "180B\n",
            "Dedup\n",
            "992\n",
            "80G A100\n",
            "-\n",
            "-\n",
            "D+T\n",
            "Megatron\n",
            "BLOOM [13]\n",
            "arXiv'22\n",
            "RAIL-1.0\n",
            "BigScience\n",
            "General\n",
            "176B\n",
            "✓\n",
            "-\n",
            "366B\n",
            "Dedup+PR\n",
            "384\n",
            "80G A100\n",
            "2520h\n",
            "3.87 Mil\n",
            "D+T+P\n",
            "Megatron+DS\n",
            "Galactica [138]\n",
            "arXiv'22\n",
            "Apache-2.0\n",
            "Meta\n",
            "Science\n",
            "120B\n",
            "×\n",
            "225k\n",
            "106B\n",
            "Dedup\n",
            "128\n",
            "80GB A100\n",
            "-\n",
            "-\n",
            "-\n",
            "Metaseq\n",
            "GLaM [91]\n",
            "ICML'22\n",
            "-\n",
            "Google\n",
            "General\n",
            "1.2T\n",
            "×\n",
            "600k∗\n",
            "600B\n",
            "Clf\n",
            "1024\n",
            "TPU v4\n",
            "-\n",
            "-\n",
            "M\n",
            "GSPMD\n",
            "LaMDA [140]\n",
            "arXiv'22\n",
            "-\n",
            "Google\n",
            "Dialog\n",
            "137B\n",
            "×\n",
            "3M\n",
            "2.81T\n",
            "Filtered\n",
            "1024\n",
            "TPU v3\n",
            "1384h\n",
            "4.96 Mil\n",
            "D+M\n",
            "Lingvo\n",
            "MT-NLG [117]\n",
            "arXiv'22\n",
            "Apache-v2.0 MS.+Nvidia General\n",
            "530B\n",
            "×\n",
            "-\n",
            "270B\n",
            "-\n",
            "4480\n",
            "80G A100\n",
            "-\n",
            "-\n",
            "D+T+P\n",
            "Megatron+DS\n",
            "AlphaCode [132]\n",
            "Science'22\n",
            "Apache-v2.0\n",
            "Google\n",
            "Coding\n",
            "41B\n",
            "✓\n",
            "205k\n",
            "967B\n",
            "Heur+Dedup\n",
            "-\n",
            "TPU v4\n",
            "-\n",
            "-\n",
            "M\n",
            "JAX+Haiku\n",
            "Chinchilla [96]\n",
            "arXiv'22\n",
            "-\n",
            "Google\n",
            "General\n",
            "70B\n",
            "×\n",
            "-\n",
            "1.4T\n",
            "QF+Dedup\n",
            "-\n",
            "TPUv4\n",
            "-\n",
            "-\n",
            "-\n",
            "JAX+Haiku\n",
            "PaLM [15]\n",
            "arXiv'22\n",
            "-\n",
            "Google\n",
            "General\n",
            "540B\n",
            "×\n",
            "255k\n",
            "780B\n",
            "Heur\n",
            "6144\n",
            "TPU v4\n",
            "-\n",
            "-\n",
            "D+M\n",
            "JAX+T5X\n",
            "AlexaTM [122]\n",
            "arXiv'22\n",
            "Apache v2.0\n",
            "Amazon\n",
            "General\n",
            "20B\n",
            "×\n",
            "500k\n",
            "1.1T\n",
            "Filtered\n",
            "128\n",
            "A100\n",
            "2880h\n",
            "1.47 Mil\n",
            "M\n",
            "DS\n",
            "U-PaLM [124]\n",
            "arXiv'22\n",
            "-\n",
            "Google\n",
            "General\n",
            "540B\n",
            "×\n",
            "20k\n",
            "-\n",
            "-\n",
            "512\n",
            "TPU v4\n",
            "120h\n",
            "0.25 Mil\n",
            "-\n",
            "-\n",
            "UL2 [125]\n",
            "ICLR'23\n",
            "Apache-2.0\n",
            "Google\n",
            "General\n",
            "20B\n",
            "✓\n",
            "2M\n",
            "1T\n",
            "-\n",
            "512\n",
            "TPU v4\n",
            "-\n",
            "-\n",
            "M\n",
            "JAX+T5X\n",
            "GLM [33]\n",
            "ICLR'23\n",
            "Apache-2.0\n",
            "Multiple\n",
            "General\n",
            "130B\n",
            "×\n",
            "-\n",
            "400B\n",
            "-\n",
            "768\n",
            "40G A100\n",
            "1440h\n",
            "3.37 Mil\n",
            "M\n",
            "-\n",
            "CodeGen [130]\n",
            "ICLR'23\n",
            "Apache-2.0\n",
            "Salesforce\n",
            "Coding\n",
            "16B\n",
            "✓\n",
            "650k\n",
            "577B\n",
            "Heur+Dedup\n",
            "-\n",
            "TPU v4\n",
            "-\n",
            "-\n",
            "D+M\n",
            "JAXFormer\n",
            "LLaMA [127]\n",
            "arXiv'23\n",
            "-\n",
            "Meta\n",
            "General\n",
            "65B\n",
            "×\n",
            "350k\n",
            "1.4T Clf+Heur+Dedup\n",
            "2048\n",
            "80G A100\n",
            "504h\n",
            "4.12 Mil\n",
            "D+M\n",
            "xFormers\n",
            "PanGuΣ [92]\n",
            "arXiv'23\n",
            "-\n",
            "Huawei\n",
            "General 1.085T\n",
            "×\n",
            "-\n",
            "329B\n",
            "-\n",
            "512\n",
            "Ascend 910\n",
            "2400h\n",
            "-\n",
            "D+OP+P+O+R\n",
            "MindSpore\n",
            "BloombergGPT [141] arXiv23\n",
            "-\n",
            "Bloomberg\n",
            "Finance\n",
            "50B\n",
            "×\n",
            "139k\n",
            "569B\n",
            "Dedup\n",
            "512\n",
            "40G A100\n",
            "1272h\n",
            "1.97 Mil\n",
            "M\n",
            "PyTorch\n",
            "Xuan Yuan 2.0 [142] arXiv23\n",
            "RAIL-1.0\n",
            "Du Xiaoman Finance\n",
            "176B\n",
            "✓\n",
            "-\n",
            "366B\n",
            "Filtered\n",
            "80GB\n",
            "A100\n",
            "-\n",
            "-\n",
            "P\n",
            "DS\n",
            "CodeT5+ [34]\n",
            "arXiv'23\n",
            "BSD-3\n",
            "Salesforce\n",
            "Coding\n",
            "16B\n",
            "✓\n",
            "110k\n",
            "51.5B\n",
            "Dedup\n",
            "16\n",
            "40G A100\n",
            "-\n",
            "-\n",
            "-\n",
            "DS\n",
            "StarCoder [137]\n",
            "arXiv'23\n",
            "OpenRAIL-M BigCode\n",
            "Coding\n",
            "15.5B\n",
            "✓\n",
            "250k\n",
            "1T\n",
            "Dedup+QF+PF\n",
            "512\n",
            "80G A100\n",
            "624h\n",
            "1.28 Mil\n",
            "D+T+P\n",
            "Megatron-LM\n",
            "LLaMA-2 [21]\n",
            "arXiv'23\n",
            "LLaMA-2.0\n",
            "Meta\n",
            "General\n",
            "70B\n",
            "✓\n",
            "500k\n",
            "2T\n",
            "Minimal Filtering\n",
            "-\n",
            "80G A100\n",
            "1.7Mh\n",
            "-\n",
            "-\n",
            "-\n",
            "PaLM-2 [123]\n",
            "arXiv'23\n",
            "-\n",
            "Google\n",
            "General\n",
            "-\n",
            "×\n",
            "-\n",
            "-\n",
            "Ddedup+PF+QF\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "Table 4: Summary of instruction tuned LLMs (>10B). All abbreviations are the same as Table 3. Entries in “Data/Tokens” starting with “S-” represents the number\n",
            "of training samples.\n",
            "Models\n",
            "Publication\n",
            "Venue\n",
            "License\n",
            "Type\n",
            "Model\n",
            "Creators\n",
            "Purpose\n",
            "No. of\n",
            "Params\n",
            "Commercial\n",
            "Use\n",
            "Pre-trained\n",
            "Models\n",
            "Steps\n",
            "Trained\n",
            "Data/\n",
            "Tokens\n",
            "No. of\n",
            "Processing Units\n",
            "Processing\n",
            "Unit Type\n",
            "Train.\n",
            "Time\n",
            "Calculated\n",
            "Train. Cost\n",
            "Train.\n",
            "Parallelism\n",
            "Library\n",
            "WebGPT [156]\n",
            "arXiv'21\n",
            "-\n",
            "OpenAI\n",
            "General\n",
            "175B\n",
            "×\n",
            "GPT-3\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "T0 [17]\n",
            "ICLR'22\n",
            "Apache-2.0\n",
            "BigScience\n",
            "General\n",
            "11B\n",
            "✓\n",
            "T5\n",
            "-\n",
            "250B\n",
            "512\n",
            "TPU v3\n",
            "270h\n",
            "0.48 Mil\n",
            "-\n",
            "-\n",
            "Tk-Instruct [18]\n",
            "EMNLP'22\n",
            "MIT\n",
            "AI2+\n",
            "General\n",
            "11B\n",
            "✓\n",
            "T5\n",
            "1000\n",
            "-\n",
            "256\n",
            "TPU v3\n",
            "4h\n",
            "0.0036 Mil\n",
            "-\n",
            "Google T5\n",
            "OPT-IML [97]\n",
            "arXiv'22\n",
            "-\n",
            "Meta\n",
            "General\n",
            "175B\n",
            "×\n",
            "OPT\n",
            "8k\n",
            "2B\n",
            "128\n",
            "40G A100\n",
            "-\n",
            "-\n",
            "D+T\n",
            "Megatron\n",
            "Flan-U-PaLM [16] ICLR'22\n",
            "Apache-2.0\n",
            "Google\n",
            "General\n",
            "540B\n",
            "✓\n",
            "U-PaLM\n",
            "30k\n",
            "-\n",
            "512\n",
            "TPU v4\n",
            "-\n",
            "-\n",
            "-\n",
            "JAX+T5X\n",
            "mT0 [144]\n",
            "ACL'23\n",
            "Apache-2.0 HuggingFace+ General\n",
            "13B\n",
            "✓\n",
            "mT5\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "Sparrow [157]\n",
            "arXiv'22\n",
            "-\n",
            "Google\n",
            "Dialog\n",
            "70B\n",
            "×\n",
            "Chinchilla\n",
            "-\n",
            "-\n",
            "64\n",
            "TPU v3\n",
            "-\n",
            "-\n",
            "M\n",
            "-\n",
            "WizardCoder [154] arXiv'23\n",
            "Apache-2.0\n",
            "HK Bapt.\n",
            "Coding\n",
            "15B\n",
            "×\n",
            "StarCoder\n",
            "200\n",
            "S-78k\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "Alpaca [148]\n",
            "Github'23\n",
            "Apache-2.0\n",
            "Stanford\n",
            "General\n",
            "13B\n",
            "✓\n",
            "LLaMA\n",
            "3-Epoch\n",
            "S-52k\n",
            "8\n",
            "80G A100\n",
            "3h\n",
            "600\n",
            "FSDP\n",
            "PyTorch\n",
            "Vicuna [149]\n",
            "Github'23\n",
            "Apache-2.0\n",
            "LMSYS\n",
            "General\n",
            "13B\n",
            "✓\n",
            "LLaMA\n",
            "3-Epoch S-125k\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "FSDP\n",
            "PyTorch\n",
            "LIMA [175]\n",
            "arXiv'23\n",
            "-\n",
            "Meta+\n",
            "General\n",
            "65B\n",
            "-\n",
            "LLaMA\n",
            "15-Epoch S-1000\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "Koala [290]\n",
            "Github'23\n",
            "Apache-2.0\n",
            "UC-Berkley\n",
            "General\n",
            "13B\n",
            "×\n",
            "LLaMA\n",
            "2-Epoch S-472k\n",
            "8\n",
            "A100\n",
            "6h\n",
            "100\n",
            "-\n",
            "JAX/FLAX\n",
            "systems. It includes a variety of tasks that test a wide range of\n",
            "linguistic phenomena, making it a comprehensive tool for eval-\n",
            "uating language understanding in AI.\n",
            "5.2.2. Language Understanding\n",
            "WinoGrande [344]: A large-scale dataset inspired by the orig-\n",
            "inal Winograd [347] Schema Challenge tests models on their\n",
            "ability to resolve pronoun ambiguity and encourages the devel-\n",
            "opment of models that understand the broad context in natural\n",
            "language text.\n",
            "CoQA [306]: A conversational question-answering dataset,\n",
            "CoQA challenges models with questions that rely on conver-\n",
            "sation history and require free-form text answers. Its diverse\n",
            "content from seven domains makes it a rigorous test for mod-\n",
            "els’ ability to handle a wide range of topics and conversational\n",
            "contexts.\n",
            "WiC [307]: This dataset assesses a model’s ability to dis-\n",
            "cern word meanings based on context, aiding in tasks related\n",
            "to Word Sense Disambiguation.\n",
            "Wikitext103 [308]:\n",
            "With over 100 million tokens from\n",
            "Wikipedia’s top articles, this dataset is a rich resource for tasks\n",
            "that require understanding long-term dependencies, such as lan-\n",
            "guage modeling and translation.\n",
            "PG19 [309]: This is a digital library of diverse books from\n",
            "Project Gutenberg. It is specifically designed to facilitate re-\n",
            "search in unsupervised learning and language modeling, with a\n",
            "25\n",
            "Table 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the\n",
            "size of hidden states.\n",
            "Models\n",
            "Type\n",
            "Training\n",
            "Objective\n",
            "Attention\n",
            "Vocab\n",
            "Tokenizer\n",
            "Norm\n",
            "PE\n",
            "Activation\n",
            "Bias\n",
            "nL\n",
            "nH\n",
            "HS\n",
            "T5 (11B)\n",
            "Enc-Dec\n",
            "Span Corruption\n",
            "Standard\n",
            "32k\n",
            "SentencePiece\n",
            "Pre-RMS\n",
            "Relative\n",
            "ReLU\n",
            "×\n",
            "24\n",
            "128\n",
            "1024\n",
            "GPT3 (175B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Dense+Sparse\n",
            "-\n",
            "-\n",
            "Layer\n",
            "Learned\n",
            "GeLU\n",
            "✓\n",
            "96\n",
            "96\n",
            "12288\n",
            "mT5 (13B)\n",
            "Enc-Dec\n",
            "Span Corruption\n",
            "Standard\n",
            "250k\n",
            "SentencePiece\n",
            "Pre-RMS\n",
            "Relative\n",
            "ReLU\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "PanGu-α (200B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "40k\n",
            "BPE\n",
            "Layer\n",
            "-\n",
            "-\n",
            "-\n",
            "64\n",
            "128\n",
            "16384\n",
            "CPM-2 (198B)\n",
            "Enc-Dec\n",
            "Span Corruption\n",
            "Standard\n",
            "250k\n",
            "SentencePiece\n",
            "Pre-RMS\n",
            "Relative\n",
            "ReLU\n",
            "-\n",
            "24\n",
            "64\n",
            "-\n",
            "Codex (12B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "-\n",
            "BPE+\n",
            "Pre-Layer\n",
            "Learned\n",
            "GeLU\n",
            "-\n",
            "96\n",
            "96\n",
            "12288\n",
            "ERNIE 3.0 (10B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "-\n",
            "WordPiece\n",
            "Post-Layer\n",
            "Relative\n",
            "GeLU\n",
            "-\n",
            "48\n",
            "64\n",
            "4096\n",
            "Jurassic-1 (178B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "256k\n",
            "SentencePiece∗\n",
            "Pre-Layer\n",
            "Learned\n",
            "GeLU\n",
            "✓\n",
            "76\n",
            "96\n",
            "13824\n",
            "HyperCLOVA (82B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Dense+Sparse\n",
            "-\n",
            "BPE*\n",
            "Pre-Layer\n",
            "Learned\n",
            "GeLU\n",
            "-\n",
            "64\n",
            "80\n",
            "10240\n",
            "Yuan 1.0 (245B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "76\n",
            "-\n",
            "16384\n",
            "Gopher (280B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "32k\n",
            "SentencePiece\n",
            "Pre-RMS\n",
            "Relative\n",
            "GeLU\n",
            "✓\n",
            "80\n",
            "128\n",
            "16384\n",
            "ERNIE 3.0 Titan (260B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "-\n",
            "WordPiece\n",
            "Post-Layer\n",
            "Relative\n",
            "GeLU\n",
            "-\n",
            "48\n",
            "192\n",
            "12288\n",
            "GPT-NeoX-20B\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Parallel\n",
            "50k\n",
            "BPE\n",
            "Layer\n",
            "Rotary\n",
            "GeLU\n",
            "✓\n",
            "44\n",
            "64\n",
            "-\n",
            "OPT (175B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "-\n",
            "BPE\n",
            "-\n",
            "-\n",
            "ReLU\n",
            "✓\n",
            "96\n",
            "96\n",
            "-\n",
            "BLOOM (176B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "250k\n",
            "BPE\n",
            "Layer\n",
            "ALiBi\n",
            "GeLU\n",
            "✓\n",
            "70\n",
            "112\n",
            "14336\n",
            "Galactica (120B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "50k\n",
            "BPE+custom\n",
            "Layer\n",
            "Learned\n",
            "GeLU\n",
            "×\n",
            "96\n",
            "80\n",
            "10240\n",
            "GLaM (1.2T)\n",
            "MoE-Dec\n",
            "Next Token\n",
            "Standard\n",
            "256k\n",
            "SentencePiece\n",
            "Layer\n",
            "Relative\n",
            "GeLU\n",
            "✓\n",
            "64\n",
            "128\n",
            "32768\n",
            "LaMDA (137B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "32k\n",
            "BPE\n",
            "Layer\n",
            "Relative\n",
            "GeGLU\n",
            "-\n",
            "64\n",
            "128\n",
            "8192\n",
            "MT-NLG (530B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "50k\n",
            "BPE\n",
            "Pre-Layer\n",
            "Learned\n",
            "GeLU\n",
            "✓\n",
            "105\n",
            "128\n",
            "20480\n",
            "AlphaCode (41B)\n",
            "Enc-Dec\n",
            "Next Token\n",
            "Multi-query\n",
            "8k\n",
            "SentencePiece\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "64\n",
            "128\n",
            "6144\n",
            "Chinchilla (70B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "32k\n",
            "SentencePiece-NFKC\n",
            "Pre-RMS\n",
            "Relative\n",
            "GeLU\n",
            "✓\n",
            "80\n",
            "64\n",
            "8192\n",
            "PaLM (540B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Parallel+Multi-query\n",
            "256k\n",
            "SentencePiece\n",
            "Layer\n",
            "RoPE\n",
            "SwiGLU\n",
            "×\n",
            "118\n",
            "48\n",
            "18432\n",
            "AlexaTM (20B)\n",
            "Enc-Dec\n",
            "Denoising\n",
            "Standard\n",
            "150k\n",
            "SentencePiece\n",
            "Pre-Layer\n",
            "Learned\n",
            "GeLU\n",
            "✓\n",
            "78\n",
            "32\n",
            "4096\n",
            "Sparrow (70B)\n",
            "Causal-Dec\n",
            "Pref.&Rule RM\n",
            "-\n",
            "32k\n",
            "SentencePiece-NFKC\n",
            "Pre-RMS\n",
            "Relative\n",
            "GeLU\n",
            "✓\n",
            "16∗\n",
            "64\n",
            "8192\n",
            "U-PaLM (540B)\n",
            "Non-Causal-Dec\n",
            "MoD\n",
            "Parallel+Multi-query\n",
            "256k\n",
            "SentencePiece\n",
            "Layer\n",
            "RoPE\n",
            "SwiGLU\n",
            "×\n",
            "118\n",
            "48\n",
            "18432\n",
            "UL2 (20B)\n",
            "Enc-Dec\n",
            "MoD\n",
            "Standard\n",
            "32k\n",
            "SentencePiece\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "64\n",
            "16\n",
            "4096\n",
            "GLM (130B)\n",
            "Non-Causal-Dec\n",
            "AR Blank Infilling\n",
            "Standard\n",
            "130k\n",
            "SentencePiece\n",
            "Deep\n",
            "RoPE\n",
            "GeGLU\n",
            "✓\n",
            "70\n",
            "96\n",
            "12288\n",
            "CodeGen (16B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Parallel\n",
            "-\n",
            "BPE\n",
            "Layer\n",
            "RoPE\n",
            "-\n",
            "-\n",
            "34\n",
            "24\n",
            "-\n",
            "LLaMA (65B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "32k\n",
            "BPE\n",
            "Pre-RMS\n",
            "RoPE\n",
            "SwiGLU\n",
            "-\n",
            "80\n",
            "64\n",
            "8192\n",
            "PanGu-Σ (1085B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "-\n",
            "BPE\n",
            "Fused Layer\n",
            "-\n",
            "FastGeLU\n",
            "-\n",
            "40\n",
            "40\n",
            "5120\n",
            "BloombergGPT (50B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Standard\n",
            "131k\n",
            "Unigram\n",
            "Layer\n",
            "ALiBi\n",
            "GeLU\n",
            "✓\n",
            "70\n",
            "40\n",
            "7680\n",
            "Xuan Yuan 2.0 (176B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Self\n",
            "250k\n",
            "BPE\n",
            "Layer\n",
            "ALiBi\n",
            "GeLU\n",
            "✓\n",
            "70\n",
            "112\n",
            "14336\n",
            "CodeT5+ (16B)\n",
            "Enc-Dec\n",
            "SC+NT+Cont.+Match\n",
            "Standard\n",
            "-\n",
            "Code-Specific\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "StarCoder (15.5B)\n",
            "Causal-Dec\n",
            "FIM\n",
            "Multi-query\n",
            "49k\n",
            "BPE\n",
            "-\n",
            "Learned\n",
            "-\n",
            "-\n",
            "40\n",
            "48\n",
            "6144\n",
            "LLaMA (70B)\n",
            "Causal-Dec\n",
            "Next Token\n",
            "Grouped-query\n",
            "32k\n",
            "BPE\n",
            "Pre-RMS\n",
            "RoPE\n",
            "SwiGLUE\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "PaLM-2\n",
            "-\n",
            "MoD\n",
            "Parallel\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "special focus on long-form content.\n",
            "C4 [10]: A clean, multilingual dataset, C4 offers billions of to-\n",
            "kens from web-crawled data. It is a comprehensive resource for\n",
            "training advanced Transformer models on various languages.\n",
            "LCQMC [310]: The Large-scale Chinese Question Matching\n",
            "Corpus (LCQMC) is a dataset for evaluating the performance\n",
            "of models in semantic matching tasks. It contains pairs of ques-\n",
            "tions in Chinese and their matching status, making it a valuable\n",
            "resource for research in Chinese language understanding.\n",
            "5.2.3. Story Cloze and Sentence Completion\n",
            "StoryCloze [324]: It introduces a new “StoryCloze Test”, a\n",
            "commonsense reasoning framework for evaluating story under-\n",
            "standing, generation, and script learning. It considers a model’s\n",
            "ability to understand and generate coherent and sensible stories.\n",
            "LAMBADA [325]: This dataset evaluates contextual text un-\n",
            "derstanding through a word prediction task. Models must pre-\n",
            "dict the last word of a passage, which is easy for humans when\n",
            "given the whole passage, but not when given only the last sen-\n",
            "tence.\n",
            "5.2.4. Physical Knowledge and World Understanding\n",
            "PIQA [330]: A dataset that probes the physical knowledge of\n",
            "models, aiming to understand how well they are learning about\n",
            "the real world.\n",
            "TriviaQA [331]: A dataset that tests models on reading com-\n",
            "prehension and open domain question answering (QA) tasks,\n",
            "with a focus on Information Retrieval (IR)-style QA.\n",
            "ARC [332]: A larger version of the ARC-Challenge, this\n",
            "dataset contains both easy and challenging grade-school level,\n",
            "multiple-choice science questions. It is a comprehensive test of\n",
            "a model’s ability to understand and answer complex questions.\n",
            "ARC-Easy [332]:\n",
            "A subset of the ARC dataset, ARC-\n",
            "Easy, contains questions that are answered correctly by either\n",
            "a retrieval-based algorithm or a word co-occurrence algorithm.\n",
            "It is a great starting point for models beginning to explore ad-\n",
            "vanced question-answering.\n",
            "ARC-Challenge [332]:\n",
            "A rigorous question-answering\n",
            "dataset, ARC-Challenge includes complex, grade-school level\n",
            "questions that demand reasoning beyond simple retrieval, test-\n",
            "ing the true comprehension capabilities of models.\n",
            "5.2.5. Contextual Language Understanding\n",
            "RACE [337]: The RACE dataset is a reading comprehension\n",
            "dataset collected from English examinations in China, which\n",
            "benchmarks AI models for understanding and answering ques-\n",
            "tions on long and complex passages, simulating the challenge\n",
            "of a real-world examination.\n",
            "RACE-Middle [337]: Another subset of the RACE [337]\n",
            "dataset, RACE-Middle, contains middle school-level English\n",
            "exam questions. It offers a slightly less challenging but academ-\n",
            "ically oriented evaluation of a model’s comprehension skills.\n",
            "RACE-High [337]: A subset of the RACE [337] dataset,\n",
            "RACE-High consists of high school-level English exam ques-\n",
            "tions. It is designed to evaluate the comprehension ability of\n",
            "models in a more academic and challenging context.\n",
            "26\n",
            "Table 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,\n",
            "for most of the LLMs.\n",
            "Sequence\n",
            "LR\n",
            "Optimizers\n",
            "Precision\n",
            "Weight\n",
            "Grad\n",
            "Models\n",
            "Batch Size\n",
            "Length\n",
            "LR\n",
            "Warmup\n",
            "Decay\n",
            "AdaFactor\n",
            "Adam AdamWFP16 BF16 Mixed Decay\n",
            "Clip\n",
            "Dropout\n",
            "T5 (11B)\n",
            "211\n",
            "512\n",
            "0.01\n",
            "×\n",
            "inverse square root\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "GPT3 (175B)\n",
            "32K\n",
            "-\n",
            "6e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "mT5 (13B)\n",
            "1024\n",
            "1024\n",
            "0.01\n",
            "-\n",
            "inverse square root\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "PanGu-α (200B)\n",
            "-\n",
            "1024\n",
            "2e-5\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "CPM-2 (198B)\n",
            "1024\n",
            "1024\n",
            "0.001\n",
            "-\n",
            "-\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "Codex (12B)\n",
            "-\n",
            "-\n",
            "6e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "-\n",
            "ERNIE 3.0 (12B)\n",
            "6144\n",
            "512\n",
            "1e-4\n",
            "✓\n",
            "linear\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "-\n",
            "-\n",
            "Jurassic-1 (178B)\n",
            "3.2M\n",
            "2048\n",
            "6e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "HyperCLOVA (82B)\n",
            "1024\n",
            "-\n",
            "6e-5\n",
            "-\n",
            "cosine\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "-\n",
            "-\n",
            "Yuan 1.0 (245B)\n",
            "<10M\n",
            "2048\n",
            "1.6e-4\n",
            "✓\n",
            "cosine decay to 10%\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "-\n",
            "-\n",
            "Gopher (280B)\n",
            "3M\n",
            "2048\n",
            "4e-5\n",
            "✓\n",
            "cosine decay to 10%\n",
            "✓\n",
            "✓\n",
            "-\n",
            "✓\n",
            "-\n",
            "ERNIE 3.0 Titan (260B)\n",
            "-\n",
            "512\n",
            "1e-4\n",
            "✓\n",
            "linear\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "GPT-NeoX-20B\n",
            "1538\n",
            "2048\n",
            "0.97e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "×\n",
            "OPT (175B)\n",
            "2M\n",
            "2048\n",
            "1.2e-4\n",
            "-\n",
            "linear\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "BLOOM (176B)\n",
            "2048\n",
            "2048\n",
            "6e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "×\n",
            "Galactica (120B)\n",
            "2M\n",
            "2048\n",
            "7e-6\n",
            "✓\n",
            "linear decay to 10%\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "GLaM (1.2T)\n",
            "1M\n",
            "1024\n",
            "0.01\n",
            "-\n",
            "inverse square root\n",
            "✓\n",
            "FP32 + ✓\n",
            "-\n",
            "✓\n",
            "×\n",
            "LaMDA (137B)\n",
            "256K\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "MT-NLG (530B)\n",
            "1920\n",
            "2048\n",
            "5e-5\n",
            "✓\n",
            "cosine decay to 10%\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "AlphaCode (41B)\n",
            "2048\n",
            "1536+768\n",
            "1e-4\n",
            "✓\n",
            "cosine decay to 10%\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "Chinchilla (70B)\n",
            "1.5M\n",
            "2048\n",
            "1e-4\n",
            "✓\n",
            "cosine decay to 10%\n",
            "✓\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "PaLM (540B)\n",
            "2048\n",
            "2048\n",
            "0.01\n",
            "-\n",
            "inverse square root\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "✓\n",
            "×\n",
            "AlexaTM (20B)\n",
            "2M\n",
            "1024\n",
            "1e-4\n",
            "-\n",
            "linear decay to 5%\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "✓\n",
            "U-PaLM (540B)\n",
            "32\n",
            "2048\n",
            "1e-4\n",
            "-\n",
            "cosine\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "UL2 (20B)\n",
            "1024\n",
            "1024\n",
            "-\n",
            "-\n",
            "inverse square root\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "×\n",
            "-\n",
            "-\n",
            "GLM (130B)\n",
            "4224\n",
            "2048\n",
            "8e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "CodeGen (16B)\n",
            "2M\n",
            "2048\n",
            "5e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "✓\n",
            "-\n",
            "LLaMA (65B)\n",
            "4M Tokens\n",
            "2048\n",
            "1.5e-4\n",
            "✓\n",
            "cosine decay to 10%\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "✓\n",
            "✓\n",
            "-\n",
            "PanGu-Σ (1.085T)\n",
            "512\n",
            "1024\n",
            "2e-5\n",
            "✓\n",
            "-\n",
            "✓\n",
            "✓\n",
            "-\n",
            "-\n",
            "-\n",
            "BloombergGPT (50B)\n",
            "2048\n",
            "2048\n",
            "6e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "×\n",
            "Xuan Yuan 2.0 (176B)\n",
            "2048\n",
            "2048\n",
            "6e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "CodeT5+ (16B)\n",
            "2048\n",
            "1024\n",
            "2e-4\n",
            "-\n",
            "linear\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "-\n",
            "StarCoder (15.5B)\n",
            "512\n",
            "8k\n",
            "3e-4\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "-\n",
            "LLaMA-2 (70B)\n",
            "4M Tokens\n",
            "4k\n",
            "1.5e-4\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "-\n",
            "Table 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while\n",
            "no model uses weight decay for instruction tuning.\n",
            "Sequence\n",
            "Optimizers\n",
            "Grad\n",
            "Models\n",
            "Batch Size\n",
            "Length\n",
            "LR\n",
            "Warmup\n",
            "LR_Decay\n",
            "AdaFactor\n",
            "Adam\n",
            "AdamW\n",
            "Clip\n",
            "Dropout\n",
            "WebGPT (175B)\n",
            "BC:512, RM:32\n",
            "-\n",
            "6e-5\n",
            "-\n",
            "-\n",
            "✓\n",
            "-\n",
            "-\n",
            "T0 (11B)\n",
            "1024\n",
            "1280\n",
            "1e-3\n",
            "-\n",
            "-\n",
            "✓\n",
            "-\n",
            "✓\n",
            "Tk-Instruct (11B)\n",
            "1024\n",
            "-\n",
            "1e-5\n",
            "-\n",
            "constant\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "OPT-IML (175B)\n",
            "128\n",
            "2048\n",
            "5e-5\n",
            "×\n",
            "linear\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "Flan-U-PaLM (540B)\n",
            "32\n",
            "-\n",
            "1e-3\n",
            "-\n",
            "constant\n",
            "✓\n",
            "-\n",
            "✓\n",
            "Sparrow (70B)\n",
            "RM: 8+16, RL:16\n",
            "-\n",
            "2e-6\n",
            "✓\n",
            "cosine decay to 10%\n",
            "✓\n",
            "✓\n",
            "×\n",
            "WizardCoder (15B)\n",
            "512\n",
            "2048\n",
            "2e-5\n",
            "✓\n",
            "cosine\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "Alpaca (13B)\n",
            "128\n",
            "512\n",
            "1e-5\n",
            "✓\n",
            "cosine\n",
            "-\n",
            "-\n",
            "✓\n",
            "✓\n",
            "×\n",
            "Vicuna (13B)\n",
            "128\n",
            "-2048\n",
            "2e-5\n",
            "✓\n",
            "cosine\n",
            "✓\n",
            "-\n",
            "×\n",
            "LIMA (65B)\n",
            "32\n",
            "2048\n",
            "1e-5\n",
            "×\n",
            "linear\n",
            "✓\n",
            "-\n",
            "✓\n",
            "QuAC [338]: This dataset simulates an information-seeking\n",
            "dialog between students and teachers using hidden Wikipedia\n",
            "text. It introduces unique challenges not found in machine com-\n",
            "prehension datasets, making it a valuable resource for advanc-\n",
            "ing dialog systems.\n",
            "5.2.6. Commonsense Reasoning\n",
            "HellaSwag [345]: A dataset that challenges models to pick the\n",
            "best ending to a context uses Adversarial Filtering to create a\n",
            "‘Goldilocks’ zone of complexity, where generated text is absurd\n",
            "to humans but often misclassified by models.\n",
            "COPA [391]: This dataset evaluates a model’s progress in\n",
            "open-domain commonsense causal reasoning. Each question\n",
            "comprises a premise and two alternatives, and the model must\n",
            "select the more plausible alternative, testing a model’s ability to\n",
            "understand and reason about cause and effect.\n",
            "WSC [347]: The Winograd Schema Challenge (WSC) is a\n",
            "reading comprehension task in which a system must resolve\n",
            "references in a text, often requiring world knowledge and rea-\n",
            "soning about the text.\n",
            "CSQA [348]: The CommonsenseQA is a question-answering\n",
            "dataset that requires commonsense knowledge to evaluate the\n",
            "ability of AI models to understand and answer questions.\n",
            "27\n",
            "Table 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\n",
            "Dataset\n",
            "Type\n",
            "Size/Samples\n",
            "Tasks\n",
            "Source\n",
            "Creation\n",
            "Comments\n",
            "C4 [10]\n",
            "Pretrain\n",
            "806GB\n",
            "-\n",
            "Common Crawl\n",
            "Automated\n",
            "A clean, multilingual dataset with billions\n",
            "of tokens\n",
            "mC4 [11]\n",
            "Pretrain\n",
            "38.49TB\n",
            "-\n",
            "Common Crawl\n",
            "Automated\n",
            "A multilingual extension of the C4\n",
            "dataset, mC4 identifies over 100 lan-\n",
            "guages using cld3 from 71 monthly web\n",
            "scrapes of Common Crawl.\n",
            "PILE [291]\n",
            "Pretrain\n",
            "825GB\n",
            "-\n",
            "Common Crawl, PubMed Central,\n",
            "OpenWebText2, ArXiv, GitHub,\n",
            "Books3, and others\n",
            "Automated\n",
            "A massive dataset comprised of 22 con-\n",
            "stituent sub-datasets\n",
            "ROOTs [292]\n",
            "Pretrain\n",
            "1.61TB\n",
            "-\n",
            "498 Hugging Face datasets\n",
            "Automated\n",
            "46 natural and 13 programming lan-\n",
            "guages\n",
            "MassiveText [116]\n",
            "Pretrain\n",
            "10.5TB\n",
            "-\n",
            "MassiveWeb, Books, News,\n",
            "Wikipedia, Github, C4\n",
            "Automated\n",
            "99% of the data is in English\n",
            "Wikipedia [293]\n",
            "Pretrain\n",
            "-\n",
            "-\n",
            "Wikipedia\n",
            "Automated\n",
            "Dump of wikipedia\n",
            "RedPajama [294]\n",
            "Pretrain\n",
            "5TB\n",
            "-\n",
            "CommonCrawl, C4, Wikipedia,\n",
            "Github, Books, StackExchange\n",
            "Automated\n",
            "Open-source replica of LLaMA dataset\n",
            "PushShift.io Reddit\n",
            "Pretrain\n",
            "21.1GB\n",
            "-\n",
            "Reddit\n",
            "Automated\n",
            "Submissions and comments on Reddit\n",
            "from 2005 to 2019\n",
            "BigPython [130]\n",
            "Pretrain\n",
            "5.5TB\n",
            "Coding\n",
            "GitHub\n",
            "Automated\n",
            "-\n",
            "Pool of Prompt (P3) [17]\n",
            "Instructions\n",
            "12M\n",
            "62\n",
            "PromptSource\n",
            "Manual\n",
            "A Subset of PromptSource, created from\n",
            "177 datasets including summarization,\n",
            "QA, classification, etc.\n",
            "xP3 [144]\n",
            "Instructions\n",
            "81M\n",
            "71\n",
            "P3+Multilingual datasets\n",
            "Manual\n",
            "Extending P3 to total 46 languages\n",
            "Super-NaturalInstructions (SNI) [18]\n",
            "Instructions\n",
            "12.4M\n",
            "1616\n",
            "Multiple datasets\n",
            "Manual\n",
            "Extending P3 with additional multi-\n",
            "lingual datasets, total 46 languages\n",
            "Flan [16]\n",
            "Instructions\n",
            "15M\n",
            "1836\n",
            "Muffin+T0-SF+NIV2\n",
            "Manual\n",
            "Total 60 languages\n",
            "OPT-IML [97]\n",
            "Instructions\n",
            "18.1M\n",
            "1667\n",
            "-\n",
            "Manual\n",
            "-\n",
            "Self-Instruct [19]\n",
            "Instructions\n",
            "82k\n",
            "175\n",
            "-\n",
            "Automated\n",
            "Generated 52k instructions with 82k sam-\n",
            "ples from 175 seed tasks using GPT-3\n",
            "Alpaca [148]\n",
            "Instructions\n",
            "52k\n",
            "-\n",
            "-\n",
            "Automated\n",
            "Employed self-instruct method to gener-\n",
            "ate data from text-davinci-003\n",
            "Vicuna [149]\n",
            "Instructions\n",
            "125k\n",
            "-\n",
            "ShareGPT\n",
            "Automated\n",
            "Conversations\n",
            "shared\n",
            "by\n",
            "users\n",
            "on\n",
            "ShareGPT using public APIs\n",
            "LLaMA-GPT-4 [150]\n",
            "Instructions\n",
            "52k\n",
            "-\n",
            "Alpaca\n",
            "Automated\n",
            "Recreated Alpaca dataset with GPT-4 in\n",
            "English and Chinese\n",
            "Unnatural Instructions [295]\n",
            "Instructions\n",
            "68k\n",
            "-\n",
            "15-Seeds (SNI)\n",
            "Automated\n",
            "-\n",
            "LIMA [175]\n",
            "Instructions\n",
            "1k\n",
            "-\n",
            "Multiple datasets\n",
            "Manual\n",
            "Carefully created samples to test perfor-\n",
            "mance with fine-tuning on less data\n",
            "Anthropic-HH-RLHF [296]\n",
            "Alignment\n",
            "142k\n",
            "-\n",
            "-\n",
            "Manual\n",
            "Anthropic-HH-RLHF-2 [168]\n",
            "Alignment\n",
            "39k\n",
            "-\n",
            "-\n",
            "Manual\n",
            "5.2.7. Reading Comprehension\n",
            "BoolQ [353]: A dataset derived from Google search queries,\n",
            "BoolQ challenges models to answer binary (yes/no) questions.\n",
            "The questions are naturally occurring and are paired with a\n",
            "paragraph from a Wikipedia article containing the answer. It\n",
            "is a test of reading comprehension and reasoning.\n",
            "SQUADv2 [354]: The Stanford Question Answering Dataset\n",
            "(SQuAD) [352] is a collection of questions posed by crowd\n",
            "workers on a set of Wikipedia articles, where the answer to ev-\n",
            "ery question is a segment of text from the corresponding reading\n",
            "passage. SQuADv2 combines the original SQuAD1.1 dataset\n",
            "with over 50,000 unanswerable questions. The aim is to evalu-\n",
            "ate a model’s ability to understand and answer questions based\n",
            "on a given context and to determine when a question is unan-\n",
            "swerable.\n",
            "DROP [355]: DROP, or Discrete Reasoning Over the con-\n",
            "tent of Paragraphs, is designed to test a model’s ability to un-\n",
            "derstand a wide variety of reading phenomena. It encourages\n",
            "comprehensive and reliable evaluation of reading comprehen-\n",
            "sion capabilities.\n",
            "RTE [356]:\n",
            "The Recognizing Textual Entailment (RTE)\n",
            "datasets come from a series of annual competitions on textual\n",
            "entailment, predicting whether a given sentence logically fol-\n",
            "lows from another and evaluating a model’s understanding of\n",
            "logical relationships in a text.\n",
            "WebQA [357]: A dataset for open-domain question answering,\n",
            "WebQA offers a large collection of web-based question-answer\n",
            "pairs. It is designed to assess the ability of AI models to under-\n",
            "stand and answer questions based on web content.\n",
            "CMRC2018 [359]: This dataset is a test of Chinese language\n",
            "models’ ability to reason comprehensively and is designed with\n",
            "a challenging span-extraction format that pushes the boundaries\n",
            "of machine performance.\n",
            "5.2.8. Mathematical Reasoning\n",
            "MATH [372]: This dataset is a platform for evaluating the\n",
            "mathematical problem-solving abilities of AI models. It con-\n",
            "tains a diverse set of math problems, ranging from arithmetic\n",
            "to calculus, and is designed to test the model’s ability to under-\n",
            "stand and solve complex mathematical problems.\n",
            "Math23k [373]: This one challenges a model’s ability to un-\n",
            "derstand and solve mathematical word problems. It contains\n",
            "23,000 Chinese arithmetic word problems that require models\n",
            "to perform reasoning and computation based on the problem\n",
            "28\n",
            "Table 9: Categorized evaluation datasets used in evaluating LLMs.\n",
            "Type\n",
            "Datasets/Benchmarks\n",
            "Multi-Task\n",
            "MMLU [297], SuperGLUE [2], BIG-bench [298], GLUE [299], BBH [298], CUGE [300], Zero-\n",
            "CLUE [301], FewCLUE [302], Blended Skill Talk [303], HELM [304], KLUE-STS [305]\n",
            "Language Understanding\n",
            "CoQA [306], WiC [307], Wikitext103 [308], PG19 [309], LCQMC [310], QQP [311], WinoGender [312],\n",
            "CB [313], FinRE [314], SanWen [315], AFQMC [301], BQ Corpus [316], CNSS [317], CKBQA 13 [318],\n",
            "CLUENER [301], Weibo [319], AQuA [320], OntoNotes [321], HeadQA [322], Twitter Dataset [323]\n",
            "Story Cloze and\n",
            "Sentence Completion\n",
            "StoryCloze [324], LAMBADA [325], LCSTS [326], AdGen [327], E2E [328], CHID [329], CHID-\n",
            "FC [302]\n",
            "Physical Knowledge and\n",
            "World Understanding\n",
            "PIQA [330], TriviaQA [331], ARC [332], ARC-Easy [332], ARC-Challenge [332], PROST [333], Open-\n",
            "BookQA [334], WebNLG [335], DogWhistle Insider & Outsider [336]\n",
            "Contextual Language\n",
            "Understanding\n",
            "RACE [337], RACE-Middle [337], RACE-High [337], QuAC [338], StrategyQA [339], Quiz Bowl [340],\n",
            "cMedQA [341],cMedQA2 [342], MATINF-QA [343]\n",
            "Commonsense Reasoning\n",
            "WinoGrande [344], HellaSwag [345], COPA [346], WSC [347], CSQA [348], SIQA [349], C3 [350],\n",
            "CLUEWSC2020 [301], CLUEWSC [301], CLUEWSC-FC [302], ReCoRD [351]\n",
            "Reading Comprehension\n",
            "SQuAD [352], BoolQ [353], SQUADv2 [354], DROP [355], RTE [356], WebQA [357], CMRC2017 [358],\n",
            "CMRC2018 [359], CMRC2019 [360], COTE-BD [361], COTE-DP [361], COTE-MFW [361], Mul-\n",
            "tiRC [362], Natural Questions [363], CNSE [317], DRCD [364], DuReader [365], Dureaderrobust [366],\n",
            "DuReader-QG [365], SciQ [367], Sogou-log [368], Dureaderrobust-QG [366], QA4MRE [369], KorQuAD\n",
            "1.0 [370], CAIL2018-Task1 & Task2 [371]\n",
            "Mathematical Reasoning\n",
            "MATH [372], Math23k [373], GSM8K [374], MathQA [375], MGSM [376], MultiArith [377], AS-\n",
            "Div [378], MAWPS [379], SVAMP [380]\n",
            "Problem Solving\n",
            "HumanEval [131], DS-1000 [381], MBPP [382], APPS [372], CodeContests [132]\n",
            "Natural Language Inference\n",
            "& Logical Reasoning\n",
            "ANLI [383], MNLI-m [384], MNLI-mm [384],QNLI [352], WNLI [347], OCNLI [301], CMNLI [301],\n",
            "ANLI R1 [383], ANLI R2 [383], ANLI R3 [383], HANS [385], OCNLI-FC [302], LogiQA [386], Strate-\n",
            "gyQA [339]\n",
            "Cross-Lingual Understanding\n",
            "MLQA [387], XNLI [388], PAWS-X [389], XSum [390], XCOPA [391], XWinograd [392], TyDiQA-\n",
            "GoldP [393], MLSum [394]\n",
            "Truthfulness and Fact Checking\n",
            "TruthfulQA [395], MultiFC [396], Fact Checking on Fever [397]\n",
            "Biases and Ethics in AI\n",
            "ETHOS [398], StereoSet [399], BBQ [400], Winobias [401], CrowS-Pairs [402]\n",
            "Toxicity\n",
            "RealToxicityPrompts [403], CivilComments toxicity classification [404]\n",
            "Language Translation\n",
            "WMT [405], WMT20 [406], WMT20-enzh [406], EPRSTMT [302], CCPM [407]\n",
            "Scientific Knowledge\n",
            "AminoProbe [138], BioLAMA [138], Chemical Reactions [138], Galaxy Clusters [138], Mineral\n",
            "Groups [138]\n",
            "Dialogue\n",
            "Wizard of Wikipedia [408], Empathetic Dialogues [409], DPC-generated [96] dialogues, ConvAI2 [410],\n",
            "KdConv [411]\n",
            "Topic Classification\n",
            "TNEWS-FC [302], YNAT [305], KLUE-TC [305], CSL [301], CSL-FC [302], IFLYTEK [412]\n",
            "description.\n",
            "GSM8K [374]: A dataset of diverse grade school math word\n",
            "problems, testing a model’s ability to perform multi-step math-\n",
            "ematical reasoning.\n",
            "5.2.9. Problem Solving and Logical Reasoning\n",
            "ANLI [383]: A large-scale dataset designed to test the robust-\n",
            "ness of machine learning models in Natural Language Inference\n",
            "(NLI) is created through an iterative, adversarial process where\n",
            "humans try to generate examples that models cannot correctly\n",
            "classify.\n",
            "HumanEval [131]: A dataset for evaluating the problem-\n",
            "solving ability of AI models, which includes a diverse set of\n",
            "tasks that require various cognitive abilities, making it a com-\n",
            "prehensive tool for assessing general intelligence in AI.\n",
            "StrategyQA [339]: A question-answering dataset that re-\n",
            "quires reasoning over multiple pieces of evidence to evaluate\n",
            "the strategic reasoning ability of AI models, pushing the bound-\n",
            "aries of what machines can understand and answer.\n",
            "5.2.10. Cross-Lingual Understanding\n",
            "XNLI [388]: A cross-lingual benchmark, XNLI extends the\n",
            "MultiNLI [419] corpus to 15 languages, including low-resource\n",
            "ones like Urdu. It tests models on cross-lingual sentence under-\n",
            "standing, with 112,500 annotated pairs across three categories:\n",
            "entailment, contradiction, and neutral.\n",
            "PAWS-X [389]: PAWS-X, or Cross-lingual Paraphrase Adver-\n",
            "saries from Word Scrambling, is a multilingual version of the\n",
            "PAWS [420] dataset for paraphrase identification. It includes\n",
            "examples in seven languages and is designed to evaluate the\n",
            "performance of cross-lingual paraphrase identification models.\n",
            "5.2.11. Truthfulness\n",
            "Truthful-QA [395]: A unique benchmark that measures a\n",
            "language model’s truthfulness when generating answers. The\n",
            "dataset includes questions across various categories like health,\n",
            "law, and politics, some designed to test the model against com-\n",
            "mon human misconceptions.\n",
            "29\n",
            "Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI”\n",
            "is natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,\n",
            "“Mem.” is memorization.\n",
            "Benchmark\n",
            "Models\n",
            "Training Dataset\n",
            "BIG-\n",
            "bench\n",
            "MMLU\n",
            "Super\n",
            "GLUE\n",
            "QA\n",
            "Clf\n",
            "NLI\n",
            "MT\n",
            "Cloze/\n",
            "Completion\n",
            "RC\n",
            "CR\n",
            "MR\n",
            "Coding\n",
            "Truthful/\n",
            "Bias/\n",
            "Toxicity/\n",
            "Mem.\n",
            "T5\n",
            "C4 [10]\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "GPT-3\n",
            "Common Crawl, WebText, Books Cor-\n",
            "pora, Wikipedia\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "mT5\n",
            "mC4 [11]\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "PanGu-α\n",
            "1.1TB Chinese Text Corpus\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "CPM-2\n",
            "WuDaoCorpus [109]\n",
            "✓\n",
            "✓\n",
            "Codex\n",
            "54 million public repositories from Github\n",
            "✓\n",
            "ERNIE-3.0\n",
            "Chinese text corpora, Baidu Search, Web\n",
            "text, QA-long, QA-short, Poetry and Cou-\n",
            "plet Domain-specific data from medical,\n",
            "law, and financial area Baidu knowledge\n",
            "graph with more than 50 million facts\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "Jurassic-1\n",
            "Wikipedia, OWT, Books, C4, Pile [291],\n",
            "arXiv, GitHub\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "HyperCLOVA\n",
            "Korean blogs, Community sites, News,\n",
            "KiN Korean Wikipedia, Wikipedia (En-\n",
            "glish and Japanese), Modu-Corpus: Mes-\n",
            "senger, News, Spoken and written lan-\n",
            "guage corpus, Web corpus\n",
            "✓\n",
            "Yuan 1.0\n",
            "Common Crawl, SogouT, Sogou News,\n",
            "Baidu Baike, Wikipedia, Books\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "Gopher\n",
            "subsets of MassiveWeb Books, C4, News,\n",
            "GitHub and Wikipedia samples from Mas-\n",
            "siveText\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "ERNIE-3.0 TITAN\n",
            "Same as ERNIE 3.0 and ERNIE 3.0 ad-\n",
            "versarial dataset, ERNIE 3.0 controllable\n",
            "dataset\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "GPT-NeoX-20B\n",
            "Pile [291]\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "OPT\n",
            "RoBERTa [289], Pile [291], PushShift.io\n",
            "Reddit [413]\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "BLOOM\n",
            "ROOTs [13]\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "Galactica\n",
            "arXiv, PMC, Semantic Scholar, Wikipedia,\n",
            "StackExchange, LibreText, Open Text-\n",
            "books, RefSeq Genome, OEIS, LIPID\n",
            "MAPS, NASAExoplanet, Common Crawl,\n",
            "ScientificCC, AcademicCC, GitHub repos-\n",
            "itories Khan Problems, GSM8K, OneS-\n",
            "mallStep\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "GLaM\n",
            "Filtered Webpages, Social media conversa-\n",
            "tions Wikipedia, Forums, Books, News\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "LaMDA\n",
            "Infiniset : Public documents, Dialogs, Ut-\n",
            "terances\n",
            "✓\n",
            "MT-NLG\n",
            "Two snapshots of Common Crawl and\n",
            "Books3, OpenWebText2, Stack Exchange,\n",
            "PubMed Abstracts,\n",
            "Wikipedia,\n",
            "PG-19\n",
            "[242], BookCorpus2, NIH ExPorter, Pile,\n",
            "CC-Stories, RealNews\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "AlphaCode\n",
            "Selected GitHub repositories, CodeCon-\n",
            "tests: Codeforces, Description2Code, Co-\n",
            "deNet\n",
            "✓\n",
            "Chinchilla\n",
            "MassiveWeb,\n",
            "MassiveText Books,\n",
            "C4,\n",
            "News, GitHub, Wikipedia\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "PaLM\n",
            "webpages, books, Wikipedia, news, arti-\n",
            "cles, source code, social media conversa-\n",
            "tions\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "AlexaTM\n",
            "Wikipedia, mC4\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "U-PaLM\n",
            "Same as PaLM\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "UL2\n",
            "-\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "GLM-130B\n",
            "-\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "CodeGen\n",
            "Pile, BigQuery, BigPython\n",
            "✓\n",
            "LLaMA\n",
            "CommonCrawl, C4, Github, Wikipedia,\n",
            "Books, arXiv, StackExchange\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "PanGu-Σ\n",
            "WuDaoCorpora, CLUE, Pile, C4, Python\n",
            "code\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "BloombergGPT\n",
            "inPile, Pile, C4, Wikipedia\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "CodeT5+\n",
            "CodeSearchNet, Github Code\n",
            "✓\n",
            "✓\n",
            "StarCoder\n",
            "The Stack v1.2\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "LLaMA-2\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "PaLM-2\n",
            "Web documents, Code, Books, Maths,\n",
            "Conversation\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "30\n",
            "Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\n",
            "Models\n",
            "Training Dataset\n",
            "BIG-\n",
            "bench\n",
            "MMLU\n",
            "BBH\n",
            "RAFT\n",
            "FLAN\n",
            "SNI\n",
            "PromptSource\n",
            "TyDiQA\n",
            "HumanEval\n",
            "MBPP\n",
            "Truthful/\n",
            "Bias/\n",
            "Toxicity\n",
            "T0\n",
            "Pool of Prompts\n",
            "✓\n",
            "WebGPT\n",
            "ELI5\n",
            "[414],\n",
            "ELI5\n",
            "fact-\n",
            "check\n",
            "[156],\n",
            "TriviaQA\n",
            "[331],\n",
            "ARC-Challenge\n",
            "[332],\n",
            "ARC-\n",
            "Easy\n",
            "[332],\n",
            "Hand-written\n",
            "data,\n",
            "Demonstrations of humans, Com-\n",
            "parisons between model-generated\n",
            "answers\n",
            "✓\n",
            "Tk-INSTRUCT\n",
            "SNI [18]\n",
            "✓\n",
            "mT0\n",
            "xP3 [144]\n",
            "OPT-IML\n",
            "PromptSource [17], FLAN [16],\n",
            "SNI\n",
            "[415],\n",
            "UnifiedSKG\n",
            "[416],\n",
            "CrossFit\n",
            "[417],\n",
            "ExMix\n",
            "[418],\n",
            "T5 [10], Reasoning\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "Flan\n",
            "Muffin, T0-SF, NIv2, CoT\n",
            "✓\n",
            "✓\n",
            "✓\n",
            "WizardCoder\n",
            "Code Alpaca\n",
            "✓\n",
            "✓\n",
            "5.2.12. Biases and Ethics in AI\n",
            "ETHOS [398]: ETHOS is a hate speech detection dataset\n",
            "built from YouTube and Reddit comments. It is a tool in the\n",
            "fight against online hate speech, offering binary and multi-label\n",
            "variants for robust content moderation.\n",
            "StereoSet [399]: StereoSet is a comprehensive dataset de-\n",
            "signed to measure and evaluate the presence of stereotypical\n",
            "biases in language models. It focuses on four key domains:\n",
            "gender, profession, race, and religion. Contrasting stereotypi-\n",
            "cal bias against language modeling ability provides a valuable\n",
            "tool for understanding and mitigating biases in large language\n",
            "models.\n",
            "6. Applications\n",
            "Applying Large Language Models (LLMs) to a variety of\n",
            "downstream tasks has become a popular trend in both AI-\n",
            "related research communities and industries, with many emerg-\n",
            "ing uses being discovered and explored daily. LLMs, which are\n",
            "capable of understanding and generating human-like text, have\n",
            "found meaningful applications across a variety of fields. This\n",
            "section provides an overview of LLM applications in medicine,\n",
            "education, science, mathematics, law, finance, robotics, and\n",
            "coding. While each of these domains pose different challenges,\n",
            "LLMs open up opportunities to make significant contributions\n",
            "to these domains through their generalizability.\n",
            "General Purpose:\n",
            "LLMs are being widely considered as\n",
            "general-purpose tools for a wide variety of tasks [421]. This\n",
            "is due to their inherent ability to understand, generate, and\n",
            "manipulate human-like text in a contextually relevant man-\n",
            "ner. This allows them to perform tasks ranging from simple\n",
            "language translation and question-answering to more complex\n",
            "tasks like summarization, text generation, and even program-\n",
            "ming help [422]. The utility of LLMs is further enhanced by\n",
            "their ability to adapt to the specific style and tone of the text\n",
            "they are processing, making the outputs more user-friendly and\n",
            "context-aware. In everyday applications, LLMs can be used as\n",
            "personal assistants, helping users draft emails or schedule ap-\n",
            "pointments [423]; they can also be deployed in customer ser-\n",
            "vice to handle common questions; or applied to generate con-\n",
            "tent for digital platforms like websites, by creating human-like\n",
            "text based on given prompts [424]. Moreover, LLMs play a cru-\n",
            "cial role in data analysis, where they can filter large volumes of\n",
            "text data, summarize key points, and find patterns that would\n",
            "take humans much longer to identify [425]. Despite their wide-\n",
            "ranging applications, it is essential to remember that LLMs,\n",
            "similar to any AI system, are only as good as the data they have\n",
            "been trained on.\n",
            "Medicine: The application of LLMs in the field of medicine is\n",
            "reshaping healthcare delivery and research. For example, LLMs\n",
            "are increasingly used in clinical decision support systems to\n",
            "provide physicians with evidence-based treatment recommen-\n",
            "dations [426, 427, 428]. By analyzing patient data and medical\n",
            "literature, they can help identify potential diagnoses, suggest\n",
            "appropriate tests, and recommend optimal treatment strategies.\n",
            "Moreover, LLMs can also enhance patient interactions with\n",
            "healthcare systems; e.g., they can be used in chatbot applica-\n",
            "tions [429, 430, 431] to answer patient queries about symptoms\n",
            "or medications, schedule appointments, and even provide es-\n",
            "sential health advice. For medical research, LLMs are used to\n",
            "extract and filter information from a considerable amount of\n",
            "medical literature, identify relevant studies, summarize find-\n",
            "ings, and even predict future research trends [432, 433, 434].\n",
            "For medical education, LLMs can help create training mate-\n",
            "rials, generate exam questions, provide detailed explanations\n",
            "of complex medical topics, and offer personalized feedback to\n",
            "students [435, 436, 437, 438]. They can also simulate patient\n",
            "interactions, enabling students to practice and improve their\n",
            "clinical skills. At a broader level, LLMs can assist in public\n",
            "health initiatives by analyzing media data to detect disease out-\n",
            "breaks, monitor public sentiment towards health policies, and\n",
            "disseminate health information in a clear and understandable\n",
            "manner [439]. LLMs can be employed to support public health\n",
            "initiatives, addressing related issues such as data privacy, the\n",
            "necessity for explainability, and the potential risk of propagat-\n",
            "ing biases [440, 441].\n",
            "Education: The integration of LLMs into the educational sec-\n",
            "tor offers opportunities to enhance learning experiences, teacher\n",
            "31\n",
            "Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided\n",
            "to the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the\n",
            "benchmark.\n",
            "Task\n",
            "Dataset/Benchmark\n",
            "Top-1\n",
            "Top-2\n",
            "Top-3\n",
            "Model (Size)\n",
            "Score (N-shots)\n",
            "Model (Size)\n",
            "Score (N-shots)\n",
            "Model (Size)\n",
            "Score (N-shots)\n",
            "Multi-Task\n",
            "BIG-bench (B)\n",
            "Chinchilla (70B)\n",
            "65.1 (5-shot)\n",
            "Gopher (280B)\n",
            "53.97 (5-shot)\n",
            "PaLM (540B)\n",
            "53.7 (5-shot)\n",
            "MMLU (B)\n",
            "GPT-4 (-)\n",
            "86.4 (5-shot)\n",
            "Gemini (Ultra)\n",
            "83.7 (5-shot)\n",
            "Flan-PaLM-2( f) (Large)\n",
            "81.2 (5-shot)\n",
            "Language Understanding\n",
            "SuperGLUE (B)\n",
            "ERNIE 3.0 (12B)\n",
            "90.6 (-)\n",
            "PaLM(f) (540B)\n",
            "90.4 (-)\n",
            "T5 (11B)\n",
            "88.9 (-)\n",
            "Story Comprehension and\n",
            "Generation\n",
            "HellaSwag\n",
            "GPT-4 (-)\n",
            "95.3 (10-shot)\n",
            "Gemini (Ultra)\n",
            "87.8 (10-shot)\n",
            "PaLM-2 (Large)\n",
            "86.8 (one shot)\n",
            "StoryCloze\n",
            "GPT3 (175B)\n",
            "87.7 (few shot)\n",
            "PaLM-2 (Large)\n",
            "87.4 (one shot)\n",
            "OPT (175B)\n",
            "79.82 (-)\n",
            "Physical Knowledge and\n",
            "World Understanding\n",
            "PIQA\n",
            "PaLM-2 (Large)\n",
            "85.0 (one shot)\n",
            "LLaMa (65B)\n",
            "82.8 (zero shot)\n",
            "MT-NLG (530B)\n",
            "81.99 (zero shot)\n",
            "TriviaQA\n",
            "PaLM-2 (Large)\n",
            "86.1 (one shot)\n",
            "LLaMA-2 (70B)\n",
            "85.0 (one shot)\n",
            "PaLM (540B)\n",
            "81.4 (one shot)\n",
            "Contextual Language\n",
            "Understanding\n",
            "LAMBADA\n",
            "PaLM (540B)\n",
            "89.7 (few shot)\n",
            "MT-NLG (530B)\n",
            "87.15 (few shot)\n",
            "PaLM-2 (Large)\n",
            "86.9 (one shot)\n",
            "Commonsense Reasoning\n",
            "WinoGrande\n",
            "GPT-4 (-)\n",
            "87.5 (5-shot)\n",
            "PaLM-2 (Large)\n",
            "83.0 (one shot)\n",
            "PaLM (540B)\n",
            "81.1 (zero shot)\n",
            "SIQA\n",
            "LLaMA (65B)\n",
            "52.3 (zero shot)\n",
            "Chinchilla (70B)\n",
            "51.3 (zero shot)\n",
            "Gopher (280B)\n",
            "50.6 (zero shot)\n",
            "Reading Comprehension\n",
            "BoolQ\n",
            "PaLM(f) (540B)\n",
            "92.2 (-)\n",
            "T5 (11B)\n",
            "91.2 (-)\n",
            "PaLM-2 (Large)\n",
            "90.9 (one shot)\n",
            "Truthfulness\n",
            "Truthful-QA\n",
            "LLaMA (65B)\n",
            "57 (-)\n",
            "Mathematical Reasoning\n",
            "MATH\n",
            "Gemini (Ultra)\n",
            "53.2 (4-shot)\n",
            "PaLM-2 (Large)\n",
            "34.3 (4-shot)\n",
            "LLaMa-2 (65B)\n",
            "13.5 (4-shot)\n",
            "GSM8K\n",
            "GPT-4 (-)\n",
            "92.0 (5-shot)\n",
            "PaLM-2 (Large)\n",
            "80.7 (8-shot)\n",
            "U-PaLM (540B)\n",
            "58.5 (-)\n",
            "Problem Solving and\n",
            "Logical Reasoning\n",
            "HumanEval\n",
            "Gemini( f) (Ultra)\n",
            "74.4 (zero shot)\n",
            "GPT-4 (-)\n",
            "67.0 (zero shot)\n",
            "Code Llama (34B)\n",
            "48.8 (zero shot)\n",
            "support, and educational content development. For students, by\n",
            "analyzing their learning styles, performance, and preferences,\n",
            "LLMs can provide customized study materials and practice\n",
            "questions to develop personalized learning experiences [442].\n",
            "For teachers, LLMs can help to create lesson plans and grade\n",
            "assignments and generate diverse and inclusive educational\n",
            "content, significantly saving more time for teaching and student\n",
            "interaction [443, 444]. In language learning, LLMs serve as\n",
            "advanced conversational partners capable of simulating conver-\n",
            "sations in multiple languages, correcting grammar, enhancing\n",
            "vocabulary, and aiding pronunciation for the needs of fluency\n",
            "in practice [445]. Furthermore, LLMs improve accessibility\n",
            "in education by providing support for students with disabili-\n",
            "ties. They can generate real-time transcriptions for the hear-\n",
            "ing impaired, offer reading assistance for the visually impaired,\n",
            "and simplify complex texts for those with learning disabili-\n",
            "ties [441]. As LLMs continue to evolve, their applications in\n",
            "education can benefit more students and teachers from different\n",
            "perspectives in practice.\n",
            "Science: Similar to medical applications, LLMs can expedite\n",
            "the research process by quickly analyzing and summarizing sci-\n",
            "entific literature. By briefing comprehensible and accessible re-\n",
            "search summaries, LLMs can assist researchers in staying up-\n",
            "to-date with the latest findings, even in fields outside their area\n",
            "of expertise [446, 447]. In addition, LLMs can aid scientists\n",
            "in formulating new hypotheses and research questions since\n",
            "their ability to process large-scale datasets allows them to un-\n",
            "veil insights that might not be immediately apparent to human\n",
            "researchers [448]. Moreover, for scientific writing, LLMs can\n",
            "help researchers draft documents, suggest improvements, and\n",
            "ensure adherence to specific formatting guidelines [449, 450].\n",
            "This not only saves time but also improves the clarity of scien-\n",
            "tific communication, enabling interdisciplinary teams to work\n",
            "together more effectively.\n",
            "Maths: In addition to providing mathematical research and\n",
            "education support, LLMs can assist in solving mathematical\n",
            "problems by giving step-by-step explanations and guiding users\n",
            "through complex proofs and calculations. They can help iden-\n",
            "tify errors in reasoning or computation and suggest corrections,\n",
            "serving as an invaluable tool for both learning and verification\n",
            "purposes [451, 452]. LLMs can be employed to check the valid-\n",
            "ity of mathematical proofs, offering a preliminary filter before\n",
            "human review. While they are not a substitute for the meticu-\n",
            "lous work of mathematicians, they can help simplify the process\n",
            "of proof verification [453, 454]. Moreover, LLMs enhance ac-\n",
            "cessibility to mathematics by translating complex concepts and\n",
            "findings into understandable language for non-specialists [455],\n",
            "where the gap between theoretical mathematics and applied\n",
            "contexts such as physics, engineering, and economics can be\n",
            "bridged.\n",
            "Law: LLMs can assist with the thematic analysis of legal doc-\n",
            "uments, including generating initial coding for datasets, iden-\n",
            "tifying themes, and classifying data according to these themes.\n",
            "This collaborative effort between legal experts and LLMs has\n",
            "proved to be effective in analyzing legal texts such as court\n",
            "opinions on theft, improving both the efficiency and quality of\n",
            "the research [456]. Additionally, LLMs have been evaluated for\n",
            "their ability to generate explanations of legal terms, focusing\n",
            "on improving factual accuracy and relevance by incorporating\n",
            "sentences from case law. By feeding relevant case law into the\n",
            "LLM, the augmented models can generate higher-quality expla-\n",
            "nations with less factually incorrect information [457]. More-\n",
            "over, LLMs can be trained with specialized domain knowledge\n",
            "to perform legal reasoning tasks [458] and answer legal ques-\n",
            "tions [459].\n",
            "Finance: LLMs like BloombergGPT [141], trained on exten-\n",
            "sive proprietary financial datasets, exhibit superior performance\n",
            "on financial tasks. This indicates the value of domain-specific\n",
            "training in creating LLMs that can more accurately understand\n",
            "and process industry-specific language and concepts. The intro-\n",
            "duction of FinGPT [460] as an open-source model offers trans-\n",
            "parent and accessible resources to develop novel applications\n",
            "such as robo-advising, algorithmic trading, and low-code so-\n",
            "lutions, ultimately expanding the capabilities of financial ser-\n",
            "vices. Both BloombergGPT and FinGPT show the adaptabil-\n",
            "ity of LLMs to the financial domain, with the former showing\n",
            "32\n",
            "the power of custom datasets and the latter emphasizing a data-\n",
            "centric approach and low-rank adaptation techniques for cus-\n",
            "tomization. Moreover, LLMs demonstrate an ability to break\n",
            "down complex financial tasks into actionable plans, enabling\n",
            "end-to-end solutions that were previously unfeasible with a sin-\n",
            "gle model [461].\n",
            "Robotics: In robotics research, LLMs have promising appli-\n",
            "cations, such as enhancing human-robot interaction [28, 462,\n",
            "463, 464], task planning [227], motion planning [236], nav-\n",
            "igation [236, 465], object manipulation [226], personalized\n",
            "robots [466], etc. LLMs enable robots to understand the en-\n",
            "vironment effectively and generate plans to complete tasks col-\n",
            "laboratively [230, 26]. They can facilitate continuous learning\n",
            "by allowing robots to access and integrate information from a\n",
            "wide range of sources, helping robots acquire new skills, adapt\n",
            "to changes, and refine their paths [214, 223, 224].\n",
            "7. Challenges and Future Directions\n",
            "LLMs such as GPT-4 and its predecessors have significantly\n",
            "advanced natural language processing. Nevertheless, they also\n",
            "bring along a set of challenges. The computational cost, ad-\n",
            "versarial robustness, and interpretability are among the tech-\n",
            "nical challenges that are intrinsic to these models.\n",
            "Further-\n",
            "more, as these models are scaled up to handle more complex\n",
            "tasks or to operate in more complex or dynamic environments,\n",
            "new challenges in scalability, privacy, and real-time processing\n",
            "emerge. On the frontier of foundational research, integrating\n",
            "multi-modality and the effectiveness of transfer learning are be-\n",
            "ing keenly explored. Additionally, the continuous learning as-\n",
            "pect of these models, which aims to have models that can adapt\n",
            "to new information over time, presents a fresh set of challenges.\n",
            "These challenges not only underscore the technical intricacies\n",
            "involved but also highlight the broader impact and the future\n",
            "trajectory of LLMs in real-world applications. The following\n",
            "sections delve into these challenges, shedding light on the on-\n",
            "going and potential efforts to address them.\n",
            "Computational Cost: Training LLMs requires extensive com-\n",
            "putational resources, which increases production costs and\n",
            "raises environmental concerns due to substantial energy con-\n",
            "sumption during large-scale training. Improved performance\n",
            "occurs as computational resources increase, but the rate of\n",
            "improvement gradually decreases when both the model and\n",
            "dataset size remain fixed, following the power law of dimin-\n",
            "ishing returns [467].\n",
            "Bias and Fairness: LLMs can inherit and amplify societal bi-\n",
            "ases in their training data. These biases can manifest in the\n",
            "model’s outputs, leading to potential ethical and fairness is-\n",
            "sues [468].\n",
            "Overfitting: Although LLMs possess substantial learning ca-\n",
            "pabilities, they are susceptible to overfitting noisy and peculiar\n",
            "patterns within their extensive training data. Consequently, this\n",
            "may cause them to generate illogical responses [469]. The de-\n",
            "bate about Memorization vs. Generalization in LLMs is about\n",
            "finding the right balance. Memorization allows the model to\n",
            "remember specific details from its training data, ensuring it can\n",
            "provide accurate answers to precise questions. However, gen-\n",
            "eralization enables the model to make inferences and produce\n",
            "responses for inputs it has not seen before, which is essential\n",
            "for handling various real-world tasks. Striking the right bal-\n",
            "ance is the challenge: too much memorization can lead to over-\n",
            "fitting, making the model inflexible and struggling with new\n",
            "inputs [470].\n",
            "Economic and Research Inequality: The high cost of train-\n",
            "ing and deploying LLMs may make their development concen-\n",
            "trated within well-funded organizations, potentially worsening\n",
            "economic and research inequalities in AI [471].\n",
            "Reasoning and Planning: Some reasoning and planning tasks,\n",
            "even as seemingly simple as common-sense planning, which\n",
            "humans find easy, remain well beyond the current capabilities\n",
            "of LLMs evaluated using an assessment framework. This is not\n",
            "entirely unexpected, considering that LLMs primarily generate\n",
            "text completions based on likelihood and offer no solid guaran-\n",
            "tees in terms of reasoning abilities [472].\n",
            "Hallucinations: LLMs exhibit “hallucinations\", where they\n",
            "generate responses that, while sounding plausible, are incor-\n",
            "rect or do not align with the provided information [473]. The\n",
            "hallucination can be categorized into three categories.\n",
            "• Input-conflicting hallucination, wherein LLMs produce\n",
            "content that diverges from the input given by users.\n",
            "• Context-conflicting hallucination, where LLMs generate\n",
            "content that contradicts information they have generated\n",
            "earlier.\n",
            "• Fact-conflicting hallucination involves LLM’s generation\n",
            "of content that does not align with established world\n",
            "knowledge.\n",
            "Prompt Engineering: Prompts serve as inputs to LLMs, and\n",
            "their syntax and semantics play a crucial role in determining\n",
            "the model’s output. The prompt variations, sometimes counter-\n",
            "intuitive to humans, can result in significant changes in model\n",
            "output and are addressed through prompt engineering, which\n",
            "involves designing natural language queries to guide LLMs\n",
            "responses effectively [474, 32].\n",
            "Limited Knowledge: Information acquired during pretraining\n",
            "is limited and may become obsolete after some time.\n",
            "Re-\n",
            "training the model using updated data is costly. To generate\n",
            "factually accurate responses people use a retrieval augmen-\n",
            "tation pipeline [188].\n",
            "However, pre-trained models are not\n",
            "trained with retrieval augmentation generation (RAG) [6, 21],\n",
            "hence, adapting the training pipeline is necessary [183, 25].\n",
            "Safety and Controllability: Using LLMs comes with the risk\n",
            "of generating harmful, misleading, or inappropriate content,\n",
            "whether by accident or when given specific prompts. Ensuring\n",
            "these models are safely utilized is a significant concern [475].\n",
            "Multi-Modality:\n",
            "Multi-modal learning, where LLMs are\n",
            "trained on diverse data like text, images, and videos, aims to\n",
            "create models with richer understanding but faces challenges\n",
            "in data alignment, fusion strategies, and higher computational\n",
            "demands.\n",
            "Catastrophic Forgetting: LLMs are often pre-trained on large\n",
            "33\n",
            "datasets and then fine-tuned on domain-specific data, reducing\n",
            "training resources but facing issues like domain adaptation and\n",
            "catastrophic forgetting, which hinders the retention of original\n",
            "knowledge when learning new tasks.\n",
            "Adversarial Robustness:\n",
            "Large Language Models (LLMs)\n",
            "have shown great capabilities in various tasks but are vul-\n",
            "nerable to adversarial attacks, where slight, deliberate input\n",
            "alterations can mislead them.\n",
            "Especially with models like\n",
            "BERT, adversarial fine-tuning can enhance robustness, al-\n",
            "though it sometimes compromises generalization [476].\n",
            "As\n",
            "LLMs integrate more into complex systems, examining their\n",
            "security properties becomes crucial, given the emerging field\n",
            "of adversarial attacks on LLMs within trustworthy ML [477].\n",
            "This vulnerability is notable in safety-critical domains, ne-\n",
            "cessitating robust adversarial evaluation tools to ensure LLM\n",
            "reliability [478].\n",
            "Interpretability and Explainability: The \"black-box\" nature\n",
            "of LLMs poses challenges in understanding their decision-\n",
            "making, which is crucial for broader acceptance and trust,\n",
            "especially in sensitive domains.\n",
            "Despite their advanced\n",
            "capabilities, the lack of insight into their operation limits their\n",
            "effectiveness and trustworthiness [479, 480]. Efforts are being\n",
            "made to make LLMs more explainable to promote user trust\n",
            "and to ensure responsible AI usage. Understanding the logic\n",
            "behind LLMs’ responses is essential for fostering trust and\n",
            "ensuring they align with human values and legal standards.\n",
            "Privacy Concerns:\n",
            "Privacy concerns in Large Language\n",
            "Models (LLMs) have escalated with their growth in complexity\n",
            "and size, particularly around data sharing and potential misuse.\n",
            "There is a risk of malicious content creation, filter bypass,\n",
            "and data privacy issues, especially in e-commerce, where\n",
            "protecting customer privacy is crucial. If models are trained\n",
            "on private data, additional concerns arise if such models are\n",
            "made publicly available. LLMs tend to memorize phrases from\n",
            "their training sets, which an adversary could exploit to extract\n",
            "sensitive data, posing a threat to personal privacy [481, 482].\n",
            "Real-Time Processing: Real-time processing in Large Lan-\n",
            "guage Models (LLMs) is pivotal for various applications,\n",
            "especially with the rising popularity of mobile AI applications\n",
            "and concerns regarding information security and privacy.\n",
            "However, LLMs often have hundreds of layers and millions\n",
            "of parameters, which impede real-time processing due to the\n",
            "high computational demands and limited weight storage on\n",
            "hardware platforms, particularly in edge computing environ-\n",
            "ments [483].\n",
            "While certain efforts like MobileBERT aim\n",
            "to reduce memory requirements, they still face substantial\n",
            "execution overhead due to the large number of model layers,\n",
            "leading to high inference latency.\n",
            "Long-Term Dependencies: Large Language Models (LLMs)\n",
            "have shown considerable progress in understanding and\n",
            "generating text, yet they often struggle with preserving context\n",
            "and handling long-term dependencies, particularly in complex,\n",
            "multi-turn conversations or long documents. This limitation\n",
            "can lead to incoherent or irrelevant responses.\n",
            "Hardware Acceleration: The growth of LLMs presents signif-\n",
            "icant hardware challenges due to the increasing computational\n",
            "and memory demands associated with training and deploying\n",
            "these models. GPUs have played a crucial role in meeting the\n",
            "hardware requirements for training LLMs, with the networking\n",
            "industry also evolving to optimize hardware for training\n",
            "workloads. However, the growing size of LLMs, which has\n",
            "been outpacing hardware progress, makes model inference in-\n",
            "creasingly costly. Model quantization is a promising approach\n",
            "to bridge the widening gap between LLM size and hardware\n",
            "capacity [484].\n",
            "Although specialized hardware acceleration\n",
            "like GPUs or TPUs can significantly reduce the computational\n",
            "cost, making real-time applications more feasible, they may not\n",
            "fully resolve all limitations, necessitating further advancements\n",
            "in hardware technology.\n",
            "Regulatory and Ethical Frameworks: The rapid advancements\n",
            "in artificial intelligence have given rise to sophisticated Large\n",
            "Language Models (LLMs) like OpenAI’s GPT-4 [147] and\n",
            "Google’s Bard. These developments underscore the imperative\n",
            "for regulatory oversight to manage the ethical and social\n",
            "challenges accompanying LLMs’ widespread use [485]. For\n",
            "instance, LLMs can generate content that can be used posi-\n",
            "tively or negatively, emphasizing the need for proactive ethical\n",
            "frameworks and policy measures to guide their responsible\n",
            "use and assign accountability for their outputs [486]. Auditing\n",
            "is identified as a promising governance mechanism to ensure\n",
            "that AI systems, including LLMs, are designed and deployed\n",
            "ethically, legally, and technically robust [487].\n",
            "8. Conclusion\n",
            "This article has reviewed the developments on LLMs com-\n",
            "prehensively. It contributes to summarizing significant find-\n",
            "ings of LLMs in the existing literature and provides a de-\n",
            "tailed analysis of the design aspects, including architectures,\n",
            "datasets, and training pipelines. We identified crucial archi-\n",
            "tectural components and training strategies employed by dif-\n",
            "ferent LLMs. These aspects are presented as summaries and\n",
            "discussions throughout the article.\n",
            "Moreover, we have dis-\n",
            "cussed the performance differences of LLMs in zero-shot and\n",
            "few-shot settings, explored the impact of fine-tuning, and com-\n",
            "pared supervised and generalized models and encoder vs. de-\n",
            "coder vs. encoder-decoder architectures. A comprehensive re-\n",
            "view of multi-modal LLMs, retrieval augmented LLMs, LLMs-\n",
            "powered agents, efficient LLMs, datasets, evaluation, applica-\n",
            "tions, and challenges is also provided. This article is anticipated\n",
            "to serve as a valuable resource for researchers, offering insights\n",
            "into the recent advancements in LLMs and providing funda-\n",
            "mental concepts and details to develop better LLMs.\n",
            "References\n",
            "[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his-\n",
            "tory” for natural language processing?, in:\n",
            "Machine Learning and\n",
            "Knowledge Discovery in Databases. Research Track: European Con-\n",
            "ference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021,\n",
            "Proceedings, Part III 21, Springer, 2021, pp. 677–693. 1\n",
            "[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\n",
            "O. Levy, S. Bowman, Superglue: A stickier benchmark for general-\n",
            "purpose language understanding systems, Advances in neural informa-\n",
            "tion processing systems 32 (2019). 1, 24, 29\n",
            "34\n",
            "[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\n",
            "Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, et al., Towards a human-\n",
            "like open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\n",
            "[4] B. A. y Arcas, Do large language models understand us?, Daedalus\n",
            "151 (2) (2022) 183–197. 2\n",
            "[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\n",
            "Language models are unsupervised multitask learners, OpenAI blog\n",
            "1 (8) (2019) 9. 2, 7\n",
            "[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\n",
            "A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\n",
            "are few-shot learners, Advances in neural information processing sys-\n",
            "tems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 17, 22, 23, 24, 25, 33\n",
            "[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\n",
            "of deep bidirectional transformers for language understanding, arXiv\n",
            "preprint arXiv:1810.04805 (2018). 2, 18, 24\n",
            "[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\n",
            "L. Zettlemoyer, Deep contextualized word representations, in: NAACL-\n",
            "HLT, Association for Computational Linguistics, 2018, pp. 2227–2237.\n",
            "2\n",
            "[9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\n",
            "V. Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-\n",
            "training for natural language generation, translation, and comprehen-\n",
            "sion, arXiv preprint arXiv:1910.13461 (2019). 2\n",
            "[10] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\n",
            "Y. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with\n",
            "a unified text-to-text transformer, The Journal of Machine Learning Re-\n",
            "search 21 (1) (2020) 5485–5551. 2, 7, 8, 17, 19, 24, 25, 26, 28, 30,\n",
            "31\n",
            "[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\n",
            "A. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to-\n",
            "text transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,\n",
            "25, 28, 30\n",
            "[12] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi,\n",
            "J. Guan, P. Ke, et al., Cpm-2: Large-scale cost-effective pre-trained lan-\n",
            "guage models, AI Open 2 (2021) 216–224. 2, 8, 25\n",
            "[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´\n",
            "c, D. Hesslow,\n",
            "R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\n",
            "parameter open-access multilingual language model, arXiv preprint\n",
            "arXiv:2211.05100 (2022). 2, 4, 9, 11, 22, 23, 24, 25, 30\n",
            "[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\n",
            "M. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer\n",
            "language models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 23,\n",
            "24, 25\n",
            "[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\n",
            "P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal-\n",
            "ing language modeling with pathways, arXiv preprint arXiv:2204.02311\n",
            "(2022). 2, 6, 9, 10, 22, 23, 24, 25\n",
            "[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li,\n",
            "X. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\n",
            "language models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 17,\n",
            "22, 23, 25, 28, 31\n",
            "[17] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\n",
            "A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask\n",
            "prompted training enables zero-shot task generalization, arXiv preprint\n",
            "arXiv:2110.08207 (2021). 2, 11, 25, 28, 31\n",
            "[18] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\n",
            "A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\n",
            "Super-naturalinstructions: Generalization via declarative instructions on\n",
            "1600+ nlp tasks, in: Proceedings of the 2022 Conference on Empirical\n",
            "Methods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7,\n",
            "11, 17, 23, 25, 28, 31\n",
            "[19] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha-\n",
            "jishirzi, Self-instruct: Aligning language model with self generated in-\n",
            "structions, arXiv preprint arXiv:2212.10560 (2022). 2, 11, 18, 22, 28\n",
            "[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\n",
            "C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\n",
            "els to follow instructions with human feedback, Advances in Neural In-\n",
            "formation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16,\n",
            "22\n",
            "[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\n",
            "N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open\n",
            "foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288\n",
            "(2023). 2, 7, 10, 16, 25, 33\n",
            "[22] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-\n",
            "gatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of\n",
            "large language models, arXiv preprint arXiv:2206.07682 (2022). 2\n",
            "[23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large\n",
            "language models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2\n",
            "[24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci-\n",
            "entific research capabilities of large language models, arXiv preprint\n",
            "arXiv:2304.05332 (2023). 2\n",
            "[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\n",
            "J. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with\n",
            "retrieval augmented language models, arXiv preprint arXiv:2208.03299\n",
            "(2022). 2, 17, 18, 33\n",
            "[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\n",
            "A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\n",
            "multimodal language model, arXiv preprint arXiv:2303.03378 (2023).\n",
            "2, 19, 21, 33\n",
            "[27] A. Parisi, Y. Zhao, N. Fiedel, Talm: Tool augmented language models,\n",
            "arXiv preprint arXiv:2205.12255 (2022). 2, 18, 19\n",
            "[28] B. Zhang, H. Soh, Large language models as zero-shot human models\n",
            "for human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2,\n",
            "33\n",
            "[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi,\n",
            "Y. Shi, et al., mplug-owl: Modularization empowers large language\n",
            "models with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2,\n",
            "22\n",
            "[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo,\n",
            "T. Lu, J. Zhou, Y. Qiao, et al., Visionllm: Large language model\n",
            "is also an open-ended decoder for vision-centric tasks, arXiv preprint\n",
            "arXiv:2305.11175 (2023). 2, 22\n",
            "[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools:\n",
            "Teaching large language model to use tools via self-instruction, arXiv\n",
            "preprint arXiv:2305.18752 (2023). 2, 19, 22\n",
            "[32] E.\n",
            "Saravia,\n",
            "Prompt\n",
            "Engineering\n",
            "Guide,\n",
            "https://github.com/dair-\n",
            "ai/Prompt-Engineering-Guide (12 2022). 2, 7, 17, 33\n",
            "[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\n",
            "W. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\n",
            "model, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 22, 23, 25\n",
            "[34] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5+:\n",
            "Open code large language models for code understanding and genera-\n",
            "tion, arXiv preprint arXiv:2305.07922 (2023). 2, 10, 24, 25\n",
            "[35] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang,\n",
            "Y. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\n",
            "edge enhanced pre-training for language understanding and generation,\n",
            "arXiv preprint arXiv:2112.12731 (2021). 2, 8, 23, 25\n",
            "[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y. He, Deepspeed: System op-\n",
            "timizations enable training deep learning models with over 100 billion\n",
            "parameters, in: Proceedings of the 26th ACM SIGKDD International\n",
            "Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\n",
            "3506. 2, 5\n",
            "[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, Zero: Memory optimiza-\n",
            "tions toward training trillion parameter models, in: SC20: International\n",
            "Conference for High Performance Computing, Networking, Storage and\n",
            "Analysis, IEEE, 2020, pp. 1–16. 2, 4, 23\n",
            "[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards\n",
            "a unified view of parameter-efficient transfer learning, arXiv preprint\n",
            "arXiv:2110.04366 (2021). 2, 20, 21\n",
            "[39] Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po-\n",
            "ria, Llm-adapters: An adapter family for parameter-efficient fine-tuning\n",
            "of large language models, arXiv preprint arXiv:2304.01933 (2023). 2,\n",
            "20\n",
            "[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-\n",
            "efficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\n",
            "20\n",
            "[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\n",
            "generation, arXiv preprint arXiv:2101.00190 (2021). 2, 20\n",
            "[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\n",
            "large language models, arXiv preprint arXiv:2305.11627 (2023). 2, 21\n",
            "[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang,\n",
            "From dense to sparse: Contrastive pruning for better pre-trained lan-\n",
            "35\n",
            "guage model compression, in: Proceedings of the AAAI Conference on\n",
            "Artificial Intelligence, Vol. 36, 2022, pp. 11547–11555. 2, 21\n",
            "[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant:\n",
            "Accurate and efficient post-training quantization for large language\n",
            "models, in: ICML, Vol. 202 of Proceedings of Machine Learning Re-\n",
            "search, PMLR, 2023, pp. 38087–38099. 2, 20\n",
            "[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,\n",
            "Compression of generative pre-trained language models via quantiza-\n",
            "tion, arXiv preprint arXiv:2203.10705 (2022). 2, 20\n",
            "[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu,\n",
            "Giraffe: Adventures in expanding context lengths in llms, arXiv preprint\n",
            "arXiv:2308.10882 (2023). 2, 17\n",
            "[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn:\n",
            "Efficient con-\n",
            "text window extension of large language models, arXiv preprint\n",
            "arXiv:2309.00071 (2023). 2, 17\n",
            "[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H. Sung, Y. Yang,\n",
            "Longt5: Efficient text-to-text transformer for long sequences, arXiv\n",
            "preprint arXiv:2112.07916 (2021). 2, 17\n",
            "[49] S. Chen, S. Wong, L. Chen, Y. Tian, Extending context window\n",
            "of large language models via positional interpolation, arXiv preprint\n",
            "arXiv:2306.15595 (2023). 2, 17\n",
            "[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\n",
            "J. Zhang, Z. Dong, et al., A survey of large language models, arXiv\n",
            "preprint arXiv:2303.18223 (2023). 2, 3, 7\n",
            "[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur-\n",
            "vey on word representation models: From classical to state-of-the-art\n",
            "word representation language models, Transactions on Asian and Low-\n",
            "Resource Language Information Processing 20 (5) (2021) 1–35. 2, 3\n",
            "[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\n",
            "E. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\n",
            "cessing via large pre-trained language models: A survey, arXiv preprint\n",
            "arXiv:2111.01243 (2021). 2, 3\n",
            "[53] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\n",
            "L. He, et al., A comprehensive survey on pretrained foundation models:\n",
            "A history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023).\n",
            "2, 3\n",
            "[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\n",
            "J. Xu, Z. Sui, A survey for in-context learning, arXiv preprint\n",
            "arXiv:2301.00234 (2022). 2, 7, 17\n",
            "[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\n",
            "A survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 17\n",
            "[56] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang,\n",
            "Q. Liu, Aligning large language models with human: A survey, arXiv\n",
            "preprint arXiv:2307.12966 (2023). 2\n",
            "[57] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, A survey on model compression\n",
            "for large language models, arXiv preprint arXiv:2308.07633 (2023). 2\n",
            "[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\n",
            "modal large language models, arXiv preprint arXiv:2306.13549 (2023).\n",
            "2, 22\n",
            "[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\n",
            "ING 1992 volume 4: The 14th international conference on computa-\n",
            "tional linguistics, 1992. 4\n",
            "[60] T. Kudo, Subword regularization: Improving neural network translation\n",
            "models with multiple subword candidates, in: Proceedings of the 56th\n",
            "Annual Meeting of the Association for Computational Linguistics (Vol-\n",
            "ume 1: Long Papers), 2018, pp. 66–75. 4\n",
            "[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\n",
            "words with subword units, in: Proceedings of the 54th Annual Meet-\n",
            "ing of the Association for Computational Linguistics (Volume 1: Long\n",
            "Papers), 2016, pp. 1715–1725. 4\n",
            "[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012\n",
            "IEEE international conference on acoustics, speech and signal process-\n",
            "ing (ICASSP), IEEE, 2012, pp. 5149–5152. 4\n",
            "[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gallé,\n",
            "A. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char-\n",
            "acters: A brief history of open-vocabulary modeling and tokenization in\n",
            "nlp, arXiv preprint arXiv:2112.10508 (2021). 4\n",
            "[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n",
            "Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\n",
            "information processing systems 30 (2017). 4, 7\n",
            "[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with\n",
            "linear biases enables input length extrapolation, in: International Con-\n",
            "ference on Learning Representations, 2022.\n",
            "URL https://openreview.net/forum?id=R8sQPpGCv0 4, 17\n",
            "[66] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu, Roformer: En-\n",
            "hanced transformer with rotary position embedding, arXiv preprint\n",
            "arXiv:2104.09864 (2021). 4, 9, 17\n",
            "[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\n",
            "with sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7,\n",
            "23\n",
            "[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and\n",
            "memory-efficient exact attention with io-awareness, Advances in Neural\n",
            "Information Processing Systems 35 (2022) 16344–16359. 4\n",
            "[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\n",
            "are universal approximators, Neural networks 2 (5) (1989) 359–366. 4\n",
            "[70] V. Nair, G. E. Hinton, Rectified linear units improve restricted boltz-\n",
            "mann machines, in: Proceedings of the 27th international conference on\n",
            "machine learning (ICML-10), 2010, pp. 807–814. 4\n",
            "[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\n",
            "preprint arXiv:1606.08415 (2016). 4\n",
            "[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\n",
            "Dropout: a simple way to prevent neural networks from overfitting, The\n",
            "journal of machine learning research 15 (1) (2014) 1929–1958. 4\n",
            "[73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R.\n",
            "Ke, A. Goyal, Y. Bengio, A. Courville, C. Pal, Zoneout: Regular-\n",
            "izing rnns by randomly preserving hidden activations, arXiv preprint\n",
            "arXiv:1606.01305 (2016). 4\n",
            "[74] N.\n",
            "Shazeer,\n",
            "Glu\n",
            "variants\n",
            "improve\n",
            "transformer,\n",
            "arXiv\n",
            "preprint\n",
            "arXiv:2002.05202 (2020). 4\n",
            "[75] Y. N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with\n",
            "gated convolutional networks, in: International conference on machine\n",
            "learning, PMLR, 2017, pp. 933–941. 4\n",
            "[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\n",
            "arXiv:1607.06450 (2016). 4\n",
            "[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\n",
            "in Neural Information Processing Systems 32 (2019). 4\n",
            "[78] A. Baevski, M. Auli, Adaptive input representations for neural language\n",
            "modeling, arXiv preprint arXiv:1809.10853 (2018). 4\n",
            "[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling\n",
            "transformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4\n",
            "[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\n",
            "Megatron-lm: Training multi-billion parameter language models using\n",
            "model parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5\n",
            "[81] \"bmtrain: Efficient training for big models.\".\n",
            "URL https://github.com/OpenBMB/BMTrain 4, 5\n",
            "[82] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-\n",
            "tac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\n",
            "art natural language processing, in: Proceedings of the 2020 conference\n",
            "on empirical methods in natural language processing: system demon-\n",
            "strations, 2020, pp. 38–45. 5\n",
            "[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau-\n",
            "rin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al.,\n",
            "Jax: composable transformations of python+ numpy programs (2018).\n",
            "5\n",
            "[84] S. Li, J. Fang, Z. Bian, H. Liu, Y. Liu, H. Huang, B. Wang, Y. You,\n",
            "Colossal-ai: A unified deep learning system for large-scale parallel train-\n",
            "ing, arXiv preprint arXiv:2110.14883 (2021). 5\n",
            "[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe:\n",
            "A\n",
            "fast mixture-of-expert training system, arXiv preprint arXiv:2103.13262\n",
            "(2021). 5\n",
            "[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\n",
            "work, in: Artificial Intelligence Technology, Springer, 2022, pp. 137–\n",
            "162. 5\n",
            "[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\n",
            "T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper-\n",
            "ative style, high-performance deep learning library, Advances in neural\n",
            "information processing systems 32 (2019). 5\n",
            "[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\n",
            "S. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large-\n",
            "scale machine learning., in: Osdi, Vol. 16, Savannah, GA, USA, 2016,\n",
            "pp. 265–283. 5\n",
            "[89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,\n",
            "36\n",
            "B. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and efficient machine\n",
            "learning library for heterogeneous distributed systems, arXiv preprint\n",
            "arXiv:1512.01274 (2015). 5\n",
            "[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril-\n",
            "lion parameter models with simple and efficient sparsity, The Journal of\n",
            "Machine Learning Research 23 (1) (2022) 5232–5270. 5, 9\n",
            "[91] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\n",
            "Y. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language\n",
            "models with mixture-of-experts, in: International Conference on Ma-\n",
            "chine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 25\n",
            "[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li,\n",
            "X. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu-P: Towards trillion\n",
            "parameter language model with sparse heterogeneous computing, arXiv\n",
            "preprint arXiv:2303.10845 (2023). 5, 10, 11, 23, 25\n",
            "[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\n",
            "J. Launay, C. Raffel, What language model architecture and pretrain-\n",
            "ing objective works best for zero-shot generalization?, in: International\n",
            "Conference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\n",
            "[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou,\n",
            "H.-W. Hon, Unified language model pre-training for natural language\n",
            "understanding and generation, Advances in neural information process-\n",
            "ing systems 32 (2019). 6\n",
            "[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\n",
            "S. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language\n",
            "models, arXiv preprint arXiv:2001.08361 (2020). 6\n",
            "[96] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\n",
            "E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,\n",
            "et al., Training compute-optimal large language models, arXiv preprint\n",
            "arXiv:2203.15556 (2022). 6, 9, 25, 29\n",
            "[97] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster,\n",
            "T. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in-\n",
            "struction meta learning through the lens of generalization, arXiv preprint\n",
            "arXiv:2212.12017 (2022). 7, 11, 17, 22, 25, 28\n",
            "[98] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, C. Gan,\n",
            "Principle-driven self-alignment of language models from scratch with\n",
            "minimal human supervision, arXiv preprint arXiv:2305.03047 (2023).\n",
            "7, 16\n",
            "[99] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,\n",
            "N. Joseph, B. Mann, N. DasSarma, et al., A general language assistant\n",
            "as a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021).\n",
            "7\n",
            "[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\n",
            "P. Christiano, G. Irving, Fine-tuning language models from human pref-\n",
            "erences, arXiv preprint arXiv:1909.08593 (2019). 7\n",
            "[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec-\n",
            "tion: Improving zero-shot and few-shot learning of language models via\n",
            "chain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\n",
            "7, 11\n",
            "[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam-\n",
            "ining the power of symbolic tasks in instruction tuning, arXiv preprint\n",
            "arXiv:2304.07995 (2023). 7, 11\n",
            "[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\n",
            "D. Zhou, et al., Chain-of-thought prompting elicits reasoning in large\n",
            "language models, Advances in Neural Information Processing Systems\n",
            "35 (2022) 24824–24837. 7, 19, 22\n",
            "[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\n",
            "hery, D. Zhou, Self-consistency improves chain of thought reasoning in\n",
            "language models, arXiv preprint arXiv:2203.11171 (2022). 7, 19\n",
            "[105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, K. Narasimhan,\n",
            "Tree of thoughts: Deliberate problem solving with large language mod-\n",
            "els, arXiv preprint arXiv:2305.10601 (2023). 7, 19\n",
            "[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\n",
            "A. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learn-\n",
            "ing for nlp, in: International Conference on Machine Learning, PMLR,\n",
            "2019, pp. 2790–2799. 7, 20\n",
            "[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\n",
            "of large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7\n",
            "[108] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang,\n",
            "K. Wang, X. Zhang, et al., Pangu-α : Large-scale autoregressive pre-\n",
            "trained chinese language models with auto-parallel computation, arXiv\n",
            "preprint arXiv:2104.12369 (2021). 8, 22, 23, 25\n",
            "[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,\n",
            "J. Tang, Wudaocorpora: A super large-scale chinese corpora for pre-\n",
            "training language models, AI Open 2 (2021) 65–68. 8, 30\n",
            "[110] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\n",
            "Y. Zhao, Y. Lu, et al., Ernie 3.0: Large-scale knowledge enhanced\n",
            "pre-training for language understanding and generation, arXiv preprint\n",
            "arXiv:2107.02137 (2021). 8, 25\n",
            "[111] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, R. Salakhutdinov,\n",
            "Transformer-xl: Attentive language models beyond a fixed-length con-\n",
            "text, arXiv preprint arXiv:1901.02860 (2019). 8\n",
            "[112] O. Lieber, O. Sharir, B. Lenz, Y. Shoham, Jurassic-1: Technical details\n",
            "and evaluation, White Paper. AI21 Labs 1 (2021). 8, 23, 25\n",
            "[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\n",
            "ficiencies of self-attention, Advances in Neural Information Processing\n",
            "Systems 33 (2020) 22640–22651. 8, 11\n",
            "[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\n",
            "S. Kim, S. Kim, D. Seo, et al., What changes can large-scale language\n",
            "models bring?\n",
            "intensive study on hyperclova: Billions-scale korean\n",
            "generative pretrained transformers, arXiv preprint arXiv:2109.04650\n",
            "(2021). 8, 25\n",
            "[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo,\n",
            "L. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero-\n",
            "shot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8,\n",
            "23, 25\n",
            "[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\n",
            "J. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\n",
            "guage models: Methods, analysis & insights from training gopher, arXiv\n",
            "preprint arXiv:2112.11446 (2021). 8, 9, 25, 28\n",
            "[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\n",
            "J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al.,\n",
            "Using deepspeed and megatron to train megatron-turing nlg 530b, a\n",
            "large-scale generative language model, arXiv preprint arXiv:2201.11990\n",
            "(2022). 8, 9, 23, 25\n",
            "[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\n",
            "H. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-\n",
            "source autoregressive language model, arXiv preprint arXiv:2204.06745\n",
            "(2022). 9, 22, 23, 24, 25\n",
            "[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\n",
            "guage model (2021). 9\n",
            "[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\n",
            "B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre-\n",
            "cision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23\n",
            "[121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin-\n",
            "ton, J. Dean, Outrageously large neural networks: The sparsely-gated\n",
            "mixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23\n",
            "[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\n",
            "H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex-\n",
            "atm 20b: Few-shot learning using a large-scale multilingual seq2seq\n",
            "model, arXiv preprint arXiv:2208.01448 (2022). 9, 22, 23, 24, 25\n",
            "[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\n",
            "S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report,\n",
            "arXiv preprint arXiv:2305.10403 (2023). 9, 25\n",
            "[124] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia,\n",
            "H. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws\n",
            "with 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9,\n",
            "23, 25\n",
            "[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\n",
            "Chung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan-\n",
            "guage learning paradigms, in: The Eleventh International Conference\n",
            "on Learning Representations, 2022. 9, 10, 23, 24, 25\n",
            "[126] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen-\n",
            "eral language model pretraining with autoregressive blank infilling, in:\n",
            "Proceedings of the 60th Annual Meeting of the Association for Compu-\n",
            "tational Linguistics (Volume 1: Long Papers), 2022, pp. 320–335. 10\n",
            "[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\n",
            "T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.,\n",
            "Llama: Open and efficient foundation language models, arXiv preprint\n",
            "arXiv:2302.13971 (2023). 10, 22, 25\n",
            "[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\n",
            "preprint arXiv:2112.05682 (2021). 10\n",
            "[129] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\n",
            "37\n",
            "M. Shoeybi, B. Catanzaro, Reducing activation recomputation in large\n",
            "transformer models, Proceedings of Machine Learning and Systems 5\n",
            "(2023). 10\n",
            "[130] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,\n",
            "C. Xiong, Codegen: An open large language model for code with multi-\n",
            "turn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 10,\n",
            "22, 25, 28\n",
            "[131] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\n",
            "wards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\n",
            "guage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\n",
            "10, 25, 29\n",
            "[132] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\n",
            "T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level\n",
            "code generation with alphacode, Science 378 (6624) (2022) 1092–1097.\n",
            "10, 23, 25, 29\n",
            "[133] N. Shazeer, Fast transformer decoding: One write-head is all you need,\n",
            "arXiv preprint arXiv:1911.02150 (2019). 10\n",
            "[134] R. Y. Pang, H. He, Text generation by learning from demonstrations,\n",
            "arXiv preprint arXiv:2009.07839 (2020). 10\n",
            "[135] R. Dabre, A. Fujita, Softmax tempering for training neural machine\n",
            "translation models, arXiv preprint arXiv:2009.09372 (2020). 10\n",
            "[136] Y. Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified\n",
            "pre-trained encoder-decoder models for code understanding and genera-\n",
            "tion, arXiv preprint arXiv:2109.00859 (2021). 10\n",
            "[137] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\n",
            "M. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be\n",
            "with you!, arXiv preprint arXiv:2305.06161 (2023). 10, 25\n",
            "[138] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\n",
            "A. Poulton, V. Kerkez, R. Stojnic, Galactica: A large language model for\n",
            "science, arXiv preprint arXiv:2211.09085 (2022). 10, 23, 25, 29\n",
            "[139] FairScale authors, Fairscale: A general purpose modular pytorch library\n",
            "for high performance and large scale training, https://github.com/\n",
            "facebookresearch/fairscale (2021). 10\n",
            "[140] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\n",
            "Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., Lamda: Language models\n",
            "for dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\n",
            "[141] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\n",
            "P. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\n",
            "model for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 32\n",
            "[142] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-\n",
            "cial chat model with hundreds of billions parameters, arXiv preprint\n",
            "arXiv:2305.12002 (2023). 11, 16, 25\n",
            "[143] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\n",
            "former language model with jax (2021). 12, 23\n",
            "[144] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\n",
            "T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al.,\n",
            "Crosslingual generalization through multitask finetuning, arXiv preprint\n",
            "arXiv:2211.01786 (2022). 11, 25, 28, 31\n",
            "[145] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\n",
            "Dynosaur: A dynamic growth paradigm for instruction-tuning data cu-\n",
            "ration, arXiv preprint arXiv:2305.14327 (2023). 16\n",
            "[146] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\n",
            "C. He, X. Yue, et al., Llama-adapter v2: Parameter-efficient visual in-\n",
            "struction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24\n",
            "[147] Openai. gpt-4 technical report (2023). 16, 34\n",
            "[148] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\n",
            "T. B. Hashimoto, Stanford alpaca:\n",
            "An instruction-following llama\n",
            "model,\n",
            "https://github.com/tatsu-lab/stanford_alpaca\n",
            "(2023). 16, 25, 28\n",
            "[149] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\n",
            "S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\n",
            "open-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\n",
            "2023).\n",
            "URL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25,\n",
            "28\n",
            "[150] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\n",
            "arXiv preprint arXiv:2304.03277 (2023). 16, 28\n",
            "[151] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\n",
            "arithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\n",
            "[152] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\n",
            "Tuning llama model with chinese medical knowledge, arXiv preprint\n",
            "arXiv:2304.06975 (2023). 16\n",
            "[153] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang,\n",
            "Wizardlm: Empowering large language models to follow complex in-\n",
            "structions, arXiv preprint arXiv:2304.12244 (2023). 16\n",
            "[154] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\n",
            "D. Jiang, Wizardcoder: Empowering code large language models with\n",
            "evol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25\n",
            "[155] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick,\n",
            "M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach-\n",
            "ing language models to support answers with verified quotes, arXiv\n",
            "preprint arXiv:2203.11147 (2022). 16\n",
            "[156] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\n",
            "C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt: Browser-\n",
            "assisted question-answering with human feedback, arXiv preprint\n",
            "arXiv:2112.09332 (2021). 16, 18, 19, 25, 31\n",
            "[157] A. Glaese, N. McAleese, M. Tr˛\n",
            "ebacz, J. Aslanides, V. Firoiu, T. Ewalds,\n",
            "M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving\n",
            "alignment of dialogue agents via targeted human judgements, arXiv\n",
            "preprint arXiv:2209.14375 (2022). 16, 19, 25\n",
            "[158] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn,\n",
            "Direct preference optimization: Your language model is secretly a re-\n",
            "ward model, arXiv preprint arXiv:2305.18290 (2023). 16\n",
            "[159] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum,\n",
            "T. Zhang, Raft: Reward ranked finetuning for generative foundation\n",
            "model alignment, arXiv preprint arXiv:2304.06767 (2023). 16\n",
            "[160] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank\n",
            "responses to align language models with human feedback without tears,\n",
            "arXiv preprint arXiv:2304.05302 (2023). 16\n",
            "[161] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, H. Wang, Preference rank-\n",
            "ing optimization for human alignment, arXiv preprint arXiv:2306.17492\n",
            "(2023). 16\n",
            "[162] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine-\n",
            "tuning using human feedback, arXiv preprint arXiv:2302.02676 (2023).\n",
            "16\n",
            "[163] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\n",
            "A. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm-\n",
            "lessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 16\n",
            "[164] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\n",
            "P. Liang,\n",
            "T. B. Hashimoto,\n",
            "Alpacafarm:\n",
            "A simulation frame-\n",
            "work for methods that learn from human feedback, arXiv preprint\n",
            "arXiv:2305.14387 (2023). 16\n",
            "[165] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang,\n",
            "Prompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\n",
            "16\n",
            "[166] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši¯\n",
            "ut˙\n",
            "e, A. Chen,\n",
            "A. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac-\n",
            "ity for moral self-correction in large language models, arXiv preprint\n",
            "arXiv:2302.07459 (2023). 16\n",
            "[167] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\n",
            "training fail?, arXiv preprint arXiv:2307.02483 (2023). 16\n",
            "[168] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath,\n",
            "B. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan-\n",
            "guage models to reduce harms: Methods, scaling behaviors, and lessons\n",
            "learned, arXiv preprint arXiv:2209.07858 (2022). 16, 28\n",
            "[169] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab-\n",
            "lish, exploit: Red teaming language models from scratch, arXiv preprint\n",
            "arXiv:2306.09442 (2023). 16\n",
            "[170] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\n",
            "N. McAleese, G. Irving, Red teaming language models with language\n",
            "models, arXiv preprint arXiv:2202.03286 (2022). 16\n",
            "[171] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are\n",
            "continual learners, in: Proceedings of the 2022 Conference on Empirical\n",
            "Methods in Natural Language Processing, 2022, pp. 6107–6122. 16\n",
            "[172] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\n",
            "tuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\n",
            "[173] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty,\n",
            "C. Baral, Instruction tuned models are quick learners, arXiv preprint\n",
            "arXiv:2306.05539 (2023). 17\n",
            "[174] H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y. Yanggong,\n",
            "J. Zhao, Maybe only 0.5% data is needed: A preliminary exploration\n",
            "of low training data instruction tuning, arXiv preprint arXiv:2305.09246\n",
            "38\n",
            "(2023). 17\n",
            "[175] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat,\n",
            "P. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint\n",
            "arXiv:2305.11206 (2023). 17, 25, 28\n",
            "[176] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, S. Wang, Lm-infinite: Sim-\n",
            "ple on-the-fly length generalization for large language models, arXiv\n",
            "preprint arXiv:2308.16137 (2023). 17\n",
            "[177] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y. Zemlyan-\n",
            "skiy, D. Uthus, M. Guo, J. Lee-Thorp, Y. Tay, et al., Colt5: Faster\n",
            "long-range transformers with conditional computation, arXiv preprint\n",
            "arXiv:2303.09752 (2023). 17\n",
            "[178] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei,\n",
            "Longnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint\n",
            "arXiv:2307.02486 (2023). 17\n",
            "[179] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Effi-\n",
            "cient fine-tuning of long-context large language models, arXiv preprint\n",
            "arXiv:2309.12307 (2023). 17\n",
            "[180] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend,\n",
            "E. Karpas, A. Shashua, K. Leyton-Brown, Y. Shoham, Parallel context\n",
            "windows for large language models, in: Proceedings of the 61st Annual\n",
            "Meeting of the Association for Computational Linguistics (Volume 1:\n",
            "Long Papers), 2023, pp. 6383–6402. 17\n",
            "[181] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\n",
            "Augmenting language models with long-term memory, arXiv preprint\n",
            "arXiv:2306.07174 (2023). 17\n",
            "[182] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, S. Wang, Long\n",
            "time no see! open-domain conversation with long-term persona memory,\n",
            "arXiv preprint arXiv:2203.05797 (2022). 17\n",
            "[183] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\n",
            "can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\n",
            "Improving language models by retrieving from trillions of tokens, in:\n",
            "International conference on machine learning, PMLR, 2022, pp. 2206–\n",
            "2240. 17, 18, 33\n",
            "[184] W. Zhong, L. Guo, Q. Gao, Y. Wang, Memorybank:\n",
            "Enhanc-\n",
            "ing large language models with long-term memory, arXiv preprint\n",
            "arXiv:2305.10250 (2023). 17\n",
            "[185] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\n",
            "Reflexion: Language agents with verbal reinforcement learning, arXiv\n",
            "preprint arXiv:2303.11366 14 (2023). 17, 19\n",
            "[186] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment-\n",
            "ing llms with databases as their symbolic memory, arXiv preprint\n",
            "arXiv:2306.03901 (2023). 17\n",
            "[187] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\n",
            "J. Callan, G. Neubig, Active retrieval augmented generation, arXiv\n",
            "preprint arXiv:2305.06983 (2023). 17, 18\n",
            "[188] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\n",
            "Brown, Y. Shoham, In-context retrieval-augmented language models,\n",
            "arXiv preprint arXiv:2302.00083 (2023). 17, 18, 33\n",
            "[189] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self-\n",
            "improve with memory-of-thoughts, arXiv preprint arXiv:2305.05181\n",
            "(2023). 17\n",
            "[190] D. Schuurmans, Memory augmented large language models are compu-\n",
            "tationally universal, arXiv preprint arXiv:2301.04589 (2023). 17\n",
            "[191] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a\n",
            "general read-write memory for large language models, arXiv preprint\n",
            "arXiv:2305.14322 (2023). 17\n",
            "[192] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\n",
            "work: Bm25 and beyond, Foundations and Trends® in Information Re-\n",
            "trieval 3 (4) (2009) 333–389. 18\n",
            "[193] X. Wang,\n",
            "J. Wei,\n",
            "D. Schuurmans,\n",
            "Q. Le,\n",
            "E. Chi,\n",
            "D. Zhou,\n",
            "Rationale-augmented ensembles in language models, arXiv preprint\n",
            "arXiv:2207.00747 (2022). 18\n",
            "[194] F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J.-G. Lou, W. Chen,\n",
            "Repocoder: Repository-level code completion through iterative retrieval\n",
            "and generation, arXiv preprint arXiv:2303.12570 (2023). 18\n",
            "[195] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\n",
            "O. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive\n",
            "language models with retrieval? a comprehensive study, arXiv preprint\n",
            "arXiv:2304.06762 (2023). 18\n",
            "[196] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\n",
            "large language models, arXiv preprint arXiv:2307.07164 (2023). 18\n",
            "[197] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, What makes\n",
            "good in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804\n",
            "(2021). 18\n",
            "[198] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\n",
            "context learning, arXiv preprint arXiv:2112.08633 (2021). 18\n",
            "[199] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\n",
            "moyer, W.-t. Yih, Replug: Retrieval-augmented black-box language\n",
            "models, arXiv preprint arXiv:2301.12652 (2023). 18\n",
            "[200] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\n",
            "arXiv preprint arXiv:2306.13421 (2023). 18\n",
            "[201] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\n",
            "language model pre-training, in: International conference on machine\n",
            "learning, PMLR, 2020, pp. 3929–3938. 18\n",
            "[202] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: Efficient and ef-\n",
            "fective retrieval-augmented text generation, in: Proceedings of the 46th\n",
            "International ACM SIGIR Conference on Research and Development in\n",
            "Information Retrieval, 2023, pp. 1437–1447. 18\n",
            "[203] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\n",
            "ation, arXiv preprint arXiv:2107.07566 (2021). 18\n",
            "[204] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet-\n",
            "augmented language models through few-shot prompting for open-\n",
            "domain question answering, arXiv preprint arXiv:2203.05115 (2022).\n",
            "18\n",
            "[205] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\n",
            "gpt: A general multi-modal assistant that can plan, execute, inspect, and\n",
            "learn, arXiv preprint arXiv:2306.08640 (2023). 18, 19\n",
            "[206] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu,\n",
            "J. Gao, Chameleon: Plug-and-play compositional reasoning with large\n",
            "language models, arXiv preprint arXiv:2304.09842 (2023). 18, 19, 22\n",
            "[207] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\n",
            "Ribeiro, Art: Automatic multi-step reasoning and tool-use for large lan-\n",
            "guage models, arXiv preprint arXiv:2303.09014 (2023). 18\n",
            "[208] C.-Y. Hsieh, S.-A. Chen, C.-L. Li, Y. Fujii, A. Ratner, C.-Y. Lee, R. Kr-\n",
            "ishna, T. Pfister, Tool documentation enables zero-shot tool-usage with\n",
            "large language models, arXiv preprint arXiv:2308.00675 (2023). 18\n",
            "[209] Y. Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y. Tian, S. Li, Restgpt:\n",
            "Connecting large language models with real-world applications via rest-\n",
            "ful apis, arXiv preprint arXiv:2306.06624 (2023). 18\n",
            "[210] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan-\n",
            "guage models with massive tools via tool embeddings, arXiv preprint\n",
            "arXiv:2305.11554 (2023). 18\n",
            "[211] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\n",
            "model connected with massive apis, arXiv preprint arXiv:2305.15334\n",
            "(2023). 18\n",
            "[212] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\n",
            "lation capability of open-source large language models, arXiv preprint\n",
            "arXiv:2305.16504 (2023). 18\n",
            "[213] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,\n",
            "B. Qian, et al., Toolllm: Facilitating large language models to master\n",
            "16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 18,\n",
            "19\n",
            "[214] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt: Solv-\n",
            "ing ai tasks with chatgpt and its friends in huggingface, arXiv preprint\n",
            "arXiv:2303.17580 (2023). 19, 33\n",
            "[215] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji,\n",
            "S. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun-\n",
            "dation models with millions of apis, arXiv preprint arXiv:2303.16434\n",
            "(2023). 19\n",
            "[216] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python\n",
            "execution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 19\n",
            "[217] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced\n",
            "user assistance systems, Business & Information Systems Engineering\n",
            "58 (2016) 367–370. 19\n",
            "[218] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\n",
            "134 (1-2) (2002) 57–83. 19\n",
            "[219] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\n",
            "S. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for\n",
            "multi-agent collaborative framework, arXiv preprint arXiv:2308.00352\n",
            "(2023). 19\n",
            "[220] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\n",
            "S. Jin, E. Zhou, et al., The rise and potential of large language model\n",
            "39\n",
            "based agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 19\n",
            "[221] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\n",
            "X. Chen, Y. Lin, et al., A survey on large language model based au-\n",
            "tonomous agents, arXiv preprint arXiv:2308.11432 (2023). 19\n",
            "[222] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero-\n",
            "shot planners: Extracting actionable knowledge for embodied agents,\n",
            "in: International Conference on Machine Learning, PMLR, 2022, pp.\n",
            "9118–9147. 19\n",
            "[223] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason-\n",
            "ing with language model is planning with world model, arXiv preprint\n",
            "arXiv:2305.14992 (2023). 19, 33\n",
            "[224] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. Murthy,\n",
            "Z. Chen, J. Zhang, D. Arpit, et al., Retroformer:\n",
            "Retrospective\n",
            "large language agents with policy gradient optimization, arXiv preprint\n",
            "arXiv:2308.02151 (2023). 19, 33\n",
            "[225] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\n",
            "J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson,\n",
            "N. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono-\n",
            "logue: Embodied reasoning through planning with language models, in:\n",
            "6th Annual Conference on Robot Learning, 2022.\n",
            "URL https://openreview.net/forum?id=3R3Pz5i0tye 19\n",
            "[226] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock:\n",
            "Embodied finetuning for vision-language reasoning in robot manipula-\n",
            "tion, arXiv preprint arXiv:2305.18898 (2023). 19, 20, 33\n",
            "[227] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\n",
            "J. Thomason, A. Garg, Progprompt: Generating situated robot task plans\n",
            "using large language models, in: 2023 IEEE International Conference on\n",
            "Robotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 19,\n",
            "33\n",
            "[228] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L.\n",
            "Chiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards\n",
            "for robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 19\n",
            "[229] X. Tang, A. Zou, Z. Zhang, Y. Zhao, X. Zhang, A. Cohan, M. Gerstein,\n",
            "Medagents: Large language models as collaborators for zero-shot med-\n",
            "ical reasoning, arXiv preprint arXiv:2311.10537 (2023). 19\n",
            "[230] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\n",
            "J. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say:\n",
            "Grounding language in robotic affordances, in: Conference on Robot\n",
            "Learning, PMLR, 2023, pp. 287–318. 19, 33\n",
            "[231] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language-\n",
            "guided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023).\n",
            "20\n",
            "[232] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say-\n",
            "nav: Grounding large language models for dynamic planning to navi-\n",
            "gation in new environments, arXiv preprint arXiv:2309.04077 (2023).\n",
            "20\n",
            "[233] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y. Su,\n",
            "Llm-planner: Few-shot grounded planning for embodied agents with\n",
            "large language models, arXiv preprint arXiv:2212.04088 (2022). 20\n",
            "[234] V. S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\n",
            "your\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv\n",
            "preprint arXiv:2303.03480 (2023). 20\n",
            "[235] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for\n",
            "robot navigation, in: 2023 IEEE International Conference on Robotics\n",
            "and Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20\n",
            "[236] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning\n",
            "with large language models for object rearrangement, arXiv preprint\n",
            "arXiv:2303.06247 (2023). 20, 33\n",
            "[237] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-\n",
            "stands, too, arXiv preprint arXiv:2103.10385 (2021). 20\n",
            "[238] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-efficient tun-\n",
            "ing: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022).\n",
            "20\n",
            "[239] Y. Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao,\n",
            "Adamix: Mixture-of-adapter for parameter-efficient tuning of large lan-\n",
            "guage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\n",
            "[240] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\n",
            "W. Chen, Lora: Low-rank adaptation of large language models, arXiv\n",
            "preprint arXiv:2106.09685 (2021). 20, 21, 22\n",
            "[241] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt\n",
            "tuning can be comparable to fine-tuning across scales and tasks, in: Pro-\n",
            "ceedings of the 60th Annual Meeting of the Association for Computa-\n",
            "tional Linguistics (Volume 2: Short Papers), 2022, pp. 61–68. 20\n",
            "[242] A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi,\n",
            "Progressive prompts: Continual learning for language models, arXiv\n",
            "preprint arXiv:2301.12314 (2023). 20\n",
            "[243] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To-\n",
            "wards adaptive prefix tuning for parameter-efficient language model\n",
            "fine-tuning, arXiv preprint arXiv:2305.15212 (2023). 20\n",
            "[244] E. B. Zaken, S. Ravfogel, Y. Goldberg, Bitfit:\n",
            "Simple parameter-\n",
            "efficient fine-tuning for transformer-based masked language-models,\n",
            "arXiv preprint arXiv:2106.10199 (2021). 20\n",
            "[245] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Llm. int8 ():\n",
            "8-bit matrix multiplication for transformers at scale, arXiv preprint\n",
            "arXiv:2208.07339 (2022). 20, 21\n",
            "[246] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq:\n",
            "Accurate\n",
            "post-training quantization for generative pre-trained transformers, arXiv\n",
            "preprint arXiv:2210.17323 (2022). 20\n",
            "[247] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup-\n",
            "pression+: Accurate quantization of large language models by equiva-\n",
            "lent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\n",
            "(2023). 20\n",
            "[248] E. Frantar, D. Alistarh, Optimal brain compression: A framework for\n",
            "accurate post-training quantization and pruning, Advances in Neural In-\n",
            "formation Processing Systems 35 (2022) 4475–4488. 20\n",
            "[249] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac-\n",
            "tivation outliers for weight quantization in large language models, arXiv\n",
            "preprint arXiv:2306.02272 (2023). 21\n",
            "[250] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-\n",
            "W. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter-\n",
            "efficient adaptation of large-scale pre-trained language models, arXiv\n",
            "preprint arXiv:2210.03858 (2022). 21\n",
            "[251] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient\n",
            "finetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\n",
            "21\n",
            "[252] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Kr-\n",
            "ishnamoorthi, V. Chandra, Llm-qat: Data-free quantization aware train-\n",
            "ing for large language models, arXiv preprint arXiv:2305.17888 (2023).\n",
            "21\n",
            "[253] Y. Guo, A. Yao, H. Zhao, Y. Chen, Network sketching: Exploiting bi-\n",
            "nary structure in deep cnns, in: Proceedings of the IEEE Conference on\n",
            "Computer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\n",
            "[254] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee,\n",
            "Memory-efficient fine-tuning of compressed large language models via\n",
            "sub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023).\n",
            "21\n",
            "[255] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and effective pruning\n",
            "approach for large language models, arXiv preprint arXiv:2306.11695\n",
            "(2023). 21\n",
            "[256] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\n",
            "models, arXiv preprint arXiv:1910.04732 (2019). 21\n",
            "[257] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy,\n",
            "Y. Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A\n",
            "missing secret sauce for pruning llms to high sparsity, arXiv preprint\n",
            "arXiv:2310.05175 (2023). 21\n",
            "[258] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong,\n",
            "Structured pruning for efficient generative pre-trained language models,\n",
            "in: Findings of the Association for Computational Linguistics: ACL\n",
            "2023, 2023, pp. 10880–10895. 21\n",
            "[259] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,\n",
            "A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan-\n",
            "guage model for few-shot learning, Advances in Neural Information Pro-\n",
            "cessing Systems 35 (2022) 23716–23736. 21, 22\n",
            "[260] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\n",
            "pre-training with frozen image encoders and large language models,\n",
            "arXiv preprint arXiv:2301.12597 (2023). 21, 22\n",
            "[261] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, arXiv preprint\n",
            "arXiv:2304.08485 (2023). 21, 22\n",
            "[262] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang,\n",
            "Y. Qiao, Videochat: Chat-centric video understanding, arXiv preprint\n",
            "arXiv:2305.06355 (2023). 21, 22\n",
            "[263] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de-\n",
            "40\n",
            "tailed video understanding via large vision and language models, arXiv\n",
            "preprint arXiv:2306.05424 (2023). 21\n",
            "[264] H. Zhang,\n",
            "X. Li,\n",
            "L. Bing,\n",
            "Video-llama:\n",
            "An instruction-tuned\n",
            "audio-visual language model for video understanding, arXiv preprint\n",
            "arXiv:2306.02858 (2023). 21\n",
            "[265] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley,\n",
            "Y. Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au-\n",
            "dio captioning dataset for audio-language multimodal research, arXiv\n",
            "preprint arXiv:2303.17395 (2023). 21\n",
            "[266] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw-\n",
            "llm: Multi-modal language modeling with image, audio, video, and text\n",
            "integration, arXiv preprint arXiv:2306.09093 (2023). 21\n",
            "[267] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing\n",
            "vision-language understanding with advanced large language models,\n",
            "arXiv preprint arXiv:2304.10592 (2023). 22\n",
            "[268] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\n",
            "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\n",
            "An image is worth 16x16 words: Transformers for image recognition at\n",
            "scale, arXiv preprint arXiv:2010.11929 (2020). 22\n",
            "[269] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung,\n",
            "S. Hoi, Instructblip: Towards general-purpose vision-language models\n",
            "with instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22\n",
            "[270] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\n",
            "shot learning via instruction tuning, arXiv preprint arXiv:2212.10773\n",
            "(2022). 22\n",
            "[271] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu,\n",
            "Chatbridge: Bridging modalities with large language model as a lan-\n",
            "guage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22\n",
            "[272] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu,\n",
            "X. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi-\n",
            "lingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22\n",
            "[273] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han,\n",
            "H. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning,\n",
            "arXiv preprint arXiv:2305.14167 (2023). 22\n",
            "[274] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\n",
            "Efficient vision-language instruction tuning for large language models,\n",
            "arXiv preprint arXiv:2305.15023 (2023). 22\n",
            "[275] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y. Qiao,\n",
            "Llama-adapter: Efficient fine-tuning of language models with zero-init\n",
            "attention, arXiv preprint arXiv:2303.16199 (2023). 22\n",
            "[276] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\n",
            "Robust speech recognition via large-scale weak supervision, in: Inter-\n",
            "national Conference on Machine Learning, PMLR, 2023, pp. 28492–\n",
            "28518. 22\n",
            "[277] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi-\n",
            "modal chain-of-thought reasoning in language models, arXiv preprint\n",
            "arXiv:2302.00923 (2023). 22\n",
            "[278] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt\n",
            "tuning in vision language models, arXiv preprint arXiv:2304.07919\n",
            "(2023). 22\n",
            "[279] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk-\n",
            "ing, drawing and editing with visual foundation models, arXiv preprint\n",
            "arXiv:2303.04671 (2023). 22\n",
            "[280] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,\n",
            "M. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\n",
            "soning and action, arXiv preprint arXiv:2303.11381 (2023). 22\n",
            "[281] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li, M. Gao,\n",
            "S. Zhao, Y. Shan, et al., Caption anything: Interactive image descrip-\n",
            "tion with diverse multimodal controls, arXiv preprint arXiv:2305.02677\n",
            "(2023). 22\n",
            "[282] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2:\n",
            "Adapting clip for powerful 3d open-world learning, arXiv preprint\n",
            "arXiv:2211.11682 (2022). 22\n",
            "[283] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea-\n",
            "soning without training, in: Proceedings of the IEEE/CVF Conference\n",
            "on Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.\n",
            "22\n",
            "[284] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic\n",
            "fusion with intra-and inter-modality attention flow for visual question\n",
            "answering, in: Proceedings of the IEEE/CVF conference on computer\n",
            "vision and pattern recognition, 2019, pp. 6639–6648. 22\n",
            "[285] Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian, Deep modular co-attention net-\n",
            "works for visual question answering, in: Proceedings of the IEEE/CVF\n",
            "conference on computer vision and pattern recognition, 2019, pp. 6281–\n",
            "6290. 22\n",
            "[286] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\n",
            "W. Chang, S.-F. Chang, Idealgpt:\n",
            "Iteratively decomposing vision\n",
            "and language reasoning via large language models, arXiv preprint\n",
            "arXiv:2305.14985 (2023). 22\n",
            "[287] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, H. Li,\n",
            "Prompt, generate, then cache: Cascade of foundation models makes\n",
            "strong few-shot learners, in: Proceedings of the IEEE/CVF Conference\n",
            "on Computer Vision and Pattern Recognition, 2023, pp. 15211–15222.\n",
            "22\n",
            "[288] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the\n",
            "normalization of self-attention, CoRR abs/1910.05895 (2019). 23\n",
            "[289] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\n",
            "L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pre-\n",
            "training approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\n",
            "[290] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\n",
            "D. Song, Koala: A dialogue model for academic research, Blog post\n",
            "(April 2023).\n",
            "URL\n",
            "https://bair.berkeley.edu/blog/2023/04/03/koala/\n",
            "25\n",
            "[291] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\n",
            "J. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile:\n",
            "An\n",
            "800gb dataset of diverse text for language modeling, arXiv preprint\n",
            "arXiv:2101.00027 (2020). 28, 30\n",
            "[292] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\n",
            "T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen,\n",
            "et al., The bigscience roots corpus: A 1.6 tb composite multilingual\n",
            "dataset, Advances in Neural Information Processing Systems 35 (2022)\n",
            "31809–31826. 28\n",
            "[293] Wikipedia.\n",
            "URL https://en.wikipedia.org/wiki/Main_Page 28\n",
            "[294] Together Computer, Redpajama: An open source recipe to reproduce\n",
            "llama training dataset (Apr. 2023).\n",
            "URL\n",
            "https://github.com/togethercomputer/\n",
            "RedPajama-Data 28\n",
            "[295] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions:\n",
            "Tuning language models with (almost) no human labor, arXiv preprint\n",
            "arXiv:2212.09689 (2022). 28\n",
            "[296] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\n",
            "D. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and\n",
            "harmless assistant with reinforcement learning from human feedback,\n",
            "arXiv preprint arXiv:2204.05862 (2022). 28\n",
            "[297] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,\n",
            "J. Steinhardt, Measuring massive multitask language understanding,\n",
            "arXiv preprint arXiv:2009.03300 (2020). 24, 29\n",
            "[298] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\n",
            "A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\n",
            "the imitation game: Quantifying and extrapolating the capabilities of\n",
            "language models, arXiv preprint arXiv:2206.04615 (2022). 24, 29\n",
            "[299] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue:\n",
            "A multi-task benchmark and analysis platform for natural language un-\n",
            "derstanding, arXiv preprint arXiv:1804.07461 (2018). 24, 29\n",
            "[300] Y. Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\n",
            "J. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\n",
            "eration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021).\n",
            "29\n",
            "[301] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu,\n",
            "C. Yu, et al., Clue: A chinese language understanding evaluation bench-\n",
            "mark, arXiv preprint arXiv:2004.05986 (2020). 29\n",
            "[302] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\n",
            "X. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\n",
            "benchmark, arXiv preprint arXiv:2107.07498 (2021). 29\n",
            "[303] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y.-L. Boureau, Can\n",
            "you put it all together: Evaluating conversational agents’ ability to blend\n",
            "skills, arXiv preprint arXiv:2004.08449 (2020). 29\n",
            "[304] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\n",
            "Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluation of\n",
            "language models, arXiv preprint arXiv:2211.09110 (2022). 29\n",
            "41\n",
            "[305] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song, J. Kim,\n",
            "Y. Song, T. Oh, et al., Klue: Korean language understanding evaluation,\n",
            "arXiv preprint arXiv:2105.09680 (2021). 29\n",
            "[306] S. Reddy, D. Chen, C. D. Manning, Coqa: A conversational question\n",
            "answering challenge, Transactions of the Association for Computational\n",
            "Linguistics 7 (2019) 249–266. 25, 29\n",
            "[307] M.\n",
            "T.\n",
            "Pilehvar,\n",
            "J.\n",
            "Camacho-Collados,\n",
            "Wic:\n",
            "10,000\n",
            "example\n",
            "pairs for evaluating context-sensitive representations, arXiv preprint\n",
            "arXiv:1808.09121 6 (2018). 25, 29\n",
            "[308] S. Merity, C. Xiong, J. Bradbury, R. Socher, Pointer sentinel mixture\n",
            "models, arXiv preprint arXiv:1609.07843 (2016). 25, 29\n",
            "[309] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\n",
            "sive transformers for long-range sequence modelling, arXiv preprint\n",
            "arXiv:1911.05507 (2019). 25, 29\n",
            "[310] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: A\n",
            "large-scale chinese question matching corpus, in: Proceedings of the\n",
            "27th international conference on computational linguistics, 2018, pp.\n",
            "1952–1962. 26, 29\n",
            "[311] S.\n",
            "Iyer,\n",
            "N.\n",
            "Dandekar,\n",
            "K.\n",
            "Csernai,\n",
            "First\n",
            "quora\n",
            "dataset\n",
            "re-\n",
            "lease:\n",
            "Question\n",
            "pairs,\n",
            "https://quoradata.quora.com/\n",
            "First-Quora-Dataset-Release-Question-Pairs. 29\n",
            "[312] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Gender bias in\n",
            "coreference resolution, arXiv preprint arXiv:1804.09301 (2018). 29\n",
            "[313] M.-C. De Marneffe, M. Simons, J. Tonhauser, The commitmentbank: In-\n",
            "vestigating projection in naturally occurring discourse, in: proceedings\n",
            "of Sinn und Bedeutung, Vol. 23, 2019, pp. 107–124. 29\n",
            "[314] Z. Li, N. Ding, Z. Liu, H. Zheng, Y. Shen, Chinese relation extraction\n",
            "with multi-grained information and external linguistic knowledge, in:\n",
            "Proceedings of the 57th Annual Meeting of the Association for Compu-\n",
            "tational Linguistics, 2019, pp. 4377–4386. 29\n",
            "[315] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\n",
            "and relation extraction dataset for chinese literature text, arXiv preprint\n",
            "arXiv:1711.07010 (2017). 29\n",
            "[316] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: A\n",
            "large-scale domain-specific chinese corpus for sentence semantic equiv-\n",
            "alence identification, in: Proceedings of the 2018 conference on empiri-\n",
            "cal methods in natural language processing, 2018, pp. 4946–4951. 29\n",
            "[317] B. Liu, D. Niu, H. Wei, J. Lin, Y. He, K. Lai, Y. Xu, Matching arti-\n",
            "cle pairs with graphical decomposition and convolutions, arXiv preprint\n",
            "arXiv:1802.07459 (2018). 29\n",
            "[318] P. Li, W. Li, Z. He, X. Wang, Y. Cao, J. Zhou, W. Xu, Dataset and neu-\n",
            "ral recurrent sequence labeling model for open-domain factoid question\n",
            "answering, arXiv preprint arXiv:1607.06275 (2016). 29\n",
            "[319] N. Peng, M. Dredze, Named entity recognition for chinese social media\n",
            "with jointly trained embeddings, in: Proceedings of the 2015 conference\n",
            "on empirical methods in natural language processing, 2015, pp. 548–\n",
            "554. 29\n",
            "[320] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Program induction by ratio-\n",
            "nale generation: Learning to solve and explain algebraic word problems,\n",
            "arXiv preprint arXiv:1705.04146 (2017). 29\n",
            "[321] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\n",
            "cus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et al., Ontonotes re-\n",
            "lease 4.0, LDC2011T03, Philadelphia, Penn.: Linguistic Data Consor-\n",
            "tium (2011). 29\n",
            "[322] D. Vilares, C. Gómez-Rodríguez, Head-qa: A healthcare dataset for\n",
            "complex reasoning, arXiv preprint arXiv:1906.04701 (2019). 29\n",
            "[323] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\n",
            "in social media: A case study of african-american english, arXiv preprint\n",
            "arXiv:1608.08868 (2016). 29\n",
            "[324] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\n",
            "derwende, P. Kohli, J. Allen, A corpus and evaluation framework\n",
            "for deeper understanding of commonsense stories, arXiv preprint\n",
            "arXiv:1604.01696 (2016). 26, 29\n",
            "[325] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\n",
            "S. Pezzelle, M. Baroni, G. Boleda, R. Fernández, The lambada dataset:\n",
            "Word prediction requiring a broad discourse context, arXiv preprint\n",
            "arXiv:1606.06031 (2016). 26, 29\n",
            "[326] B. Hu, Q. Chen, F. Zhu, Lcsts: A large scale chinese short text summa-\n",
            "rization dataset, arXiv preprint arXiv:1506.05865 (2015). 29\n",
            "[327] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\n",
            "ation with planning-based hierarchical variational model, arXiv preprint\n",
            "arXiv:1908.06605 (2019). 29\n",
            "[328] J. Novikova, O. Dušek, V. Rieser, The e2e dataset: New challenges for\n",
            "end-to-end generation, arXiv preprint arXiv:1706.09254 (2017). 29\n",
            "[329] C. Zheng, M. Huang, A. Sun, Chid: A large-scale chinese idiom dataset\n",
            "for cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\n",
            "[330] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al., Piqa: Reasoning about phys-\n",
            "ical commonsense in natural language, in: Proceedings of the AAAI\n",
            "conference on artificial intelligence, Vol. 34, 2020, pp. 7432–7439. 26,\n",
            "29\n",
            "[331] M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer, Triviaqa: A large scale\n",
            "distantly supervised challenge dataset for reading comprehension, arXiv\n",
            "preprint arXiv:1705.03551 (2017). 26, 29, 31\n",
            "[332] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\n",
            "O. Tafjord, Think you have solved question answering? try arc, the ai2\n",
            "reasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 26, 29,\n",
            "31\n",
            "[333] S. Aroca-Ouellette, C. Paik, A. Roncone, K. Kann, Prost:\n",
            "Phys-\n",
            "ical reasoning of objects through space and time, arXiv preprint\n",
            "arXiv:2106.03634 (2021). 29\n",
            "[334] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, Can a suit of armor con-\n",
            "duct electricity? a new dataset for open book question answering, arXiv\n",
            "preprint arXiv:1809.02789 (2018). 29\n",
            "[335] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\n",
            "D. Moussallem, A. Shimorina, The 2020 bilingual, bi-directional\n",
            "webnlg+ shared task overview and evaluation results (webnlg+ 2020),\n",
            "in: Proceedings of the 3rd International Workshop on Natural Language\n",
            "Generation from the Semantic Web (WebNLG+), 2020. 29\n",
            "[336] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Blow the dog whistle:\n",
            "A chinese dataset for cant understanding with common sense and world\n",
            "knowledge, arXiv preprint arXiv:2104.02704 (2021). 29\n",
            "[337] G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, Race:\n",
            "Large-scale\n",
            "reading comprehension dataset from examinations, arXiv preprint\n",
            "arXiv:1704.04683 (2017). 26, 29\n",
            "[338] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\n",
            "L. Zettlemoyer, Quac: Question answering in context, arXiv preprint\n",
            "arXiv:1808.07036 (2018). 27, 29\n",
            "[339] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, Did aristo-\n",
            "tle use a laptop? a question answering benchmark with implicit reason-\n",
            "ing strategies, Transactions of the Association for Computational Lin-\n",
            "guistics 9 (2021) 346–361. 29\n",
            "[340] J. Boyd-Graber, B. Satinoff, H. He, H. Daumé III, Besting the quiz mas-\n",
            "ter: Crowdsourcing incremental classification games, in: Proceedings of\n",
            "the 2012 joint conference on empirical methods in natural language pro-\n",
            "cessing and computational natural language learning, 2012, pp. 1290–\n",
            "1301. 29\n",
            "[341] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Chinese medical\n",
            "question answer matching using end-to-end character-level multi-scale\n",
            "cnns, Applied Sciences 7 (8) (2017) 767. 29\n",
            "[342] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive in-\n",
            "teraction networks for chinese medical question answer selection, IEEE\n",
            "Access 6 (2018) 74061–74071. 29\n",
            "[343] C. Xu, J. Pei, H. Wu, Y. Liu, C. Li, Matinf: A jointly labeled large-scale\n",
            "dataset for classification, question answering and summarization, arXiv\n",
            "preprint arXiv:2004.12302 (2020). 29\n",
            "[344] K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y. Choi, Winogrande: An\n",
            "adversarial winograd schema challenge at scale, Communications of the\n",
            "ACM 64 (9) (2021) 99–106. 25, 29\n",
            "[345] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: Can a\n",
            "machine really finish your sentence?, arXiv preprint arXiv:1905.07830\n",
            "(2019). 27, 29\n",
            "[346] M. Roemmele, C. A. Bejan, A. S. Gordon, Choice of plausible alter-\n",
            "natives: An evaluation of commonsense causal reasoning., in: AAAI\n",
            "spring symposium: logical formalizations of commonsense reasoning,\n",
            "2011, pp. 90–95. 29\n",
            "[347] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\n",
            "lenge, in: Thirteenth international conference on the principles of knowl-\n",
            "edge representation and reasoning, 2012. 25, 27, 29\n",
            "[348] A. Talmor, J. Herzig, N. Lourie, J. Berant, Commonsenseqa: A question\n",
            "answering challenge targeting commonsense knowledge, arXiv preprint\n",
            "arXiv:1811.00937 (2018). 27, 29\n",
            "[349] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y. Choi, Socialiqa:\n",
            "42\n",
            "Commonsense reasoning about social interactions, arXiv preprint\n",
            "arXiv:1904.09728 (2019). 29\n",
            "[350] K. Sun, D. Yu, D. Yu, C. Cardie, Investigating prior knowledge for chal-\n",
            "lenging chinese machine reading comprehension, Transactions of the\n",
            "Association for Computational Linguistics 8 (2020) 141–155. 29\n",
            "[351] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\n",
            "ing the gap between human and machine commonsense reading compre-\n",
            "hension, arXiv preprint arXiv:1810.12885 (2018). 29\n",
            "[352] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000+ questions\n",
            "for machine comprehension of text, arXiv preprint arXiv:1606.05250\n",
            "(2016). 28, 29\n",
            "[353] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,\n",
            "K. Toutanova, Boolq: Exploring the surprising difficulty of natural\n",
            "yes/no questions, arXiv preprint arXiv:1905.10044 (2019). 28, 29\n",
            "[354] P. Rajpurkar, R. Jia, P. Liang, Know what you don’t know: Unanswer-\n",
            "able questions for squad, arXiv preprint arXiv:1806.03822 (2018). 28,\n",
            "29\n",
            "[355] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop:\n",
            "A reading comprehension benchmark requiring discrete reasoning over\n",
            "paragraphs, arXiv preprint arXiv:1903.00161 (2019). 28, 29\n",
            "[356] I. Dagan, O. Glickman, B. Magnini, The pascal recognising textual en-\n",
            "tailment challenge, in: Machine learning challenges workshop, Springer,\n",
            "2005, pp. 177–190. 28, 29\n",
            "[357] Y. Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, Y. Bisk, Webqa: Mul-\n",
            "tihop and multimodal qa, in: Proceedings of the IEEE/CVF Conference\n",
            "on Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\n",
            "28, 29\n",
            "[358] Y. Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Dataset for the first\n",
            "evaluation on chinese machine reading comprehension, arXiv preprint\n",
            "arXiv:1709.08299 (2017). 29\n",
            "[359] Y. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu,\n",
            "A span-extraction dataset for chinese machine reading comprehension,\n",
            "arXiv preprint arXiv:1810.07366 (2018). 28, 29\n",
            "[360] Y. Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu,\n",
            "A sentence cloze dataset for chinese machine reading comprehension,\n",
            "arXiv preprint arXiv:2004.03116 (2020). 29\n",
            "[361] Y. Li, T. Liu, D. Li, Q. Li, J. Shi, Y. Wang, Character-based bilstm-crf\n",
            "incorporating pos and dictionaries for chinese opinion target extraction,\n",
            "in: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\n",
            "29\n",
            "[362] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Look-\n",
            "ing beyond the surface: A challenge set for reading comprehension\n",
            "over multiple sentences, in: Proceedings of the 2018 Conference of the\n",
            "North American Chapter of the Association for Computational Linguis-\n",
            "tics: Human Language Technologies, Volume 1 (Long Papers), 2018,\n",
            "pp. 252–262. 29\n",
            "[363] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Al-\n",
            "berti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., Natural ques-\n",
            "tions: a benchmark for question answering research, Transactions of the\n",
            "Association for Computational Linguistics 7 (2019) 453–466. 29\n",
            "[364] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, S. Tsai, Drcd: A chinese ma-\n",
            "chine reading comprehension dataset, arXiv preprint arXiv:1806.00920\n",
            "(2018). 29\n",
            "[365] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu,\n",
            "Q. She, et al., Dureader: a chinese machine reading comprehension\n",
            "dataset from real-world applications, arXiv preprint arXiv:1711.05073\n",
            "(2017). 29\n",
            "[366] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: A\n",
            "chinese dataset towards evaluating the robustness of machine reading\n",
            "comprehension models, arXiv preprint arXiv:2004.11142 (2020). 29\n",
            "[367] J. Welbl, N. F. Liu, M. Gardner, Crowdsourcing multiple choice science\n",
            "questions, arXiv preprint arXiv:1707.06209 (2017). 29\n",
            "[368] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\n",
            "ranking with kernel pooling, in: Proceedings of the 40th International\n",
            "ACM SIGIR conference on research and development in information\n",
            "retrieval, 2017, pp. 55–64. 29\n",
            "[369] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, R. Morante,\n",
            "Qa4mre 2011-2013: Overview of question answering for machine read-\n",
            "ing evaluation, in: Information Access Evaluation. Multilinguality, Mul-\n",
            "timodality, and Visualization: 4th International Conference of the CLEF\n",
            "Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Pro-\n",
            "ceedings 4, Springer, 2013, pp. 303–320. 29\n",
            "[370] S. Lim, M. Kim, J. Lee, Korquad1. 0: Korean qa dataset for machine\n",
            "reading comprehension, arXiv preprint arXiv:1909.07005 (2019). 29\n",
            "[371] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y. Feng, X. Han,\n",
            "Z. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\n",
            "ment prediction, arXiv preprint arXiv:1807.02478 (2018). 29\n",
            "[372] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\n",
            "C. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge\n",
            "competence with apps, arXiv preprint arXiv:2105.09938 (2021). 28, 29\n",
            "[373] Y. Wang, X. Liu, S. Shi, Deep neural solver for math word problems,\n",
            "in: Proceedings of the 2017 conference on empirical methods in natural\n",
            "language processing, 2017, pp. 845–854. 28, 29\n",
            "[374] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\n",
            "M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers\n",
            "to solve math word problems, arXiv preprint arXiv:2110.14168 (2021).\n",
            "29\n",
            "[375] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\n",
            "E. Jiang, C. J. Cai, M. Terry, Q. V. Le, C. Sutton, Program synthesis with\n",
            "large language models, CoRR abs/2108.07732 (2021). 29\n",
            "[376] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W.\n",
            "Chung, Y. Tay, S. Ruder, D. Zhou, et al., Language models are mul-\n",
            "tilingual chain-of-thought reasoners, arXiv preprint arXiv:2210.03057\n",
            "(2022). 29\n",
            "[377] S. Roy, D. Roth, Solving general arithmetic word problems, arXiv\n",
            "preprint arXiv:1608.01413 (2016). 29\n",
            "[378] S.-Y. Miao, C.-C. Liang, K.-Y. Su, A diverse corpus for evaluating\n",
            "and developing english math word problem solvers, arXiv preprint\n",
            "arXiv:2106.15772 (2021). 29\n",
            "[379] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi,\n",
            "Mawps: A math word problem repository, in: Proceedings of the 2016\n",
            "conference of the north american chapter of the association for computa-\n",
            "tional linguistics: human language technologies, 2016, pp. 1152–1157.\n",
            "29\n",
            "[380] A. Patel, S. Bhattamishra, N. Goyal, Are nlp models really able to solve\n",
            "simple math word problems?, arXiv preprint arXiv:2103.07191 (2021).\n",
            "29\n",
            "[381] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih,\n",
            "D. Fried, S. Wang, T. Yu, Ds-1000: A natural and reliable benchmark for\n",
            "data science code generation, in: International Conference on Machine\n",
            "Learning, PMLR, 2023, pp. 18319–18345. 29\n",
            "[382] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\n",
            "E. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large\n",
            "language models, arXiv preprint arXiv:2108.07732 (2021). 29\n",
            "[383] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Adver-\n",
            "sarial nli: A new benchmark for natural language understanding, arXiv\n",
            "preprint arXiv:1910.14599 (2019). 29\n",
            "[384] A. Williams, N. Nangia, S. R. Bowman, A broad-coverage challenge\n",
            "corpus for sentence understanding through inference, arXiv preprint\n",
            "arXiv:1704.05426 (2017). 29\n",
            "[385] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong reasons: Diag-\n",
            "nosing syntactic heuristics in natural language inference, arXiv preprint\n",
            "arXiv:1902.01007 (2019). 29\n",
            "[386] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, Y. Zhang, Logiqa: A chal-\n",
            "lenge dataset for machine reading comprehension with logical reason-\n",
            "ing, arXiv preprint arXiv:2007.08124 (2020). 29\n",
            "[387] P. Lewis, B. O˘\n",
            "guz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Eval-\n",
            "uating cross-lingual extractive question answering, arXiv preprint\n",
            "arXiv:1910.07475 (2019). 29\n",
            "[388] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\n",
            "H. Schwenk, V. Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\n",
            "resentations, arXiv preprint arXiv:1809.05053 (2018). 29\n",
            "[389] Y. Yang,\n",
            "Y. Zhang,\n",
            "C. Tar,\n",
            "J. Baldridge,\n",
            "Paws-x:\n",
            "A cross-\n",
            "lingual adversarial dataset for paraphrase identification, arXiv preprint\n",
            "arXiv:1908.11828 (2019). 29\n",
            "[390] S. Narayan, S. B. Cohen, M. Lapata, Don’t give me the details, just the\n",
            "summary!, Topic-Aware Convolutional Neural Networks for Extreme\n",
            "Summarization. ArXiv, abs (1808). 29\n",
            "[391] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli´\n",
            "c, A. Korhonen,\n",
            "Xcopa: A multilingual dataset for causal commonsense reasoning, arXiv\n",
            "preprint arXiv:2005.00333 (2020). 27, 29\n",
            "[392] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\n",
            "43\n",
            "as a baseline for cross-lingual transfer in commonsense reasoning, arXiv\n",
            "preprint arXiv:2106.12066 (2021). 29\n",
            "[393] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Niko-\n",
            "laev, J. Palomaki, Tydi qa: A benchmark for information-seeking ques-\n",
            "tion answering in typologically diverse languages, Transactions of the\n",
            "Association for Computational Linguistics 8 (2020) 454–470. 29\n",
            "[394] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,\n",
            "Mlsum:\n",
            "The multilingual summarization corpus,\n",
            "arXiv preprint\n",
            "arXiv:2004.14900 (2020). 29\n",
            "[395] S. Lin, J. Hilton, O. Evans, Truthfulqa: Measuring how models mimic\n",
            "human falsehoods, arXiv preprint arXiv:2109.07958 (2021). 29\n",
            "[396] I. Augenstein,\n",
            "C. Lioma,\n",
            "D. Wang,\n",
            "L. C. Lima,\n",
            "C. Hansen,\n",
            "C. Hansen, J. G. Simonsen, Multifc:\n",
            "A real-world multi-domain\n",
            "dataset for evidence-based fact checking of claims, arXiv preprint\n",
            "arXiv:1909.03242 (2019). 29\n",
            "[397] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fever: a\n",
            "large-scale dataset for fact extraction and verification, arXiv preprint\n",
            "arXiv:1803.05355 (2018). 29\n",
            "[398] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\n",
            "hate speech detection dataset, arXiv preprint arXiv:2006.08328 (2020).\n",
            "29, 31\n",
            "[399] M. Nadeem, A. Bethke, S. Reddy, Stereoset: Measuring stereotypical\n",
            "bias in pretrained language models, arXiv preprint arXiv:2004.09456\n",
            "(2020). 29, 31\n",
            "[400] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thomp-\n",
            "son, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\n",
            "question answering, arXiv preprint arXiv:2110.08193 (2021). 29\n",
            "[401] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, K.-W. Chang, Gender bias\n",
            "in coreference resolution: Evaluation and debiasing methods, arXiv\n",
            "preprint arXiv:1804.06876 (2018). 29\n",
            "[402] N. Nangia, C. Vania, R. Bhalerao, S. R. Bowman, Crows-pairs: A chal-\n",
            "lenge dataset for measuring social biases in masked language models,\n",
            "arXiv preprint arXiv:2010.00133 (2020). 29\n",
            "[403] S. Gehman, S. Gururangan, M. Sap, Y. Choi, N. A. Smith, Realtoxic-\n",
            "ityprompts: Evaluating neural toxic degeneration in language models,\n",
            "arXiv preprint arXiv:2009.11462 (2020). 29\n",
            "[404] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Nuanced\n",
            "metrics for measuring unintended bias with real data for text classifica-\n",
            "tion, in: Companion proceedings of the 2019 world wide web confer-\n",
            "ence, 2019, pp. 491–500. 29\n",
            "[405] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\n",
            "M. Huck, A. J. Yepes, P. Koehn, V. Logacheva, C. Monz, et al., Find-\n",
            "ings of the 2016 conference on machine translation, in: Proceedings of\n",
            "the First Conference on Machine Translation: Volume 2, Shared Task\n",
            "Papers, 2016, pp. 131–198. 29\n",
            "[406] B. Loïc, B. Magdalena, B. Ondˇ\n",
            "rej, F. Christian, G. Yvette, G. Ro-\n",
            "man, H. Barry, H. Matthias, J. Eric, K. Tom, et al., Findings of the\n",
            "2020 conference on machine translation (wmt20), in: Proceedings of\n",
            "the Fifth Conference on Machine Translation, Association for Compu-\n",
            "tational Linguistics„ 2020, pp. 1–55. 29\n",
            "[407] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: A chinese classical poetry\n",
            "matching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\n",
            "[408] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Wizard of\n",
            "wikipedia: Knowledge-powered conversational agents, arXiv preprint\n",
            "arXiv:1811.01241 (2018). 29\n",
            "[409] H. Rashkin, E. M. Smith, M. Li, Y.-L. Boureau, Towards empathetic\n",
            "open-domain conversation models: A new benchmark and dataset, arXiv\n",
            "preprint arXiv:1811.00207 (2018). 29\n",
            "[410] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,\n",
            "D. Kiela, A. Szlam, I. Serban, R. Lowe, et al., The second conversa-\n",
            "tional intelligence challenge (convai2), in: The NeurIPS’18 Competi-\n",
            "tion: From Machine Learning to Intelligent Conversations, Springer,\n",
            "2020, pp. 187–208. 29\n",
            "[411] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: A chinese\n",
            "multi-domain dialogue dataset towards multi-turn knowledge-driven\n",
            "conversation, arXiv preprint arXiv:2004.04100 (2020). 29\n",
            "[412] L. CO, Iflytek: a multiple categories chinese text classifier. competition\n",
            "official website (2019). 29\n",
            "[413] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The\n",
            "pushshift reddit dataset, in: Proceedings of the international AAAI con-\n",
            "ference on web and social media, Vol. 14, 2020, pp. 830–839. 30\n",
            "[414] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\n",
            "form question answering, arXiv preprint arXiv:1907.09190 (2019). 31\n",
            "[415] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\n",
            "A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al.,\n",
            "Benchmarking generalization via in-context instructions on 1,600+ lan-\n",
            "guage tasks, arXiv preprint arXiv:2204.07705 (2022). 31\n",
            "[416] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\n",
            "M. Zhong, P. Yin, S. I. Wang, et al., Unifiedskg: Unifying and multi-\n",
            "tasking structured knowledge grounding with text-to-text language mod-\n",
            "els, arXiv preprint arXiv:2201.05966 (2022). 31\n",
            "[417] Q. Ye, B. Y. Lin, X. Ren, Crossfit: A few-shot learning challenge\n",
            "for cross-task generalization in nlp, arXiv preprint arXiv:2104.08835\n",
            "(2021). 31\n",
            "[418] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V. Mehta,\n",
            "H. Zhuang, V. Q. Tran, D. Bahri, J. Ni, et al., Ext5: Towards extreme\n",
            "multi-task scaling for transfer learning, arXiv preprint arXiv:2111.10952\n",
            "(2021). 31\n",
            "[419] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\n",
            "pus for sentence understanding through inference, in: Proceedings of\n",
            "the 2018 Conference of the North American Chapter of the Associ-\n",
            "ation for Computational Linguistics: Human Language Technologies,\n",
            "Volume 1 (Long Papers), Association for Computational Linguistics,\n",
            "New Orleans, Louisiana, 2018, pp. 1112–1122. doi:10.18653/v1/\n",
            "N18-1101.\n",
            "URL https://aclanthology.org/N18-1101 29\n",
            "[420] Y. Zhang, J. Baldridge, L. He, PAWS: Paraphrase adversaries from word\n",
            "scrambling, in: Proceedings of the 2019 Conference of the North Amer-\n",
            "ican Chapter of the Association for Computational Linguistics: Human\n",
            "Language Technologies, Volume 1 (Long and Short Papers), Associa-\n",
            "tion for Computational Linguistics, Minneapolis, Minnesota, 2019, pp.\n",
            "1298–1308. doi:10.18653/v1/N19-1131.\n",
            "URL https://aclanthology.org/N19-1131 29\n",
            "[421] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, Is chat-\n",
            "GPT a general-purpose natural language processing task solver?, in: The\n",
            "2023 Conference on Empirical Methods in Natural Language Process-\n",
            "ing, 2023.\n",
            "URL https://openreview.net/forum?id=u03xn1COsO 31\n",
            "[422] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\n",
            "N. Akhtar, J. Wu, S. Mirjalili, et al., Large language models: a com-\n",
            "prehensive survey of its applications, challenges, limitations, and future\n",
            "prospects, TechRxiv (2023). 31\n",
            "[423] X. L. Dong, S. Moon, Y. E. Xu, K. Malik, Z. Yu, Towards next-\n",
            "generation intelligent assistants leveraging llm techniques, in: Proceed-\n",
            "ings of the 29th ACM SIGKDD Conference on Knowledge Discovery\n",
            "and Data Mining, 2023, pp. 5792–5793. 31\n",
            "[424] K. Pandya, M. Holia, Automating customer service using langchain:\n",
            "Building custom open-source gpt chatbot for organizations, arXiv\n",
            "preprint arXiv:2310.05421 (2023). 31\n",
            "[425] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\n",
            "R. Geng, et al., Can llm already serve as a database interface?\n",
            "a\n",
            "big bench for large-scale database grounded text-to-sqls, arXiv preprint\n",
            "arXiv:2305.03111 (2023). 31\n",
            "[426] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, M. D. Succi, Evaluating\n",
            "chatgpt as an adjunct for radiologic decision-making, medRxiv (2023)\n",
            "2023–02. 31\n",
            "[427] M. Benary, X. D. Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas-\n",
            "sir, C. Sigler, M. Knödler, U. Keller, D. Beule, et al., Leveraging large\n",
            "language models for decision support in personalized oncology, JAMA\n",
            "Network Open 6 (11) (2023) e2343689–e2343689. 31\n",
            "[428] C. M. Chiesa-Estomba, J. R. Lechien, L. A. Vaira, A. Brunet, G. Cam-\n",
            "maroto, M. Mayo-Yanez, A. Sanchez-Barrueco, C. Saga-Gutierrez, Ex-\n",
            "ploring the potential of chat-gpt as a supportive tool for sialendoscopy\n",
            "clinical decision making and patient information support, European\n",
            "Archives of Oto-Rhino-Laryngology (2023) 1–6. 31\n",
            "[429] S. Montagna, S. Ferretti, L. C. Klopfenstein, A. Florio, M. F. Pengo,\n",
            "Data decentralisation of llm-based chatbot systems in chronic disease\n",
            "self-management, in: Proceedings of the 2023 ACM Conference on In-\n",
            "formation Technology for Social Good, 2023, pp. 205–212. 31\n",
            "[430] D. Bill, T. Eriksson, Fine-tuning a llm using reinforcement learning from\n",
            "human feedback for a therapy chatbot application (2023). 31\n",
            "[431] M. Abbasian, I. Azimi, A. M. Rahmani, R. Jain, Conversational health\n",
            "44\n",
            "agents: A personalized llm-powered agent framework, arXiv preprint\n",
            "arXiv:2310.02374 (2023). 31\n",
            "[432] K. V. Lemley, Does chatgpt help us understand the medical literature?,\n",
            "Journal of the American Society of Nephrology (2023) 10–1681. 31\n",
            "[433] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\n",
            "next-generation large language model (llm) or chatgpt is required for\n",
            "biomedical engineering and research, Annals of Biomedical Engineering\n",
            "(2023) 1–4. 31\n",
            "[434] Y. Du, S. Zhao, Y. Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The\n",
            "calla dataset: Probing llms’ interactive knowledge acquisition from chi-\n",
            "nese medical literature, arXiv preprint arXiv:2309.04198 (2023). 31\n",
            "[435] A. Abd-Alrazaq, R. AlSaad, D. Alhuwail, A. Ahmed, P. M. Healy,\n",
            "S. Latifi, S. Aziz, R. Damseh, S. A. Alrazak, J. Sheikh, et al., Large\n",
            "language models in medical education: Opportunities, challenges, and\n",
            "future directions, JMIR Medical Education 9 (1) (2023) e48291. 31\n",
            "[436] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, A. Dagan,\n",
            "Chatgpt passing usmle shines a spotlight on the flaws of medical educa-\n",
            "tion (2023). 31\n",
            "[437] S. Ahn, The impending impacts of large language models on medical\n",
            "education, Korean Journal of Medical Education 35 (1) (2023) 103. 31\n",
            "[438] E. Waisberg, J. Ong, M. Masalkhi, A. G. Lee, Large language model\n",
            "(llm)-driven chatbots for neuro-ophthalmic medical education, Eye\n",
            "(2023) 1–3. 31\n",
            "[439] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\n",
            "Artificial intelligence and public health: Evaluating chatgpt responses to\n",
            "vaccination myths and misconceptions, Vaccines 11 (7) (2023) 1217. 31\n",
            "[440] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\n",
            "Tozzi, C. Rizzo, Chatgpt and the rise of large language models: the new\n",
            "ai-driven infodemic threat in public health, Frontiers in Public Health 11\n",
            "(2023) 1166120. 31\n",
            "[441] N. L. Rane, A. Tawde, S. P. Choudhary, J. Rane, Contribution and per-\n",
            "formance of chatgpt and other large language models (llm) for scientific\n",
            "and research advancements: a double-edged sword, International Re-\n",
            "search Journal of Modernization in Engineering Technology and Science\n",
            "5 (10) (2023) 875–899. 31, 32\n",
            "[442] W. Dai, J. Lin, H. Jin, T. Li, Y.-S. Tsai, D. Gaševi´\n",
            "c, G. Chen, Can large\n",
            "language models provide feedback to students? a case study on chatgpt,\n",
            "in: 2023 IEEE International Conference on Advanced Learning Tech-\n",
            "nologies (ICALT), IEEE, 2023, pp. 323–325. 32\n",
            "[443] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\n",
            "F. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.,\n",
            "Chatgpt for good? on opportunities and challenges of large language\n",
            "models for education, Learning and individual differences 103 (2023)\n",
            "102274. 32\n",
            "[444] N. Rane, Enhancing the quality of teaching and learning through chat-\n",
            "gpt and similar large language models: Challenges, future prospects,\n",
            "and ethical considerations in education, Future Prospects, and Ethical\n",
            "Considerations in Education (September 15, 2023) (2023). 32\n",
            "[445] J. C. Young, M. Shishido, Investigating openai’s chatgpt potentials in\n",
            "generating chatbot’s dialogue for english as a foreign language learning,\n",
            "International Journal of Advanced Computer Science and Applications\n",
            "14 (6) (2023). 32\n",
            "[446] J. Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Exploring\n",
            "the impacts of chatgpt on future scientific work, SocArXiv (2023). 32\n",
            "[447] P. G. Schmidt, A. J. Meir, Using generative ai for literature searches and\n",
            "scholarly writing: Is the integrity of the scientific discourse in jeopardy?,\n",
            "arXiv preprint arXiv:2311.06981 (2023). 32\n",
            "[448] Y. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, S. Pan,\n",
            "Large language models for scientific synthesis, inference and explana-\n",
            "tion, arXiv preprint arXiv:2310.07984 (2023). 32\n",
            "[449] B. Aczel, E.-J. Wagenmakers, Transparency guidance for chatgpt usage\n",
            "in scientific writing, PsyArXiv (2023). 32\n",
            "[450] S. Altmäe, A. Sola-Leyva, A. Salumets, Artificial intelligence in sci-\n",
            "entific writing: a friend or a foe?, Reproductive BioMedicine Online\n",
            "(2023). 32\n",
            "[451] S. Imani, L. Du, H. Shrivastava, Mathprompter: Mathematical reasoning\n",
            "using large language models, arXiv preprint arXiv:2303.05398 (2023).\n",
            "32\n",
            "[452] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Scaling relationship\n",
            "on learning mathematical reasoning with large language models, arXiv\n",
            "preprint arXiv:2308.01825 (2023). 32\n",
            "[453] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil,\n",
            "R. Prenger, A. Anandkumar, Leandojo: Theorem proving with retrieval-\n",
            "augmented language models, arXiv preprint arXiv:2306.15626 (2023).\n",
            "32\n",
            "[454] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\n",
            "T. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart, et al., Evaluating\n",
            "language models for mathematics through interactions, arXiv preprint\n",
            "arXiv:2306.01694 (2023). 32\n",
            "[455] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He,\n",
            "Z. Liu, et al., Summary of chatgpt-related research and perspective\n",
            "towards the future of large language models, Meta-Radiology (2023)\n",
            "100017. 32\n",
            "[456] J. Drápal, H. Westermann, J. Savelka, Using large language models\n",
            "to support thematic analysis in empirical legal studies, arXiv preprint\n",
            "arXiv:2310.18729 (2023). 32\n",
            "[457] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann, H. Xu, Explain-\n",
            "ing legal concepts with augmented large language models (gpt-4), arXiv\n",
            "preprint arXiv:2306.09525 (2023). 32\n",
            "[458] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,\n",
            "A. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, et al., Legal-\n",
            "bench: A collaboratively built benchmark for measuring legal reasoning\n",
            "in large language models, arXiv preprint arXiv:2308.11462 (2023). 32\n",
            "[459] J. Cui, Z. Li, Y. Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\n",
            "large language model with integrated external knowledge bases, arXiv\n",
            "preprint arXiv:2306.16092 (2023). 32\n",
            "[460] H. Yang, X.-Y. Liu, C. D. Wang, Fingpt: Open-source financial large\n",
            "language models, arXiv preprint arXiv:2306.06031 (2023). 32\n",
            "[461] Y. Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\n",
            "survey, in: Proceedings of the Fourth ACM International Conference on\n",
            "AI in Finance, 2023, pp. 374–382. 33\n",
            "[462] A. Lykov, D. Tsetserukou, Llm-brain: Ai-driven fast generation of\n",
            "robot behaviour tree based on large language model, arXiv preprint\n",
            "arXiv:2305.19352 (2023). 33\n",
            "[463] E. Billing, J. Rosén, M. Lamb, Language models for human-robot inter-\n",
            "action, in: ACM/IEEE International Conference on Human-Robot Inter-\n",
            "action, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\n",
            "2023, pp. 905–906. 33\n",
            "[464] Y. Ye, H. You, J. Du, Improved trust in human-robot collaboration with\n",
            "chatgpt, IEEE Access (2023). 33\n",
            "[465] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Leveraging commonsense\n",
            "knowledge from large language models for task and motion planning,\n",
            "in: RSS 2023 Workshop on Learning for Task and Motion Planning,\n",
            "2023. 33\n",
            "[466] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\n",
            "S. Rusinkiewicz, T. Funkhouser, Tidybot: Personalized robot assistance\n",
            "with large language models, arXiv preprint arXiv:2305.05658 (2023).\n",
            "33\n",
            "[467] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations\n",
            "for deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). 33\n",
            "[468] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dan-\n",
            "gers of stochastic parrots: Can language models be too big?, in: Pro-\n",
            "ceedings of the 2021 ACM conference on fairness, accountability, and\n",
            "transparency, 2021, pp. 610–623. 33\n",
            "[469] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding\n",
            "deep learning (still) requires rethinking generalization, Communications\n",
            "of the ACM 64 (3) (2021) 107–115. 33\n",
            "[470] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\n",
            "trained language models, arXiv preprint arXiv:2105.00828 (2021). 33\n",
            "[471] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI\n",
            "Now (2019) 1–33. 33\n",
            "[472] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Large lan-\n",
            "guage models still can’t plan (a benchmark for llms on planning and\n",
            "reasoning about change), arXiv preprint arXiv:2206.10498 (2022). 33\n",
            "[473] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\n",
            "Y. Zhang, Y. Chen, et al., Siren’s song in the ai ocean: A survey on hal-\n",
            "lucination in large language models, arXiv preprint arXiv:2309.01219\n",
            "(2023). 33\n",
            "[474] A. Webson, E. Pavlick, Do prompt-based models really understand the\n",
            "meaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 33\n",
            "[475] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, On second\n",
            "thought, let’s not think step by step! bias and toxicity in zero-shot rea-\n",
            "45\n",
            "soning, arXiv preprint arXiv:2212.08061 (2022). 33\n",
            "[476] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Adversar-\n",
            "ial training for large neural language models, ArXiv (April 2020).\n",
            "URL\n",
            "https://www.microsoft.com/en-us/research/\n",
            "publication/adversarial-training-for-large-neural-language-models/\n",
            "34\n",
            "[477] E. Shayegani, M. A. A. Mamun, Y. Fu, P. Zaree, Y. Dong, N. Abu-\n",
            "Ghazaleh, Survey of vulnerabilities in large language models revealed\n",
            "by adversarial attacks (2023). arXiv:2310.10844. 34\n",
            "[478] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, M. Kankanhalli, An\n",
            "llm can fool itself: A prompt-based adversarial attack (2023). arXiv:\n",
            "2310.13345. 34\n",
            "[479] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\n",
            "M. Du, Explainability for large language models: A survey (2023).\n",
            "arXiv:2309.01029. 34\n",
            "[480] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, L. H. Gilpin, Can large\n",
            "language models explain themselves? a study of llm-generated self-\n",
            "explanations (2023). arXiv:2310.11207. 34\n",
            "[481] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramèr, What does it\n",
            "mean for a language model to preserve privacy?, in: Proceedings of the\n",
            "2022 ACM Conference on Fairness, Accountability, and Transparency,\n",
            "2022, pp. 2280–2292. 34\n",
            "[482] R. Plant, V. Giuffrida, D. Gkatzia, You are what you write:\n",
            "Pre-\n",
            "serving privacy in the era of large language models, arXiv preprint\n",
            "arXiv:2204.09391 (2022). 34\n",
            "[483] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu,\n",
            "B. Ren, Y. Wang, Real-time execution of large-scale language models\n",
            "on mobile (2020). arXiv:2009.06823. 34\n",
            "[484] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y. Liu, M. Guo,\n",
            "Y. Zhu, Olive:\n",
            "Accelerating large language models via hardware-\n",
            "friendly outlier-victim pair quantization, in: Proceedings of the 50th\n",
            "Annual International Symposium on Computer Architecture, 2023, pp.\n",
            "1–15. 34\n",
            "[485] B. Meskó, E. J. Topol, The imperative for regulatory oversight of large\n",
            "language models (or generative ai) in healthcare, npj Digital Medicine\n",
            "6 (1) (2023) 120. 34\n",
            "[486] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.-K. R. Choo, Ethical considerations\n",
            "and policy implications for large language models: Guiding responsible\n",
            "development and deployment, arXiv preprint arXiv:2308.02678 (2023).\n",
            "34\n",
            "[487] J. Mökander, J. Schuett, H. R. Kirk, L. Floridi, Auditing large language\n",
            "models: a three-layered approach, AI and Ethics (2023) 1–31. 34\n",
            "46\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Upload pdf\n",
        "# pip install PyMuPDF\n",
        "import fitz\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    document=fitz.open(pdf_path)\n",
        "    text=\"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page=document.load_page(page_num)\n",
        "        text+=page.get_text()\n",
        "    return text\n",
        "\n",
        "pdf_path=\"data/llm_models.pdf\"\n",
        "text=extract_text_from_pdf(pdf_path)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The Pmarca Blog Archives\n",
            "(select posts from 2007-2009)\n",
            "Marc Andreessen\n",
            "copyright: Andreessen Horow\n"
          ]
        }
      ],
      "source": [
        "print(documents[0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one document - and it's the entire text of Frakenstein\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0DV5Yx5R6yu"
      },
      "source": [
        "Let's take a peek visually at what we're doing here - and why it might be useful:\n",
        "\n",
        "<img src=\"https://i.imgur.com/rtM6Ci6.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can see (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "373"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter()\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\ufeff\\nThe Pmarca Blog Archives\\n(select posts from 2007-2009)\\nMarc Andreessen\\ncopyright: Andreessen Horowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: Why not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The only thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too little? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA GUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional CEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: Turnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why 120\\nThe Pmarca Guide to Personal Productivi']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format.\n",
        "\n",
        "Loosely, this means turning the text into numbers.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embeddings and Dense Vector Search: A Quick Primer\n",
        "\n",
        "If you come from an NLP background, embeddings are something you might be intimately familiar with - otherwise, you might find the topic a bit...dense. (this attempt at a joke will make more sense later)\n",
        "\n",
        "In all seriousness, embeddings are a powerful piece of the NLP puzzle, so let's dive in!\n",
        "\n",
        "> NOTE: While this notebook language/NLP-centric, embeddings have uses beyond just text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Do We Even Need Embeddings?\n",
        "\n",
        "In order to fully understand what Embeddings are, we first need to understand why we have them!\n",
        "\n",
        "Machine Learning algorithms, ranging from the very big to the very small, all have one thing in common:\n",
        "\n",
        "They need numeric inputs.\n",
        "\n",
        "So we need a process by which to translate the domain we live in, dominated by images, audio, language, and more, into the domain of the machine: Numbers.\n",
        "\n",
        "Another thing we want to be able to do is capture \"semantic information\" about words/phrases so that we can use algorithmic approaches to determine if words are closely related or not!\n",
        "\n",
        "So, we need to come up with a process that does these two things well:\n",
        "\n",
        "- Convert non-numeric data into numeric-data\n",
        "- Capture potential semantic relationships between individual pieces of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How Do Embeddings Capture Semantic Relationships?\n",
        "\n",
        "In a simplified sense, embeddings map a word or phrase into n-dimensional space with a dense continuous vector, where each dimension in the vector represents some \"latent feature\" of the data.\n",
        "\n",
        "This is best represented in a classic example:\n",
        "\n",
        "![image](https://i.imgur.com/K5eQtmH.png)\n",
        "\n",
        "As can be seen in the extremely simplified example: The X_1 axis represents age, and the X_2 axis represents hair.\n",
        "\n",
        "The relationship of \"puppy -> dog\" reflects the same relationship as \"baby -> adult\", but dogs are (typically) hairier than humans. However, adults typically have more hair than babies - so they are shifted slightly closer to dogs on the X_2 axis!\n",
        "\n",
        "Now, this is a simplified and contrived example - but it is *essentially* the mechanism by which embeddings capture semantic information.\n",
        "\n",
        "In reality, the dimensions don't sincerely represent hard-concepts like \"age\" or \"hair\", but it's useful as a way to think about how the semantic relationships are captured.\n",
        "\n",
        "Alright, with some history behind us - let's examine how these might help us choose relevant context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's begin with a simple example - simply looking at how close to embedding vectors are for a given phrase.\n",
        "\n",
        "When we use the term \"close\" in this notebook - we're referring to a distance measure called \"cosine similarity\".\n",
        "\n",
        "We discussed above that if two embeddings are close - they are semantically similar, cosine similarity gives us a quick way to measure how similar two vectors are!\n",
        "\n",
        "Closeness is measured from 1 to -1, with 1 being extremely close and -1 being extremely close to opposite in meaning.\n",
        "\n",
        "Let's implement it with Numpy below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec_1, vec_2):\n",
        "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))\n",
        "\n",
        "def euclidean_similarity(vec1, vec2):\n",
        "    temp=np.array(vec1)-np.array(vec2)\n",
        "    return np.sqrt(np.dot(temp.T, temp))\n",
        "    #return np.linalg.norm(vec1-vec2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's use the `text-embedding-3-small` embedding model (more on that in a second) to embed two sentences. In order to use this embedding model endpoint - we'll need to provide our OpenAI API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaOETZGpR6yv",
        "outputId": "1239abf1-faff-49f2-a67c-7350e50fb1b9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
        "\n",
        "embedding_model = EmbeddingModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define our two sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "puppy_sentence = \"I love puppies!\"\n",
        "dog_sentence = \"I love dogs!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can convert those into embedding vectors using OpenAI!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
        "dog_vector = embedding_model.get_embedding(dog_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can determine how closely they are related using our distance measure!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8340215662150428\n",
            "0.5761569954908198\n"
          ]
        }
      ],
      "source": [
        "print(cosine_similarity(puppy_vector, dog_vector))\n",
        "print(euclidean_similarity(puppy_vector,dog_vector))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember, with cosine similarity, close to 1. means they're very close!\n",
        "\n",
        "Let's see what happens if we use a different set of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3723958073309947"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "puppy_sentence = \"I love puppies!\"\n",
        "cat_sentence = \"I dislike cats!\"\n",
        "\n",
        "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
        "cat_vector = embedding_model.get_embedding(cat_sentence)\n",
        "\n",
        "cosine_similarity(puppy_vector, cat_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see - these vectors are further apart - as expected!\n",
        "\n",
        "Now that we've gotten some background - lets see this put together with a few extra layers on top!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "        -> Dimensionality reduction with techniques such as Principal Component Analysis (PCA)\n",
        "        -> Fine-tuning-> fine-tune model to a different embedding size\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "    -> OpenAI's text embedding models typically use transformer architectures, which inherently define the embedding dimension as part of their configuration.\n",
        "    -> The specific embedding dimension is chosen based on a balance of computational efficiency and the ability to capture semantic information effectively.\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\ufeff\\nThe Pmarca Blog Archives\\n(select posts from 2007-2009)\\nMarc Andreessen\\ncopyright: Andreessen Horowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: Why not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The only thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too little? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA GUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional CEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: Turnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why 120\\nThe Pmarca Guide to Personal Productivi',\n",
              " \"RCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why 120\\nThe Pmarca Guide to Personal Productivity 127\\nPSYCHOLOGY AND ENTREPRENEURSHIP\\nThe Psychology of Entrepreneurial Misjudgment:\\nBiases 1-6\\n142\\nAge and the Entrepreneur: Some data 154\\nLuck and the entrepreneur: The four kinds of luck 162\\nSerial Entrepreneurs 168\\nTHE BACK PAGES\\nTop 10 science Dction novelists of the '00s ... so far\\n(June 2007)\\n173\\nBubbles on the brain (October 2009) 180\\nOK, you're right, it IS a bubble (October 2009) 186\\nThe Pmarca Guide to\\nStartups\\nPart 1: Why not to do a startup\\nIn this series of posts I will walk through some of my accumulated knowledge and experience in building high-tech startups.\\nMy speciXc experience is from three companies I have cofounded: Netscape, sold to America Online in 1998 for $4.2\\nbillion; Opsware (formerly Loudcloud), a public soaware company with an approximately $1 billion market\",\n",
              " 'ence is from three companies I have cofounded: Netscape, sold to America Online in 1998 for $4.2\\nbillion; Opsware (formerly Loudcloud), a public soaware company with an approximately $1 billion market cap; and now\\nNing, a new, private consumer Internet company.\\nBut more generally, I’ve been fortunate enough to be involved\\nin and exposed to a broad range of other startups — maybe 40\\nor 50 in enough detail to know what I’m talking about — since\\narriving in Silicon Valley in 1994: as a board member, as an angel\\ninvestor, as an advisor, as a friend of various founders, and as a\\nparticipant in various venture capital funds.\\nThis series will focus on lessons learned from this entire crosssection of Silicon Valley startups — so don’t think that anything\\nI am talking about is referring to one of my own companies:\\nmost likely when I talk about a scenario I have seen or something I have experienced, it is from some other startup that I\\nam not naming but was involved with some other way than as a',\n",
              " 'y own companies:\\nmost likely when I talk about a scenario I have seen or something I have experienced, it is from some other startup that I\\nam not naming but was involved with some other way than as a\\nfounder.\\nFinally, much of my perspective is based on Silicon Valley and\\nthe environment that we have here — the culture, the people,\\nthe venture capital base, and so on. Some of it will travel well\\nto other regions and countries, some probably will not. Caveat\\nemptor.\\nWith all that out of the way, let’s start at the beginning: why not\\nto do a startup.\\nStartups, even in the wake of the crash of 2000, have become\\nimbued with a real mystique — you read a lot about how great\\nit is to do a startup, how much fun it is, what with the getting to\\ninvent the future, all the free meals, foosball tables, and all the\\nrest.\\nNow, it is true that there are a lot of great things about doing a\\nstartup. They include, in my experience:\\nMost fundamentally, the opportunity to be in control of your own\\ndestiny ',\n",
              " ' and all the\\nrest.\\nNow, it is true that there are a lot of great things about doing a\\nstartup. They include, in my experience:\\nMost fundamentally, the opportunity to be in control of your own\\ndestiny — you get to succeed or fail on your own, and you don’t\\nhave some bozo telling you what to do. For a certain kind of personality, this alone is reason enough to do a startup.\\nThe opportunity to create something new — the proverbial blank\\nsheet of paper. You have the ability — actually, the obligation\\n— to imagine a product that does not yet exist and bring it\\ninto existence, without any of the constraints normally faced by\\nlarger companies.\\nThe opportunity to have an impact on the world — to give people\\na new way to communicate, a new way to share information, a\\nnew way to work together, or anything else you can think of that\\nwould make the world a better place. Think it should be easier\\nfor low-income people to borrow money? Start Prosper. Think\\ntelevision should be opened up to an inXnit',\n",
              " 'ything else you can think of that\\nwould make the world a better place. Think it should be easier\\nfor low-income people to borrow money? Start Prosper. Think\\ntelevision should be opened up to an inXnite number of channels? Start Joost. Think that computers should be based on Unix\\nand open standards and not proprietary technology? Start Sun.\\nThe ability to create your ideal culture and work with a dream team\\nof people you get to assemble yourself. Want your culture to be\\nbased on people who have fun every day and enjoy working\\ntogether? Or, are hyper-competitive both in work and play? Or,\\nare super-focused on creating innovative new rocket science\\nPart 1: Why not to do a startup 3\\ntechnologies? Or, are global in perspective from day one? You\\nget to choose, and to build your culture and team to suit.\\nAnd Xnally, money — startups done right can of course be highly\\nlucrative. This is not just an issue of personal greed — when\\nthings go right, your team and employees will themselves do\\nvery ',\n",
              " 'to suit.\\nAnd Xnally, money — startups done right can of course be highly\\nlucrative. This is not just an issue of personal greed — when\\nthings go right, your team and employees will themselves do\\nvery well and will be able to support their families, send their\\nkids to college, and realize their dreams, and that’s really cool.\\nAnd if you’re really lucky, you as the entrepreneur can ultimately make profound philanthropic gias that change society\\nfor the better.\\nHowever, there are many more reasons to not do a startup.\\nFirst, and most importantly, realize that a startup puts you on\\nan emotional rollercoaster unlike anything you have ever experienced.\\nYou will Yip rapidly from a day in which you are euphorically\\nconvinced you are going to own the world, to a day in which\\ndoom seems only weeks away and you feel completely ruined,\\nand back again.\\nOver and over and over.\\nAnd I’m talking about what happens to stable entrepreneurs.\\nThere is so much uncertainty and so much risk around practically',\n",
              " 'way and you feel completely ruined,\\nand back again.\\nOver and over and over.\\nAnd I’m talking about what happens to stable entrepreneurs.\\nThere is so much uncertainty and so much risk around practically everything you are doing. Will the product ship on time?\\nWill it be fast enough? Will it have too many bugs? Will it be\\neasy to use? Will anyone use it? Will your competitor beat you\\nto market? Will you get any press coverage? Will anyone invest\\nin the company? Will that key new engineer join? Will your key\\nuser interface designer quit and go to Google? And on and on\\nand on…\\nSome days things will go really well and some things will go\\nreally poorly. And the level of stress that you’re under generally\\nwill magnify those transient data points into incredible highs\\nand unbelievable lows at whiplash speed and huge magnitude.\\nSound like fun?\\n4 The Pmarca Blog Archives\\nSecond, in a startup, absolutely nothing happens unless you make it\\nhappen.\\nThis one throws both founders and employees new to ',\n",
              " 'ash speed and huge magnitude.\\nSound like fun?\\n4 The Pmarca Blog Archives\\nSecond, in a startup, absolutely nothing happens unless you make it\\nhappen.\\nThis one throws both founders and employees new to startups.\\nIn an established company — no matter how poorly run or\\ndemoralized — things happen. They just happen. People come\\nin to work. Code gets written. User interfaces get designed.\\nServers get provisioned. Markets get analyzed. Pricing gets studied and determined. Sales calls get made. The wastebaskets get\\nemptied. And so on.\\nA startup has none of the established systems, rhythms, infrastructure that any established company has.\\nIn a startup it is very easy for the code to not get written, for the\\nuser interfaces to not get designed… for people to not come into\\nwork… and for the wastebaskets to not get emptied.\\nYou as the founder have to put all of these systems and routines\\nand habits in place and get everyone actually rowing — forget\\neven about rowing in the right direction: just ro',\n",
              " 'ets to not get emptied.\\nYou as the founder have to put all of these systems and routines\\nand habits in place and get everyone actually rowing — forget\\neven about rowing in the right direction: just rowing at all is\\nhard enough at the start.\\nAnd until you do, absolutely nothing happens.\\nUnless, of course, you do it yourself.\\nHave fun emptying those wastebaskets.\\nThird, you get told no — a lot.\\nUnless you’ve spent time in sales, you are probably not familiar\\nwith being told no a lot.\\nIt’s not so much fun.\\nGo watch Death of a Salesman and then Glengarry Glen Ross.\\nThat’s roughly what it’s like.\\nYou’re going to get told no by potential employees, potential\\ninvestors, potential customers, potential partners, reporters,\\nanalysts…\\nPart 1: Why not to do a startup 5\\nOver and over and over.\\nAnd when you do get a “yes”, half the time you’ll get a call two\\ndays later and it’ll turn out the answer has morphed into “no”.\\nBetter start working on your fake smile.\\nFourth, hiring is a huge pain in the a',\n",
              " ' you do get a “yes”, half the time you’ll get a call two\\ndays later and it’ll turn out the answer has morphed into “no”.\\nBetter start working on your fake smile.\\nFourth, hiring is a huge pain in the ass.\\nYou will be amazed how many windowshoppers you’ll deal with.\\nA lot of people think they want to be part of a startup, but when\\nthe time comes to leave their cushy job at HP or Apple, they\\nYinch — and stay.\\nGoing through the recruiting process and being seduced by a\\nstartup is heady stuW for your typical engineer or midlevel manager at a big company — you get to participate vicariously in the\\nthrill of a startup without actually having to join or do any of the\\nhard work.\\nAs a founder of a startup trying to hire your team, you’ll run into\\nthis again and again.\\nWhen Jim Clark decided to start a new company in 1994, I was\\none of about a dozen people at various Silicon Valley companies\\nhe was talking to about joining him in what became Netscape.\\nI was the only one who went all the way to sa',\n",
              " ' a new company in 1994, I was\\none of about a dozen people at various Silicon Valley companies\\nhe was talking to about joining him in what became Netscape.\\nI was the only one who went all the way to saying “yes” (largely\\nbecause I was 22 and had no reason not to do it).\\nThe rest Yinched and didn’t do it.\\nAnd this was Jim Clark, a legend in the industry who was coming\\noW of the most successful company in Silicon Valley in 1994 —\\nSilicon Graphics Inc.\\nHow easy do you think it’s going to be for you?\\nThen, once you do get through the windowshoppers and actually hire some people, your success rate on hiring is probably\\nnot going to be higher than 50%, and that’s if you’re good at it.\\nBy that I mean that half or more of the people you hire aren’t\\n6 The Pmarca Blog Archives\\ngoing to work out. They’re going to be too lazy, too slow, easily\\nrattled, political, bipolar, or psychotic.\\nAnd then you have to either live with them, or Xre them.\\nWhich ones of those sounds like fun?\\nFiah, God help you, ',\n",
              " '’re going to be too lazy, too slow, easily\\nrattled, political, bipolar, or psychotic.\\nAnd then you have to either live with them, or Xre them.\\nWhich ones of those sounds like fun?\\nFiah, God help you, at some point you’re going to have to hire executives.\\nYou think hiring employees is hard and risky — wait until you\\nstart hiring for VP Engineering, VP Marketing, VP Sales, VP HR,\\nGeneral Counsel, and CFO.\\nSixth, the hours.\\nThere’s been a lot of talk in Silicon Valley lately about work/life\\nbalance — about how you should be able to do a startup and\\nsimultaneously live a full and fulXlling outside life.\\nNow, personally, I have a lot of sympathy for that point of view.\\nAnd I try hard in my companies (well, at least my last two companies) to do whatever I can to help make sure that people aren’t\\nground down to little tiny spots on the Yoor by the workload\\nand the hours.\\nBut, it’s really diZcult.\\nThe fact is that startups are incredibly intense experiences and\\ntake a lot out of people in the ',\n",
              " '\\nground down to little tiny spots on the Yoor by the workload\\nand the hours.\\nBut, it’s really diZcult.\\nThe fact is that startups are incredibly intense experiences and\\ntake a lot out of people in the best of circumstances.\\nAnd just because you want people to have work/life balance, it’s\\nnot so easy when you’re close to running out of cash, your product hasn’t shipped yet, your VC is mad at you, and your Kleiner\\nPerkins-backed competitor in Menlo Park — you know, the one\\nwhose employees’ average age seems to be about 19 — is kicking\\nyour butt.\\nWhich is what it’s going to be like most of the time.\\nAnd even if you can help your employees have proper work/life\\nbalance, as a founder you certainly won’t.\\nPart 1: Why not to do a startup 7\\n(In case you were wondering, by the way, the hours do compound the stress.)\\nSeventh, it’s really easy for the culture of a startup to go sideways.\\nThis combines the Xrst and second items above.\\nThis is the emotional rollercoaster wreaking havoc on not just\\ny',\n",
              " 'ound the stress.)\\nSeventh, it’s really easy for the culture of a startup to go sideways.\\nThis combines the Xrst and second items above.\\nThis is the emotional rollercoaster wreaking havoc on not just\\nyou but your whole company.\\nIt takes time for the culture of any company to become “set” —\\nfor the team of people who have come together for the Xrst time\\nto decide collectively what they’re all about, what they value —\\nand how they look at challenge and adversity.\\nIn the best case, you get an amazing dynamic of people really\\npulling together, supporting one another, and working their collective tails oW in pursuit of a dream.\\nIn the worst case, you end up with widespread, self-reinforcing\\nbitterness, disillusionment, cynicism, bad morale, contempt for\\nmanagement, and depression.\\nAnd you as the founder have much less inYuence over this than\\nyou’ll think you do.\\nGuess which way it usually goes.\\nEighth, there are lots of X factors that can come along and whup\\nyou right upside the head, and th',\n",
              " ' founder have much less inYuence over this than\\nyou’ll think you do.\\nGuess which way it usually goes.\\nEighth, there are lots of X factors that can come along and whup\\nyou right upside the head, and there’s absolutely nothing you\\ncan do about them.\\nStock market crashes.\\nTerrorist attacks.\\nNatural disasters.\\nA better funded startup with a more experienced team that’s\\nbeen hard at work longer than you have, in stealth mode, that\\nunexpectedly releases a product that swialy comes to dominate\\nyour market, completely closing oW your opportunity, and you\\nhad no idea they were even working on it.\\n8 The Pmarca Blog Archives\\nAt best, any given X factor might slam shut the fundraising\\nwindow, cause customers to delay or cancel purchases — or, at\\nworst, shut down your whole company.\\nRussian mobsters laundering millions of dollars of dirty money\\nthrough your service, resulting in the credit card companies\\nclosing you down.\\nYou think I’m joking about that one?\\nOK, now here’s the best part:\\nI haven’t ',\n",
              " 'undering millions of dollars of dirty money\\nthrough your service, resulting in the credit card companies\\nclosing you down.\\nYou think I’m joking about that one?\\nOK, now here’s the best part:\\nI haven’t even talked about Xguring out what product to build,\\nbuilding it, taking it to market, and standing out from the crowd.\\nAll the risks in the core activities of what your company actually\\ndoes are yet to come, and to be discussed in future posts in this\\nseries.\\nPart 1: Why not to do a startup 9\\nPart 2: When the VCs say \"no\"\\nThis post is about what to do between when the VCs say “no” to\\nfunding your startup, and when you either change their minds\\nor Xnd some other path.\\nI’m going to assume that you’ve done all the basics: developed a\\nplan and a pitch, decided that venture Xnancing is right for you\\nand you are right for venture Xnancing, lined up meetings with\\nproperly qualiXed VCs, and made your pitch.\\nAnd the answer has come back and it’s “no”.\\nOne “no” doesn’t mean anything — the VC could ',\n",
              " 'ou\\nand you are right for venture Xnancing, lined up meetings with\\nproperly qualiXed VCs, and made your pitch.\\nAnd the answer has come back and it’s “no”.\\nOne “no” doesn’t mean anything — the VC could just be having\\na bad day, or she had a bad experience with another company in\\nyour category, or she had a bad experience with another company with a similar name, or she had a bad experience with\\nanother founder who kind of looks like you, or her Mercedes\\nSLR McLaren’s engine could have blown up on the freeway that\\nmorning — it could be anything. Go meet with more VCs.\\nIf you meet with three VCs and they all say “no”, it could just be\\na big coincidence. Go meet with more VCs.\\nIf you meet with Xve, or six, or eight VCs and they all say no, it’s\\nnot a coincidence.\\nThere is something wrong with your plan.\\nOr, even if there isn’t, there might as well be, because you’re still\\nnot getting funded.\\nMeeting with more VCs aaer a bunch have said no is probably\\na waste of time. Instead, retool your pl',\n",
              " 'our plan.\\nOr, even if there isn’t, there might as well be, because you’re still\\nnot getting funded.\\nMeeting with more VCs aaer a bunch have said no is probably\\na waste of time. Instead, retool your plan — which is what this\\npost is about.\\nBut Hrst, lay the groundwork to go back in later.\\nIt’s an old — and true — cliche that VCs rarely actually say “no”\\n— more oaen they say “maybe”, or “not right now”, or “my partners aren’t sure”, or “that’s interesting, let me think about it”.\\nThey do that because they don’t want to invest in your company\\ngiven the current facts, but they want to keep the door open in\\ncase the facts change.\\nAnd that’s exactly what you want — you want to be able to go\\nback to them with a new set of facts, and change their minds,\\nand get to “yes”.\\nSo be sure to take “no” gracefully — politely ask them for feedback (which they probably won’t give you, at least not completely honestly — nobody likes calling someone else’s baby\\nugly — believe me, I’ve done it), thank them ',\n",
              " 'acefully — politely ask them for feedback (which they probably won’t give you, at least not completely honestly — nobody likes calling someone else’s baby\\nugly — believe me, I’ve done it), thank them for their time, and\\nask if you can call them again if things change.\\nTrust me — they’d much rather be saying “yes” than “no” —\\nthey need all the good investments they can get.\\nSecond, consider the environment.\\nBeing told “no” by VCs in 1999 is a lot diWerent than being told\\n“no” in 2002.\\nIf you were told “no” in 1999, I’m sure you’re a wonderful person and you have huge potential and your mother loves you\\nvery much, but your plan really was seriously Yawed.\\nIf you were told “no” in 2002, you probably actually were the\\nnext Google, but most of the VCs were hiding under their desks\\nand they just missed it.\\nIn my opinion, we’re now in a much more rational environment\\nthan either of those extremes — a lot of good plans are being\\nfunded, along with some bad ones, but not all the bad ones.\\nPart ',\n",
              " ' missed it.\\nIn my opinion, we’re now in a much more rational environment\\nthan either of those extremes — a lot of good plans are being\\nfunded, along with some bad ones, but not all the bad ones.\\nPart 2: When the VCs say \"no\" 11\\nI’ll proceed under the assumption that we’re in normal times.\\nBut if things get truly euphoric or truly funereal again, the rest\\nof this post will probably not be very helpful — in either case.\\nThird, retool your plan.\\nThis is the hard part — changing the facts of your plan and what\\nyou are trying to do, to make your company more fundable.\\nTo describe the dimensions that you should consider as you\\ncontemplate retooling your plan, let me introduce the onion\\ntheory of risk.\\nIf you’re an investor, you look at the risk around an investment\\nas if it’s an onion. Just like you peel an onion and remove each\\nlayer in turn, risk in a startup investment comes in layers that\\nget peeled away — reduced — one by one.\\nYour challenge as an entrepreneur trying to raise venture ca',\n",
              " ' you peel an onion and remove each\\nlayer in turn, risk in a startup investment comes in layers that\\nget peeled away — reduced — one by one.\\nYour challenge as an entrepreneur trying to raise venture capital\\nis to keep peeling layers of risk oW of your particular onion until\\nthe VCs say “yes” — until the risk in your startup is reduced to\\nthe point where investing in your startup doesn’t look terrifying\\nand merely looks risky.\\nWhat are the layers of risk for a high-tech\\nstartup?\\nIt depends on the startup, but here are some of the common\\nones:\\nFounder risk — does the startup have the right founding team?\\nA common founding team might include a great technologist,\\nplus someone who can run the company, at least to start. Is the\\ntechnologist really all that? Is the business person capable of\\nrunning the company? Is the business person missing from the\\nteam altogether? Is it a business person or business people with\\nno technologist, and therefore virtually unfundable?\\nMarket risk — is there a ',\n",
              " 'ing the company? Is the business person missing from the\\nteam altogether? Is it a business person or business people with\\nno technologist, and therefore virtually unfundable?\\nMarket risk — is there a market for the product (using the term\\nproduct and service interchangeably)? Will anyone want it? Will\\nthey pay for it? How much will they pay? How do we know?\\n12 The Pmarca Blog Archives\\nCompetition risk — are there too many other startups already\\ndoing this? Is this startup suZciently diWerentiated from the\\nother startups, and also diWerentiated from any large incumbents?\\nTiming risk — is it too early? Is it too late?\\nFinancing risk — aaer we invest in this round, how many additional rounds of Xnancing will be required for the company to\\nbecome proXtable, and what will the dollar total be? How certain\\nare we about these estimates? How do we know?\\nMarketing risk — will this startup be able to cut through the\\nnoise? How much will marketing cost? Do the economics of customer acquisition — t',\n",
              " 'ow certain\\nare we about these estimates? How do we know?\\nMarketing risk — will this startup be able to cut through the\\nnoise? How much will marketing cost? Do the economics of customer acquisition — the cost to acquire a customer, and the revenue that customer will generate — work?\\nDistribution risk — does this startup need certain distribution\\npartners to succeed? Will it be able to get them? How? (For\\nexample, this is a common problem with mobile startups that\\nneed deals with major mobile carriers to succeed.)\\nTechnology risk — can the product be built? Does it involve rocket\\nscience — or an equivalent, like artiXcial intelligence or natural\\nlanguage processing? Are there fundamental breakthroughs that\\nneed to happen? If so, how certain are we that they will happen,\\nor that this team will be able to make them?\\nProduct risk — even assuming the product can in theory be built,\\ncan this team build it?\\nHiring risk — what positions does the startup need to hire for in\\norder to execute its ',\n",
              " 'l be able to make them?\\nProduct risk — even assuming the product can in theory be built,\\ncan this team build it?\\nHiring risk — what positions does the startup need to hire for in\\norder to execute its plan? E.g. a startup planning to build a highscale web service will need a VP of Operations — will the founding team be able to hire a good one?\\nLocation risk — where is the startup located? Can it hire the right\\ntalent in that location? And will I as the VC need to drive more\\nthan 20 minutes in my Mercedes SLR McLaren to get there?\\nYou know, when you stack up all these layers and look at the\\nPart 2: When the VCs say \"no\" 13\\nfull onion, you realize it’s amazing that any venture investments\\never get made.\\nWhat you need to do is take a hard-headed look at each of these\\nrisks — and any others that are speciXc to your startup and its\\ncategory — and put yourself in the VC’s shoes: what could this\\nstartup do to minimize or eliminate enough of these risks to\\nmake the company fundable?\\nThen do tho',\n",
              " 't are speciXc to your startup and its\\ncategory — and put yourself in the VC’s shoes: what could this\\nstartup do to minimize or eliminate enough of these risks to\\nmake the company fundable?\\nThen do those things.\\nThis isn’t very much fun, since it will probably involve making\\nsigniXcant changes to your plan, but look on the bright side: it’s\\nexcellent practice for when your company ultimately goes public and has to Xle an S1 registration statement with the SEC, in\\nwhich you have to itemize in huge detail every conceivable risk\\nand bad thing that could ever possibly happen to you, up to and\\nincluding global warming.\\nSome ideas on reducing risk\\nFounder risk — the tough one. If you’re the technologist on a\\nfounding team with a business person, you have to consider the\\npossibility that the VCs don’t think the business person is strong\\nenough to be the founding CEO. Or vice versa, maybe they\\nthink the technologist isn’t strong enough to build the product.\\nYou may have to swap out one or more ',\n",
              " 'on’t think the business person is strong\\nenough to be the founding CEO. Or vice versa, maybe they\\nthink the technologist isn’t strong enough to build the product.\\nYou may have to swap out one or more founders, and/or add\\none or more founders.\\nI put this one right up front because it can be a huge issue and\\nthe odds of someone being honest with you about it in the speciXc are not that high.\\nMarket risk — you probably need to validate the market, at a\\npractical level. Sometimes more detailed and analytical market\\nresearch will solve the problem, but more oaen you actually\\nneed to go get some customers to demonstrate that the market\\nexists. Preferably, paying customers. Or at least credible\\nprospects who will talk to VCs to validate the market hypothesis.\\nCompetition risk — is your diWerentiation really sharp enough?\\n14 The Pmarca Blog Archives\\nRethink this one from the ground up. Lots of startups do not\\nhave strong enough diWerentiation out of the gate, even aaer\\nthey get funded. If you ',\n",
              " 'tion really sharp enough?\\n14 The Pmarca Blog Archives\\nRethink this one from the ground up. Lots of startups do not\\nhave strong enough diWerentiation out of the gate, even aaer\\nthey get funded. If you don’t have a really solid idea as to how\\nyou’re dramatically diWerent from or advantaged over known\\nand unknown competitors, you might not want to start a company in the Xrst place.\\nTwo additional points on competition risk that founders routinely screw up in VC pitches:\\nNever, ever say that you have no competitors. That signals\\nnaivete. Great markets draw competitors, and so if you really\\nhave no competition, you must not be in a great market. Even\\nif you really believe you have no competitors, create a competitive landscape slide with adjacent companies in related market\\nsegments and be ready to talk crisply about how you are like and\\nunlike those adjacent companies.\\nAnd never, ever say your market projections indicate you’re\\ngoing to be hugely successful if you get only 2% of your\\n(extr',\n",
              " 'y to talk crisply about how you are like and\\nunlike those adjacent companies.\\nAnd never, ever say your market projections indicate you’re\\ngoing to be hugely successful if you get only 2% of your\\n(extremely large) market. That also signals naivete. If you’re\\ngoing aaer 2% of a large market, that means the presumably\\nlarger companies that are going to take the other 98% are going\\nto kill you. You have to have a theory for how you’re going to get\\na signiXcantly higher market share than 2%. (I pick 2% because\\nthat’s the cliche, but if you’re a VC, you’ve probably heard someone use it.)\\nTiming risk — the only thing to do here is to make more\\nprogress, and demonstrate that you’re not too early or too late.\\nGetting customers in the bag is the most valuable thing you can\\ndo on this one.\\nFinancing risk — rethink very carefully how much money you\\nwill need to raise aaer this round of Xnancing, and try to change\\nthe plan in plausible ways to require less money. For example,\\nonly serve Cristal at ',\n",
              " 'risk — rethink very carefully how much money you\\nwill need to raise aaer this round of Xnancing, and try to change\\nthe plan in plausible ways to require less money. For example,\\nonly serve Cristal at your launch party, and not Remy Martin\\n“Black Pearl” Louis XIII cognac.\\nMarketing risk — Xrst, make sure your diWerentiation is superPart 2: When the VCs say \"no\" 15\\nsharp, because without that, you probably won’t be able to stand\\nout from the noise.\\nThen, model out your customer acquisition economics in detail\\nand make sure that you can show how you’ll get more revenue\\nfrom a customer than it will cost in sales and marketing expense\\nto acquire that customer. This is a common problem for startups pursuing the small business market, for example.\\nIf it turns out you need a lot of money in absolute terms for\\nmarketing, look for alternate approaches — perhaps guerilla\\nmarketing, or some form of virality.\\nDistribution risk — this is a very tough one — if your plan has\\ndistribution risk, which i',\n",
              " 'te terms for\\nmarketing, look for alternate approaches — perhaps guerilla\\nmarketing, or some form of virality.\\nDistribution risk — this is a very tough one — if your plan has\\ndistribution risk, which is to say you need a key distribution\\npartner to make it work, personally I’d recommend shelving the\\nplan and doing something else. Otherwise, you may need to go\\nget the distribution deal before you can raise money, which is\\nalmost impossible.\\nTechnology risk — there’s only one way around this, which is to\\nbuild the product, or at least get it to beta, and then raise money.\\nProduct risk — same answer — build it.\\nHiring risk — the best way to address this is to Xgure out which\\nposition/positions the VCs are worried about, and add it/them\\nto the founding team. This will mean additional dilution for\\nyou, but it’s probably the only way to solve the problem.\\nLocation risk — this is the one you’re really not going to like. If\\nyou’re not in a major center of entrepreneurialism and you’re\\nhaving tr',\n",
              " 'or\\nyou, but it’s probably the only way to solve the problem.\\nLocation risk — this is the one you’re really not going to like. If\\nyou’re not in a major center of entrepreneurialism and you’re\\nhaving trouble raising money, you probably need to move.\\nThere’s a reason why most Xlms get made in Los Angeles, and\\nthere’s a reason most venture-backed US tech startups happen\\nin Silicon Valley and handful of other places — that’s where the\\nmoney is. You can start a company wherever you want, but you\\nmay not be able to get it funded there.\\nYou’ll notice that a lot of what you may need to do is kick the ball\\nfurther down the road — make more progress against your plan\\nbefore you raise venture capital.\\n16 The Pmarca Blog Archives\\nThis obviously raises the issue of how you’re supposed to do\\nthat before you’ve raised money.\\nTry to raise angel money, or bootstrap oW of initial customers\\nor consulting contracts, or work on it aaer hours while keeping\\nyour current job, or quit your job and live oW of cr',\n",
              " 'you’ve raised money.\\nTry to raise angel money, or bootstrap oW of initial customers\\nor consulting contracts, or work on it aaer hours while keeping\\nyour current job, or quit your job and live oW of credit cards for\\na while.\\nLots of entrepreneurs have done these things and succeeded —\\nand of course, many have failed.\\nNobody said this would be easy.\\nThe most valuable thing you can do is actually build your product. When in doubt, focus on that.\\nThe next most valuable thing you can do is get customers — or,\\nfor a consumer Internet service, establish a pattern of page view\\ngrowth.\\nThe whole theory of venture capital is that VCs are investing\\nin risk — another term for venture capital is “risk capital” —\\nbut the reality is that VCs will only take on so much risk, and\\nthe best thing you can do to optimize your chances of raising\\nmoney is to take out risk.\\nPeel away at the onion.\\nThen, once you’ve done that, recraa the pitch around the new\\nfacts. Go do the pitches again. And repeat as necessa',\n",
              " 'o optimize your chances of raising\\nmoney is to take out risk.\\nPeel away at the onion.\\nThen, once you’ve done that, recraa the pitch around the new\\nfacts. Go do the pitches again. And repeat as necessary.\\nAnd to end on a happy note, remember that “yes” can turn into\\n“no” at any point up until the cash hits your company’s bank\\naccount.\\nSo keep your options open all the way to the end.\\nPart 2: When the VCs say \"no\" 17\\nPart 3: \"But I don\\'t know any VCs!\"\\nIn my last post in this series, When the VCs say “no”, I discussed\\nwhat to do once you have been turned down for venture funding for the Xrst time.\\nHowever, this presupposes you’ve been able to pitch VCs in the\\nXrst place. What if you have a startup for which you’d like to\\nraise venture funding, but you don’t know any VCs?\\nI can certainly sympathize with this problem — when I was in\\ncollege working on Mosaic at the University of Illinois, the term\\n“venture capital” might as well have been “klaatu barada nikto”\\nfor all I knew. I had never m',\n",
              " 'athize with this problem — when I was in\\ncollege working on Mosaic at the University of Illinois, the term\\n“venture capital” might as well have been “klaatu barada nikto”\\nfor all I knew. I had never met a venture capitalist, no venture\\ncapitalist had ever talked to me, and I wouldn’t have recognized\\none if I’d stumbled over his checkbook on the sidewalk. Without\\nJim Clark, I’m not at all certain I would have been able to raise\\nmoney to start a company like Netscape, had it even occured to\\nme to start a company in the Xrst place.\\nThe starting point for raising money from VCs when you don’t\\nknow any VCs is to realize that VCs work mostly through referrals — they hear about a promising startup or entrepreneur\\nfrom someone they have worked with before, like another\\nentrepreneur, an executive or engineer at one of the startups\\nthey have funded, or an angel investor with whom they have\\npreviously co-invested.\\nThe reason for this is simply the math: any individual VC can\\nonly fund a few compa',\n",
              " 'r engineer at one of the startups\\nthey have funded, or an angel investor with whom they have\\npreviously co-invested.\\nThe reason for this is simply the math: any individual VC can\\nonly fund a few companies per year, and for every one she\\nfunds, she probably meets with 15 or 20, and there are hundreds\\nmore that would like to meet with her that she doesn’t possibly\\nhave time to meet with. She has to rely on her network to help\\nher screen the hundreds down to 15 or 20, so she can spend her\\ntime Xnding the right one out of the 15 or 20.\\nTherefore, submitting a business plan “over the transom”, or\\nunsolicited, to a venture Xrm is likely to amount to just as much\\nas submitting a screenplay “over the transom” to a Hollywood\\ntalent agency — that is, precisely nothing.\\nSo the primary trick becomes getting yourself into a position\\nwhere you’re one of the 15 or 20 a particular venture capitalist\\nis meeting with based on referrals from her network, not one of\\nthe hundreds of people who don’t come r',\n",
              " 'etting yourself into a position\\nwhere you’re one of the 15 or 20 a particular venture capitalist\\nis meeting with based on referrals from her network, not one of\\nthe hundreds of people who don’t come recommended by anyone and whom she has no intention of meeting.\\nBut before you think about doing that, the Xrst order of business\\nis to (paraphrasing for a family audience) “have your stuG\\ntogether” — create and develop your plan, your presentation,\\nand your supporting materials so that when you do meet with a\\nVC, you impress her right out of the gate as bringing her a fundable startup founded by someone who knows what he — that’s\\nyou — is doing.\\nMy recommendation is to read up on all the things you should\\ndo to put together a really eWective business plan and presentation, and then pretend you have already been turned down once\\n— then go back to my last post and go through all the diWerent\\nthings you should anticipate and Xx before you actually do walk\\nthrough the door.\\nOne of the reason V',\n",
              " ' have already been turned down once\\n— then go back to my last post and go through all the diWerent\\nthings you should anticipate and Xx before you actually do walk\\nthrough the door.\\nOne of the reason VCs only meet with startups through their\\nnetworks is because too many of the hundreds of other startups\\nthat they could meet with come across as amateurish and uninformed, and therefore not fundable, when they do take meetings with them. So you have a big opportunity to cut through\\nthe noise by making a great Hrst impression — which requires\\nreally thinking things through ahead of time and doing all the\\nhard work up front to really make your pitch and plan a masterpiece.\\nPart 3: \"But I don\\'t know any VCs!\" 19\\nWorking backwards from that, the best thing you can walk in\\nwith is a working product. Or, if you can’t get to a working\\nproduct without raising venture funding, then at least a beta\\nor prototype of some form — a web site that works but hasn’t\\nlaunched, or a soaware mockup with partia',\n",
              " '. Or, if you can’t get to a working\\nproduct without raising venture funding, then at least a beta\\nor prototype of some form — a web site that works but hasn’t\\nlaunched, or a soaware mockup with partial functionality,\\nor something. And of course it’s even better if you walk in with\\nexisting “traction” of some form — customers, beta customers,\\nsome evidence of adoption by Internet users, whatever is appropriate for your particular startup.\\nWith a working product that could be the foundation of a fundable startup, you have a much better chance of getting funded\\nonce you do get in the door. Back to my rule of thumb from\\nthe last post: when in doubt, work on the product.\\nFailing a working product and ideally customers or users, be\\nsure to have as Ieshed out a presentation as you possibly can\\n— including mockups, screenshots, market analyses, customer\\nresearch such as interviews with real prospects, and the like.\\nDon’t bother with a long detailed written business plan. Most\\nVCs will either f',\n",
              " 'including mockups, screenshots, market analyses, customer\\nresearch such as interviews with real prospects, and the like.\\nDon’t bother with a long detailed written business plan. Most\\nVCs will either fund a startup based on a Yeshed out Powerpoint\\npresentation of about 20 slides, or they won’t fund it at all.\\nCorollary: any VC who requires a long detailed written business\\nplan is probably not the right VC to be working with.\\nNext: qualify, qualify, qualify. Do extensive research on venture capitalists and Xnd the ones who focus on the sector relevant to your startup. It is completely counterproductive to\\neveryone involved for you to pitch a health care VC on a consumer Internet startup, or vice versa. Individual VCs are usually\\nquite focused in the kinds of companies they are looking for,\\nand identifying those VCs and screening out all the others is\\nabsolutely key.\\nNow, on to developing contacts\\nThe best way to develop contacts with VCs, in my opinion, is to\\nwork at a venture-backed sta',\n",
              " 'nd identifying those VCs and screening out all the others is\\nabsolutely key.\\nNow, on to developing contacts\\nThe best way to develop contacts with VCs, in my opinion, is to\\nwork at a venture-backed startup, kick butt, get promoted, and\\nnetwork the whole way.\\n20 The Pmarca Blog Archives\\nIf you can’t get hired by a venture-backed startup right now,\\nwork at a well-regarded large tech company that employs a lot\\nof people like Google or Apple, gain experience, and then go to\\nwork at a venture-backed startup, kick butt, get promoted, and\\nnetwork the whole way.\\nAnd if you can’t get hired by a well-regarded large tech company, go get a bachelor’s or master’s degree at a major research\\nuniversity from which well-regarded large tech companies regularly recruit, then work at a well-regarded large tech company\\nthat employs a lot of people like Google or Apple, gain experience, and then go to work at a venture-backed startup, kick butt,\\nget promoted, and network the whole way.\\nI sound like I’m jokin',\n",
              " ' company\\nthat employs a lot of people like Google or Apple, gain experience, and then go to work at a venture-backed startup, kick butt,\\nget promoted, and network the whole way.\\nI sound like I’m joking, but I’m completely serious — this is the\\npath taken by many venture-backed entrepreneurs I know.\\nSome alternate techniques that don’t take\\nquite as long\\nIf you’re still in school, immediately transfer to, or plan on\\ngoing to graduate school at, a large research university with\\nwell-known connections to the venture capital community, like\\nStanford or MIT.\\nGraduate students at Stanford are directly responsible for such\\ncompanies as Sun, Cisco, Yahoo, and Google, so needless to say,\\nSilicon Valley VCs are continually on the prowl on the Stanford\\nengineering campus for the next Jerry Yang or Larry Page.\\n(In contrast, the University of Illinois, where I went to school, is\\nmostly prowled by mutant cold-weather cows.)\\nAlternately, jump all over Y Combinator. This program, created by entreprene',\n",
              " 'rry Page.\\n(In contrast, the University of Illinois, where I went to school, is\\nmostly prowled by mutant cold-weather cows.)\\nAlternately, jump all over Y Combinator. This program, created by entrepreneur Paul Graham and his partners, funds\\nearly-stage startups in an organized program in Silicon Valley\\nand Boston and then makes sure the good ones get in front of\\nventure capitalists for follow-on funding. It’s a great idea and a\\nhuge opportunity for the people who participate in it.\\nPart 3: \"But I don\\'t know any VCs!\" 21\\nRead VC blogs — read them all, and read them very very carefully. VCs who blog are doing entrepreneurs a huge service both\\nin conveying highly useful information as well as frequently\\nputting themselves out there to be contacted by entrepreneurs\\nin various ways including email, comments, and even uploaded\\npodcasts. Each VC is diWerent in terms of how she wants to\\nengage with people online, but by all means read as many VC\\nblogs as you can and interact with as many of them',\n",
              " 'l, comments, and even uploaded\\npodcasts. Each VC is diWerent in terms of how she wants to\\nengage with people online, but by all means read as many VC\\nblogs as you can and interact with as many of them as you can\\nin appropriate ways.\\nAt the very least you will start to get a really good sense of which\\nVCs who blog are interested in which kinds of companies.\\nAt best, a VC blogger may encourage her readers to communicate with her in various ways, including soliciting email pitches\\nin certain startup categories of interest to her.\\nFred Wilson of Union Square Ventures has even gone so far as\\nto encourage entrepreneurs to record and upload audio pitches\\nfor new ventures so he can listen to them on his IPod. I don’t\\nknow if he’s still doing that, but it’s worth reading his blog and\\nXnding out.\\nAlong those lines, some VCs are aggressive early adopters of\\nnew forms of communication and interaction — current examples being Facebook and Twitter. Observationally, when a VC is\\nexploring a new commu',\n",
              " 'ong those lines, some VCs are aggressive early adopters of\\nnew forms of communication and interaction — current examples being Facebook and Twitter. Observationally, when a VC is\\nexploring a new communiation medium like Facebook or Twitter, she can be more interested in interacting with various people over that new medium than she might otherwise be. So,\\nwhen such a new thing comes out — like, hint hint, Facebook or\\nTwitter — jump all over it, see which VCs are using it, and interact with them that way — sensibly, of course.\\nMore generally, it’s a good idea for entrepreneurs who are\\nlooking for funding to blog — about their startup, about interesting things going on, about their point of view. This puts an\\nentrepreneur in the Yow of conversation, which can lead to\\ninteraction with VCs through the normal medium of blogging.\\nAnd, when a VC does decide to take a look at you and your company, she can read your blog to get a sense of who you are and\\n22 The Pmarca Blog Archives\\nhow you think',\n",
              " 'ugh the normal medium of blogging.\\nAnd, when a VC does decide to take a look at you and your company, she can read your blog to get a sense of who you are and\\n22 The Pmarca Blog Archives\\nhow you think. It’s another great opportunity to put forward a\\nfantastic Xrst impression.\\nFinally, if you are a programmer, I highly encourage you, if you\\nhave time, to create or contribute to a meaningful open source\\nproject. The open source movement is an amazing opportunity\\nfor programmers all over the world to not only build useful\\nsoaware that lots of people can use, but also build their own reputations completely apart from whatever day jobs they happen\\nto have. Being able to email a VC and say, “I’m the creator of\\nopen source program X which has 50,000 users worldwide, and\\nI want to tell you about my new startup” is a lot more eWective\\nthan your normal pitch.\\nIf you engage in a set of these techniques over time,\\nyou should be able to interact with at least a few VCs in ways that\\nthey Xnd useful ',\n",
              " 'y new startup” is a lot more eWective\\nthan your normal pitch.\\nIf you engage in a set of these techniques over time,\\nyou should be able to interact with at least a few VCs in ways that\\nthey Xnd useful and that might lead to further conversations\\nabout funding, or even introductions to other VCs.\\nI’m personally hoping that the next Google comes out of a VC\\nbeing sent an email pitch aaer the entrepreneur read that VC’s\\nblog. Then every VC on the planet will suddenly start blogging,\\novernight.\\nIf none of those ideas work for you\\nYour alternatives in reverse (declining) order of preference for\\nfunding are, in my view: angel funding, bootstrapping via consulting contracts or early customers, keeping your day job and\\nworking on your startup in your spare time, and credit card\\ndebt.\\nAngel funding — funding from individuals who like to invest\\nsmall amounts of money in early-stage startups, oaen before\\nVCs come in — can be a great way to go since good angels know\\ngood VCs and will be eager to in',\n",
              " '— funding from individuals who like to invest\\nsmall amounts of money in early-stage startups, oaen before\\nVCs come in — can be a great way to go since good angels know\\ngood VCs and will be eager to introduce you to them so that\\nyour company goes on to be successful for the angel as well as\\nfor you.\\nPart 3: \"But I don\\'t know any VCs!\" 23\\nThis of course begs the question of how to raise angel money,\\nwhich is another topic altogether!\\nI am not encouraging the other three alternatives — bootstrapping, working on it part time, or credit card debt. Each has\\nserious problems. But, it is easy to name highly successful entrepreneurs who have followed each of those paths, so they are\\nworth noting.\\n24 The Pmarca Blog Archives\\nPart 4: The only thing that matters\\nThis post is all about the only thing that matters for a new\\nstartup.\\nBut Xrst, some theory:\\nIf you look at a broad cross-section of startups — say, 30 or 40\\nor more; enough to screen out the pure Yukes and look for patterns — two obvious ',\n",
              " 'hat matters for a new\\nstartup.\\nBut Xrst, some theory:\\nIf you look at a broad cross-section of startups — say, 30 or 40\\nor more; enough to screen out the pure Yukes and look for patterns — two obvious facts will jump out at you.\\nFirst obvious fact: there is an incredibly wide divergence of success — some of those startups are insanely successful, some\\nhighly successful, many somewhat successful, and quite a few of\\ncourse outright fail.\\nSecond obvious fact: there is an incredibly wide divergence of\\ncaliber and quality for the three core elements of each startup\\n— team, product, and market.\\nAt any given startup, the team will range from outstanding to\\nremarkably Yawed; the product will range from a masterpiece\\nof engineering to barely functional; and the market will range\\nfrom booming to comatose.\\nAnd so you start to wonder — what correlates the most to success — team, product, or market? Or, more bluntly, what causes\\nsuccess? And, for those of us who are students of startup failure\\n— wha',\n",
              " 'tose.\\nAnd so you start to wonder — what correlates the most to success — team, product, or market? Or, more bluntly, what causes\\nsuccess? And, for those of us who are students of startup failure\\n— what’s most dangerous: a bad team, a weak product, or a\\npoor market?\\nLet’s start by deXning terms.\\nThe caliber of a startup team can be deXned as the suitability of\\nthe CEO, senior staW, engineers, and other key staW relative to\\nthe opportunity in front of them.\\nYou look at a startup and ask, will this team be able to optimally\\nexecute against their opportunity? I focus on eWectiveness as\\nopposed to experience, since the history of the tech industry is\\nfull of highly successful startups that were staWed primarily by\\npeople who had never “done it before”.\\nThe quality of a startup’s product can be deXned as how impressive the product is to one customer or user who actually uses\\nit: How easy is the product to use? How feature rich is it? How\\nfast is it? How extensible is it? How polished is it? ',\n",
              " 'deXned as how impressive the product is to one customer or user who actually uses\\nit: How easy is the product to use? How feature rich is it? How\\nfast is it? How extensible is it? How polished is it? How many (or\\nrather, how few) bugs does it have?\\nThe size of a startup’s market is the the number, and growth\\nrate, of those customers or users for that product.\\n(Let’s assume for this discussion that you can make money at\\nscale — that the cost of acquiring a customer isn’t higher than\\nthe revenue that customer will generate.)\\nSome people have been objecting to my classiXcation as follows:\\n“How great can a product be if nobody wants it?” In other words,\\nisn’t the quality of a product deXned by how appealing it is to\\nlots of customers?\\nNo. Product quality and market size are completely diWerent.\\nHere’s the classic scenario: the world’s best soaware application\\nfor an operating system nobody runs. Just ask any soaware\\ndeveloper targeting the market for BeOS, Amiga, OS/2, or\\nNeXT applications',\n",
              " '.\\nHere’s the classic scenario: the world’s best soaware application\\nfor an operating system nobody runs. Just ask any soaware\\ndeveloper targeting the market for BeOS, Amiga, OS/2, or\\nNeXT applications what the diWerence is between great product\\nand big market.\\nSo:\\nIf you ask entrepreneurs or VCs which of team, product, or market is most important, many will say team. This is the obvious\\n26 The Pmarca Blog Archives\\nanswer, in part because in the beginning of a startup, you know\\na lot more about the team than you do the product, which hasn’t\\nbeen built yet, or the market, which hasn’t been explored yet.\\nPlus, we’ve all been raised on slogans like “people are our most\\nimportant asset” — at least in the US, pro-people sentiments\\npermeate our culture, ranging from high school self-esteem\\nprograms to the Declaration of Independence’s inalienable\\nrights to life, liberty, and the pursuit of happiness — so the\\nanswer that team is the most important feels right.\\nAnd who wants to take the positio',\n",
              " 'ms to the Declaration of Independence’s inalienable\\nrights to life, liberty, and the pursuit of happiness — so the\\nanswer that team is the most important feels right.\\nAnd who wants to take the position that people don’t matter?\\nOn the other hand, if you ask engineers, many will say product.\\nThis is a product business, startups invent products, customers\\nbuy and use the products. Apple and Google are the best companies in the industry today because they build the best products. Without the product there is no company. Just try having\\na great team and no product, or a great market and no product.\\nWhat’s wrong with you? Now let me get back to work on the\\nproduct.\\nPersonally, I’ll take the third position — I’ll assert that market is\\nthe most important factor in a startup’s success or failure.\\nWhy?\\nIn a great market — a market with lots of real potential customers — the market pulls product out of the startup.\\nThe market needs to be fulXlled and the market will be fulXlled,\\nby the Xrst viab',\n",
              " 'Why?\\nIn a great market — a market with lots of real potential customers — the market pulls product out of the startup.\\nThe market needs to be fulXlled and the market will be fulXlled,\\nby the Xrst viable product that comes along.\\nThe product doesn’t need to be great; it just has to basically\\nwork. And, the market doesn’t care how good the team is, as\\nlong as the team can produce that viable product.\\nIn short, customers are knocking down your door to get the\\nproduct; the main goal is to actually answer the phone and\\nrespond to all the emails from people who want to buy.\\nPart 4: The only thing that matters 27\\nAnd when you have a great market, the team is remarkably easy\\nto upgrade on the Yy.\\nThis is the story of search keyword advertising, and Internet\\nauctions, and TCP/IP routers.\\nConversely, in a terrible market, you can have the best product\\nin the world and an absolutely killer team, and it doesn’t matter — you’re going to fail.\\nYou’ll break your pick for years trying to Xnd customers',\n",
              " ', in a terrible market, you can have the best product\\nin the world and an absolutely killer team, and it doesn’t matter — you’re going to fail.\\nYou’ll break your pick for years trying to Xnd customers who\\ndon’t exist for your marvelous product, and your wonderful\\nteam will eventually get demoralized and quit, and your startup\\nwill die.\\nThis is the story of videoconferencing, and workYow soaware,\\nand micropayments.\\nIn honor of Andy RachleW, formerly of Benchmark Capital, who\\ncrystallized this formulation for me, let me present RachleE’s\\nLaw of Startup Success:\\nThe #1 company-killer is lack of market.\\nAndy puts it this way:\\n• When a great team meets a lousy market, market wins.\\n• When a lousy team meets a great market, market wins.\\n• When a great team meets a great market, something special\\nhappens.\\nYou can obviously screw up a great market — and that has been\\ndone, and not infrequently — but assuming the team is baseline\\ncompetent and the product is fundamentally acceptable, a great\\nmar',\n",
              " 'happens.\\nYou can obviously screw up a great market — and that has been\\ndone, and not infrequently — but assuming the team is baseline\\ncompetent and the product is fundamentally acceptable, a great\\nmarket will tend to equal success and a poor market will tend to\\nequal failure. Market matters most.\\nAnd neither a stellar team nor a fantastic product will redeem a\\nbad market.\\nOK, so what?\\n28 The Pmarca Blog Archives\\nWell, Hrst question: Since team is the thing you have the most\\ncontrol over at the start, and everyone wants to have a great\\nteam, what does a great team actually get you?\\nHopefully a great team gets you at least an OK product, and ideally a great product.\\nHowever, I can name you a bunch of examples of great teams\\nthat totally screwed up their products. Great products are really,\\nreally hard to build.\\nHopefully a great team also gets you a great market — but I can\\nalso name you lots of examples of great teams that executed\\nbrilliantly against terrible markets and failed. Market',\n",
              " 'eally hard to build.\\nHopefully a great team also gets you a great market — but I can\\nalso name you lots of examples of great teams that executed\\nbrilliantly against terrible markets and failed. Markets that don’t\\nexist don’t care how smart you are.\\nIn my experience, the most frequent case of great team paired\\nwith bad product and/or terrible market is the second- or thirdtime entrepreneur whose Xrst company was a huge success.\\nPeople get cocky, and slip up. One highly successful soaware\\nentrepreneur is burning through something like $80 million in\\nventure funding in his latest startup and has practically nothing\\nto show for it except for some great press clippings and a couple of beta customers — because there is virtually no market for\\nwhat he is building.\\nConversely, I can name you any number of weak teams whose\\nstartups were highly successful due to explosively large markets\\nfor what they were doing.\\nFinally, to quote Tim Shephard: “A great team is a team that will\\nalways beat a med',\n",
              " 'umber of weak teams whose\\nstartups were highly successful due to explosively large markets\\nfor what they were doing.\\nFinally, to quote Tim Shephard: “A great team is a team that will\\nalways beat a mediocre team, given the same market and product.”\\nSecond question: Can’t great products sometimes create huge\\nnew markets?\\nAbsolutely.\\nThis is a best case scenario, though.\\nVMWare is the most recent company to have done it —\\nPart 4: The only thing that matters 29\\nVMWare’s product was so profoundly transformative out of the\\ngate that it catalyzed a whole new movement toward operating\\nsystem virtualization, which turns out to be a monster market.\\nAnd of course, in this scenario, it also doesn’t really matter\\nhow good your team is, as long as the team is good enough to\\ndevelop the product to the baseline level of quality the market\\nrequires and get it fundamentally to market.\\nUnderstand I’m not saying that you should shoot low in terms of quality\\nof team, or that VMWare’s team was not incredibl',\n",
              " 'seline level of quality the market\\nrequires and get it fundamentally to market.\\nUnderstand I’m not saying that you should shoot low in terms of quality\\nof team, or that VMWare’s team was not incredibly strong —\\nit was, and is. I’m saying, bring a product as transformative as\\nVMWare’s to market and you’re going to succeed, full stop.\\nShort of that, I wouldn’t count on your product creating a new\\nmarket from scratch.\\nThird question: as a startup founder, what should I do about\\nall this?\\nLet’s introduce RachleE’s Corollary of Startup Success:\\nThe only thing that matters is getting to product/market Ft.\\nProduct/market Xt means being in a good market with a product\\nthat can satisfy that market.\\nYou can always feel when product/market Ft isn’t happening. The\\ncustomers aren’t quite getting value out of the product, word\\nof mouth isn’t spreading, usage isn’t growing that fast, press\\nreviews are kind of “blah”, the sales cycle takes too long, and lots\\nof deals never close.\\nAnd you can always fe',\n",
              " 'out of the product, word\\nof mouth isn’t spreading, usage isn’t growing that fast, press\\nreviews are kind of “blah”, the sales cycle takes too long, and lots\\nof deals never close.\\nAnd you can always feel product/market Ft when it’s happening. The\\ncustomers are buying the product just as fast as you can make it\\n— or usage is growing just as fast as you can add more servers.\\nMoney from customers is piling up in your company checking\\naccount. You’re hiring sales and customer support staW as fast as\\nyou can. Reporters are calling because they’ve heard about your\\nhot new thing and they want to talk to you about it. You start\\ngetting entrepreneur of the year awards from Harvard Busi30 The Pmarca Blog Archives\\nness School. Investment bankers are staking out your house. You\\ncould eat free for a year at Buck’s.\\nLots of startups fail before product/market Ht ever happens.\\nMy contention, in fact, is that they fail because they never get to\\nproduct/market Xt.\\nCarried a step further, I believe that ',\n",
              " 'r at Buck’s.\\nLots of startups fail before product/market Ht ever happens.\\nMy contention, in fact, is that they fail because they never get to\\nproduct/market Xt.\\nCarried a step further, I believe that the life of any startup can be\\ndivided into two parts: before product/market Ft (call this “BPMF”)\\nand aMer product/market Ft (“APMF”).\\nWhen you are BPMF, focus obsessively on getting to product/\\nmarket Ht.\\nDo whatever is required to get to product/market Ht. Including\\nchanging out people, rewriting your product, moving into a different market, telling customers no when you don’t want to,\\ntelling customers yes when you don’t want to, raising that fourth\\nround of highly dilutive venture capital — whatever is required.\\nWhen you get right down to it, you can ignore almost everything else.\\nI’m not suggesting that you do ignore everything else — just that\\njudging from what I’ve seen in successful startups, you can.\\nWhenever you see a successful startup, you see one that has\\nreached product/mark',\n",
              " ' suggesting that you do ignore everything else — just that\\njudging from what I’ve seen in successful startups, you can.\\nWhenever you see a successful startup, you see one that has\\nreached product/market Ht — and usually along the way\\nscrewed up all kinds of other things, from channel model to\\npipeline development strategy to marketing plan to press relations to compensation policies to the CEO sleeping with the\\nventure capitalist. And the startup is still successful.\\nConversely, you see a surprising number of really well-run\\nstartups that have all aspects of operations completely buttoned\\ndown, HR policies in place, great sales model, thoroughly\\nthought-through marketing plan, great interview processes,\\noutstanding catered food, 30″ monitors for all the programmers, top tier VCs on the board — heading straight oG a cliG\\ndue to not ever Hnding product/market Ht.\\nPart 4: The only thing that matters 31\\nIronically, once a startup is successful, and you ask the founders\\nwhat made it success',\n",
              " 'rd — heading straight oG a cliG\\ndue to not ever Hnding product/market Ht.\\nPart 4: The only thing that matters 31\\nIronically, once a startup is successful, and you ask the founders\\nwhat made it successful, they will usually cite all kinds of things\\nthat had nothing to do with it. People are terrible at understanding causation. But in almost every case, the cause was actually\\nproduct/market Xt.\\nBecause, really, what else could it possibly be?\\n[Editorial note: this post obviously raises way more questions than it\\nanswers. How exactly do you go about getting to product/market Ft if\\nyou don’t hit it right out of the gate? How do you evaluate markets for\\nsize and quality, especially before they’re fully formed? What actually\\nmakes a product “Ft” a market? What role does timing play? How do\\nyou know when to change strategy and go aMer a diEerent market or\\nbuild a diEerent product? When do you need to change out some or all\\nof your team? And why can’t you count on on a great team to build the\\n',\n",
              " 'now when to change strategy and go aMer a diEerent market or\\nbuild a diEerent product? When do you need to change out some or all\\nof your team? And why can’t you count on on a great team to build the\\nright product and Fnd the right market? All these topics will be discussed in future posts in this series.]\\n32 The Pmarca Blog Archives\\nPart 5: The Moby Dick theory of big\\ncompanies\\n“There she blows,” was sung out from the mast-head.\\n“Where away?” demanded the captain.\\n“Three points oE the lee bow, sir.”\\n“Raise up your wheel. Steady!” “Steady, sir.”\\n“Mast-head ahoy! Do you see that whale now?”\\n“Ay ay, sir! A shoal of Sperm Whales! There she blows! There she breaches!”\\n“Sing out! sing out every time!”\\n“Ay Ay, sir! There she blows! there — there — THAR she blows — bowes\\n— bo-o-os!”\\n“How far oE?”\\n“Two miles and a half.”\\n“Thunder and lightning! so near! Call all hands.”\\n– J. Ross Browne’s Etchings of a Whaling Cruize, 1846\\nThere are times in the life of a startup when you have to deal\\nwith big',\n",
              " '\\n“Two miles and a half.”\\n“Thunder and lightning! so near! Call all hands.”\\n– J. Ross Browne’s Etchings of a Whaling Cruize, 1846\\nThere are times in the life of a startup when you have to deal\\nwith big companies.\\nMaybe you’re looking for a partnership or distribution deal.\\nPerhaps you want an investment. Sometimes you want a marketing or sales alliance. From time to time you need a big com-\\npany’s permission to do something. Or maybe a big company\\nhas approached you and says it wants to buy your startup.\\nThe most important thing you need to know going into any discussion or interaction with a big company is that you’re Captain\\nAhab, and the big company is Moby Dick.\\n“Scarcely had we proceeded two days on the sea, when about sunrise a great many Whales and other monsters of the sea, appeared.\\nAmong the former, one was of a most monstrous size. … This came\\ntowards us, open-mouthed, raising the waves on all sides, and beating the sea before him into a foam.”\\n— Tooke’s Lucian, “The True His',\n",
              " '.\\nAmong the former, one was of a most monstrous size. … This came\\ntowards us, open-mouthed, raising the waves on all sides, and beating the sea before him into a foam.”\\n— Tooke’s Lucian, “The True History”\\nWhen Captain Ahab went in search of the great white whale\\nMoby Dick, he had absolutely no idea whether he would Xnd\\nMoby Dick, whether Moby Dick would allow himself to be\\nfound, whether Moby Dick would try to immediately capsize\\nthe ship or instead play cat and mouse, or whether Moby Dick\\nwas oW mating with his giant whale girlfriend.\\nWhat happened was entirely up to Moby Dick.\\nAnd Captain Ahab would never be able explain to himself or\\nanyone else why Moby Dick would do whatever it was he’d do.\\nYou’re Captain Ahab, and the big company is Moby Dick.\\n“Clap eye on Captain Ahab, young man, and thou wilt Xnd that he\\nhas only one leg.”\\n“What do you mean, sir? Was the other one lost by a whale?”\\n“Lost by a whale! Young man, come nearer to me: it was devoured,\\nchewed up, crunched by the mons',\n",
              " 'nd thou wilt Xnd that he\\nhas only one leg.”\\n“What do you mean, sir? Was the other one lost by a whale?”\\n“Lost by a whale! Young man, come nearer to me: it was devoured,\\nchewed up, crunched by the monstrousest parmacetty that ever\\nchipped a boat! — ah, ah!”\\n— Moby Dick\\nHere’s why:\\nThe behavior of any big company is largely inexplicable when\\nviewed from the outside.\\n34 The Pmarca Blog Archives\\nI always laugh when someone says, “Microsoa is going to do X”,\\nor “Google is going to do Y”, or “Yahoo is going to do Z”.\\nOdds are, nobody inside Microsoa, Google, or Yahoo knows\\nwhat Microsoa, Google, or Yahoo is going to do in any given circumstance on any given issue.\\nSure, maybe the CEO knows, if the issue is really big, but you’re\\nprobably not dealing at the CEO level, and so that doesn’t matter.\\nThe inside of any big company is a very, very complex system\\nconsisting of many thousands of people, of whom at least hundreds and probably thousands are executives who think they\\nhave some level of d',\n",
              " 'The inside of any big company is a very, very complex system\\nconsisting of many thousands of people, of whom at least hundreds and probably thousands are executives who think they\\nhave some level of decision-making authority.\\nOn any given issue, many people inside the company are going\\nto get some kind of vote on what happens — maybe 8 people,\\nmaybe 10, 15, 20, sometimes many more.\\nWhen I was at IBM in the early 90’s, they had a formal decision\\nmaking process called “concurrence” — on any given issue, a\\nwritten list of the 50 or so executives from all over the company\\nwho would be aWected by the decision in any way, no matter\\nhow minor, would be assembled, and any one of those executives could “nonconcur” and veto the decision. That’s an\\nextreme case, but even a non-extreme version of this process —\\nand all big companies have one; they have to — is mind-bendingly complex to try to understand, even from the inside, let\\nalone the outside.\\n“… and the breath of the whale is frequently atte',\n",
              " ' process —\\nand all big companies have one; they have to — is mind-bendingly complex to try to understand, even from the inside, let\\nalone the outside.\\n“… and the breath of the whale is frequently attended with such an\\ninsupportable smell, as to bring on a disorder of the brain.”\\n— Ulloa’s South America\\nYou can count on there being a whole host of impinging forces\\nthat will aWect the dynamic of decision-making on any issue at\\na big company.\\nThe consensus building process, trade-oWs, quids pro quo, politics, rivalries, arguments, mentorships, revenge for past wrongs,\\nPart 5: The Moby Dick theory of big companies 35\\nturf-building, engineering groups, product managers, product\\nmarketers, sales, corporate marketing, Xnance, HR, legal, channels, business development, the strategy team, the international\\ndivisions, investors, Wall Street analysts, industry analysts, good\\npress, bad press, press articles being written that you don’t know\\nabout, customers, prospects, lost sales, prospects on th',\n",
              " 'national\\ndivisions, investors, Wall Street analysts, industry analysts, good\\npress, bad press, press articles being written that you don’t know\\nabout, customers, prospects, lost sales, prospects on the fence,\\npartners, this quarter’s sales numbers, this quarter’s margins, the\\nbond rating, the planning meeting that happened last week, the\\nplanning meeting that got cancelled this week, bonus programs,\\npeople joining the company, people leaving the company, people getting Xred by the company, people getting promoted, people getting sidelined, people getting demoted, who’s sleeping\\nwith whom, which dinner party the CEO went to last night,\\nthe guy who prepares the Powerpoint presentation for the staW\\nmeeting accidentally putting your startup’s name in too small a\\nfont to be read from the back of the conference room…\\nYou can’t possibly even identify all the factors that will come to\\nbear on a big company’s decision, much less try to understand\\nthem, much less try to inYuence them very much a',\n",
              " 'f the conference room…\\nYou can’t possibly even identify all the factors that will come to\\nbear on a big company’s decision, much less try to understand\\nthem, much less try to inYuence them very much at all.\\n“The larger whales, whalers seldom venture to attack. They stand in\\nso great dread of some of them, that when out at sea they are afraid\\nto mention even their names, and carry dung, lime-stone, juniperwood, and some other articles of the same nature in their boats, in\\norder to terrify and prevent their too near approach.”\\n— Uno Von Troil’s Letters on Banks’s and Solander’s Voyage to Iceland In 1772\\nBack to Moby Dick.\\nMoby Dick might stalk you for three months, then jump out of\\nthe water and raise a huge ruckus, then vanish for six months,\\nthen come back and beach your whole boat, or alternately give\\nyou the clear shot you need to harpoon his giant butt.\\nAnd you’re never going to know why.\\nA big company might study you for three months, then\\napproach you and tell you they want to inv',\n",
              " 'ernately give\\nyou the clear shot you need to harpoon his giant butt.\\nAnd you’re never going to know why.\\nA big company might study you for three months, then\\napproach you and tell you they want to invest in you or partner\\nwith you or buy you, then vanish for six months, then come out\\n36 The Pmarca Blog Archives\\nwith a directly competitive product that kills you, or alternately\\nacquire you and make you and your whole team rich.\\nAnd you’re never going to know why.\\nThe upside of dealing with a big company is that there’s potentially a ton of whale meat in it for you.\\nSorry, mixing my metaphors. The right deal with the right big\\ncompany can have a huge impact on a startup’s success.\\n“And what thing soever besides cometh within the chaos of this\\nmonster’s mouth, be it beast, boat, or stone, down it goes all incontinently that foul great swallow of his, and perisheth in the bottomless gulf of his paunch.”\\n— Holland’s Plutarch’s Morals\\nThe downside of dealing with a big company is that he can',\n",
              " \"n it goes all incontinently that foul great swallow of his, and perisheth in the bottomless gulf of his paunch.”\\n— Holland’s Plutarch’s Morals\\nThe downside of dealing with a big company is that he can capsize you — maybe by stepping on you in one way or another\\nand killing you, but more likely by wrapping you up in a bad\\npartnership that ends up holding you back, or just making you\\nwaste a huge amount of time in meetings and get distracted\\nfrom your core mission.\\nSo what to do?\\nFirst, don’t do startups that require deals with big companies to\\nmake them successful.\\nThe risk of never getting those deals is way too high, no matter\\nhow hard you are willing to work at it.\\nAnd even if you get the deals, they probably won’t work out the\\nway you hoped.\\n“‘Stern all!’ exclaimed the mate, as upon turning his head, he saw\\nthe distended jaws of a large Sperm Whale close to the head of the\\nboat, threatening it with instant destruction; — ‘Stern all, for your\\nlives!'”\\n— Wharton the Whale Killer\\nSecon\",\n",
              " \"ning his head, he saw\\nthe distended jaws of a large Sperm Whale close to the head of the\\nboat, threatening it with instant destruction; — ‘Stern all, for your\\nlives!'”\\n— Wharton the Whale Killer\\nSecond, never assume that a deal with a big company is closed\\nPart 5: The Moby Dick theory of big companies 37\\nuntil the ink hits the paper and/or the cash hits the company\\nbank account.\\nThere is always something that can cause a deal that looks like\\nit’s closed, to suddenly get blown to smithereens — or vanish\\nwithout a trace.\\nAt day-break, the three mast-heads were punctually manned\\nafresh.\\n“D’ye see him?” cried Ahab aaer allowing a little space for the light\\nto spread.\\n“See nothing, sir.”\\n— Moby Dick\\nThird, be extremely patient.\\nBig companies play “hurry up and wait” all the time. In the last\\nfew years I’ve dealt with one big East Coast technology company in particular that has played “hurry up and wait” with me\\nat least four separate times — including a mandatory immediate cross-country Yig\",\n",
              " 'w years I’ve dealt with one big East Coast technology company in particular that has played “hurry up and wait” with me\\nat least four separate times — including a mandatory immediate cross-country Yight just to have dinner with the #2 executive\\n— and has never followed through on anything.\\nIf you want a deal with a big company, it is probably going to\\ntake a lot longer to put together than you think.\\n“My God! Mr. Chace, what is the matter?” I answered, “we have\\nbeen stove by a whale.”\\n— “Narrative of the Shipwreck of the Whale Ship Essex of Nantucket, Which Was Attacked and Finally Destroyed by a Large\\nSperm Whale in the PaciXc Ocean” by Owen Chace of Nantucket,\\nFirst Mate of Said Vessel, New York, 1821\\nFourth, beware bad deals.\\nI am thinking of one startup right now that is extremely promising, has great technology and a unique oWering, that did two\\nbig deals early with high-proXle big company partners, and has\\nbecome completely hamstrung in its ability to execute on its\\ncore business',\n",
              " 'ising, has great technology and a unique oWering, that did two\\nbig deals early with high-proXle big company partners, and has\\nbecome completely hamstrung in its ability to execute on its\\ncore business as a result.\\n38 The Pmarca Blog Archives\\nFiLh, never, ever assume a big company will do the obvious\\nthing.\\nWhat is obvious to you — or any outsider — is probably not\\nobvious on the inside, once all the other factors that are\\ninvolved are taken into account.\\nSixth, be aware that big companies care a lot more about what\\nother big companies are doing than what any startup is doing.\\nHell, big companies oaen care a lot more about what other big\\ncompanies are doing than they care about what their customers\\nare doing.\\nMoby Dick cared a lot more about what the other giant white\\nwhales were doing than those annoying little people in that\\nYimsy boat.\\n“The Whale is harpooned to be sure; but bethink you, how you\\nwould manage a powerful unbroken colt, with the mere appliance\\nof a rope tied to the root',\n",
              " ' those annoying little people in that\\nYimsy boat.\\n“The Whale is harpooned to be sure; but bethink you, how you\\nwould manage a powerful unbroken colt, with the mere appliance\\nof a rope tied to the root of his tail.”\\n— A Chapter on Whaling in Ribs and Trucks\\nSeventh, if doing deals with big companies is going to be a key\\npart of your strategy, be sure to hire a real pro who has done it\\nbefore.\\nOnly the best and most experienced whalers had a chance at\\ntaking down Moby Dick.\\nThis is why senior sales and business development people get\\npaid a lot of money. They’re worth it.\\n“Oh! Ahab,” cried Starbuck, “not too late is it, even now, the third\\nday, to desist. See! Moby Dick seeks thee not. It is thou, thou, that\\nmadly seekest him!”\\n— Moby Dick\\nEighth, don’t get obsessed.\\nDon’t turn into Captain Ahab.\\nBy all means, talk to big companies about all kinds of things, but\\nPart 5: The Moby Dick theory of big companies 39\\nalways be ready to have the conversation just drop and to return\\nto your core ',\n",
              " 'Ahab.\\nBy all means, talk to big companies about all kinds of things, but\\nPart 5: The Moby Dick theory of big companies 39\\nalways be ready to have the conversation just drop and to return\\nto your core business.\\nRare is the startup where a deal with a big company leads to success, or lack thereof leads to huge failure.\\n(However, see also Microsoa and Digital Research circa 1981.\\nTalk about a huge whale.)\\nClosing thought:\\nDiving beneath the settling ship, the whale ran quivering along\\nits keel; but turning under water, swialy shot to the surface\\nagain, far oW the other bow, but within a few yards of Ahab’s\\nboat, where, for a time, the whale lay quiescent.\\n“…Towards thee I roll, thou all-destroying but unconquering whale;\\nto the last I grapple with thee; from hell’s heart I stab at thee; for\\nhate’s sake I spit my last breath at thee. Sink all coZns and all\\nhearses to one common pool! and since neither can be mine, let\\nme then tow to pieces, while still chasing thee, though tied to thee,\\nth',\n",
              " 'ate’s sake I spit my last breath at thee. Sink all coZns and all\\nhearses to one common pool! and since neither can be mine, let\\nme then tow to pieces, while still chasing thee, though tied to thee,\\nthou damned whale! THUS, I give up the spear!”\\nThe harpoon was darted; the stricken whale Yew forward; with\\nigniting velocity the line ran through the grooves; — ran foul. Ahab\\nstooped to clear it; he did clear it; but the Yying turn caught him\\nround the neck, and voicelessly as Turkish mutes bowstring their\\nvictim, he was shot out of the boat, ere the crew knew he was gone.\\n–Moby Dick\\n40 The Pmarca Blog Archives\\nPart 6: How much funding is too\\nlittle? Too much?\\nIn this post, I answer these questions:\\nHow much funding for a startup is too little?\\nHow much funding for a startup is too much?\\nAnd how can you know, and what can you do about it?\\nThe Xrst question to ask is, what is the correct, or appropriate,\\namount of funding for a startup?\\nThe answer to that question, in my view, is based my t',\n",
              " 'ow can you know, and what can you do about it?\\nThe Xrst question to ask is, what is the correct, or appropriate,\\namount of funding for a startup?\\nThe answer to that question, in my view, is based my theory that\\na startup’s life can be divided into two parts — Before Product/\\nMarket Fit, and AMer Product/Market Fit.\\nBefore Product/Market Fit, a startup should ideally raise at\\nleast enough money to get to Product/Market Fit.\\nALer Product/Market Fit, a startup should ideally raise at least\\nenough money to fully exploit the opportunity in front of it,\\nand then to get to proXtability while still fully exploiting that\\nopportunity.\\nI will further argue that the deXnition of “at least enough\\nmoney” in each case should include a substantial amount of\\nextra money beyond your default plan, so that you can withstand bad surprises. In other words, insurance. This is partic-\\nularly true for startups that have not yet achieved Product/\\nMarket Fit, since you have no real idea how long that will take.\\n',\n",
              " 'u can withstand bad surprises. In other words, insurance. This is partic-\\nularly true for startups that have not yet achieved Product/\\nMarket Fit, since you have no real idea how long that will take.\\nThese answers all sound obvious, but in my experience, a surprising number of startups go out to raise funding and do not\\nhave an underlying theory of how much money they are raising\\nand for precisely what purpose they are raising it.\\nWhat if you can’t raise that much money at\\nonce?\\nObviously, many startups Xnd that they cannot raise enough\\nmoney at one time to accomplish these objectives — but I\\nbelieve this is still the correct underlying theory for how much\\nmoney a startup should raise and around which you should orient your thinking.\\nIf you are Before Product/Market Fit and you can’t raise\\nenough money in one shot to get to Product/Market Fit, then\\nyou will need get as far as you can on each round and demonstrate progress towards Product/Market Fit when you raise each\\nnew round.\\nIf you',\n",
              " '\\nenough money in one shot to get to Product/Market Fit, then\\nyou will need get as far as you can on each round and demonstrate progress towards Product/Market Fit when you raise each\\nnew round.\\nIf you are ALer Product/Market Fit and you can’t raise enough\\nmoney in one shot to fully exploit your opportunity, you have\\na high-class problem and will probably — but not deXnitely —\\nXnd that it gets continually easier to raise new money as you\\nneed it.\\nWhat if you don’t want to raise that much\\nmoney at once?\\nYou can argue you should raise a smaller amount of money at\\na time, because if you are making progress — either BPMF or\\nAPMF — you can raise the rest of the money you need later, at\\na higher valuation, and give away less of the company.\\nThis is the reason some entrepreneurs who can raise a lot of\\nmoney choose to hold back.\\n42 The Pmarca Blog Archives\\nHere’s why you shouldn’t do that:\\nWhat are the consequences of not raising enough money?\\nNot raising enough money risks the survival of your',\n",
              " 't of\\nmoney choose to hold back.\\n42 The Pmarca Blog Archives\\nHere’s why you shouldn’t do that:\\nWhat are the consequences of not raising enough money?\\nNot raising enough money risks the survival of your company,\\nfor the following reasons:\\nFirst, you may have — and probably will have — unanticipated\\nsetbacks within your business.\\nMaybe a new product release slips, or you have unexpected\\nquality issues, or one of your major customers goes bankrupt, or\\na challenging new competitor emerges, or you get sued by a big\\ncompany for patent infringement, or you lose a key engineer.\\nSecond, the funding window may not be open when you need\\nmore money.\\nSometimes investors are highly enthusiastic about funding new\\nbusinesses, and sometimes they’re just not.\\nWhen they’re not — when the “window is shut”, as the saying\\ngoes — it is very hard to convince them otherwise, even though\\nthose are many of the best times to invest in startups because\\nof the prevailing atmosphere of fear and dread that is holding\\n',\n",
              " 'the saying\\ngoes — it is very hard to convince them otherwise, even though\\nthose are many of the best times to invest in startups because\\nof the prevailing atmosphere of fear and dread that is holding\\neveryone else back.\\nThose of us who were in startups that lived through 2001-2003\\nknow exactly what this can be like.\\nThird, something completely unanticipated, and bad, might\\nhappen.\\nAnother major terrorist attack is the one that I frankly worry\\nabout the most. A superbug. All-out war in the Middle East.\\nNorth Korea demonstrating the ability to launch a true nucleartipped ICBM. Giant Yaming meteorites. Such worst-case scenarios will not only close the funding window, they might keep\\nit closed for a long time.\\nFunny story: it turns out that a lot of Internet business models\\nfrom the late 90’s that looked silly at the time actually work\\nPart 6: How much funding is too little? Too much? 43\\nreally well — either in their original form or with some tweaking.\\nAnd there are quite a few startups f',\n",
              " 'that looked silly at the time actually work\\nPart 6: How much funding is too little? Too much? 43\\nreally well — either in their original form or with some tweaking.\\nAnd there are quite a few startups from the late 90’s that are\\ndoing just great today — examples being OpenTable (which is\\nabout to go public) and TellMe (which recently sold itself to\\nMicrosoa for $800 million), and my own company Opsware\\n— which would be bankrupt today if we hadn’t raised a ton of\\nmoney when we could, and instead just did its Xrst $100 million\\nrevenue year and has a roughly $1 billion public market value.\\nI’ll go so far as to say that the big diWerence between the startups\\nfrom that era that are doing well today versus the ones that no\\nlonger exist, is that the former group raised a ton of money\\nwhen they could, and the latter did not.\\nSo how much money should I raise?\\nIn general, as much as you can. Without giving away control of\\nyour company, and without being insane.\\nEntrepreneurs who try to play it too',\n",
              " 'd, and the latter did not.\\nSo how much money should I raise?\\nIn general, as much as you can. Without giving away control of\\nyour company, and without being insane.\\nEntrepreneurs who try to play it too aggressive and hold back\\non raising money when they can because they think they can\\nraise it later occasionally do very well, but are gambling their\\nwhole company on that strategy in addition to all the normal\\nstartup risks.\\nSuppose you raise a lot of money and you do really well. You’ll\\nbe really happy and make a lot of money, even if you don’t make\\nquite as much money as if you had rolled the dice and raised\\nless money up front.\\nSuppose you don’t raise a lot of money when you can and it\\nbackXres. You lose your company, and you’ll be really, really sad.\\nIs it really worth that risk?\\nThere is one additional consequence to raising a lot of money\\nthat you should bear in mind, although it is more important for\\nsome companies than others.\\n44 The Pmarca Blog Archives\\nThat is liquidation prefer',\n",
              " ' one additional consequence to raising a lot of money\\nthat you should bear in mind, although it is more important for\\nsome companies than others.\\n44 The Pmarca Blog Archives\\nThat is liquidation preference. In the scenario where your\\ncompany ultimately gets acquired: the more money you raise\\nfrom outside investors, the higher the acquisition price has to be\\nfor the founders and employees to make money on top of the\\ninitial payout to the investors.\\nIn other words, raising a lot of money can make it much harder\\nto eWectively sell your company for less than a very high price,\\nwhich you may not be able to get when the time comes.\\nIf you are convinced that your company is going to get bought,\\nand you don’t think the purchase price will be that high, then\\nraising less money is a good idea purely in terms of optimizing\\nfor your own Xnancial outcome. However, that strategy has lots\\nof other risks and will be addressed in another entertaining post,\\nto be entitled “Why building to Yip is a bad id',\n",
              " 'in terms of optimizing\\nfor your own Xnancial outcome. However, that strategy has lots\\nof other risks and will be addressed in another entertaining post,\\nto be entitled “Why building to Yip is a bad idea”.\\nTaking these factors into account, though, in a normal scenario,\\nraising more money rather than less usually makes sense,\\nsince you are buying yourself insurance against both internal\\nand external potential bad events — and that is more important than worrying too much about dilution or liquidation preference.\\nHow much money is too much?\\nThere are downside consequences to raising too much money.\\nI already discussed two of them — possibly incremental dilution\\n(which I dismissed as a real concern in most situations), and possibly excessively high liquidation preference (which should be\\nmonitored but not obsessed over).\\nThe big downside consequence to too much money, though,\\nis cultural corrosion.\\nYou don’t have to be in this industry very long before you run\\ninto the startup that has ra',\n",
              " 'tored but not obsessed over).\\nThe big downside consequence to too much money, though,\\nis cultural corrosion.\\nYou don’t have to be in this industry very long before you run\\ninto the startup that has raised a ton of money and has become\\ninfected with a culture of complacency, laziness, and arrogance.\\nRaising a ton of money feels really good — you feel like you’ve\\nPart 6: How much funding is too little? Too much? 45\\ndone something, that you’ve accomplished something, that\\nyou’re successful when a lot of other people weren’t.\\nAnd of course, none of those things are true.\\nRaising money is never an accomplishment in and of itself —\\nit just raises the stakes for all the hard work you would have had\\nto do anyway: actually building your business.\\nSome signs of cultural corrosion caused by raising too much\\nmoney:\\n• Hiring too many people — slows everything down and makes\\nit much harder for you to react and change. You are almost\\ncertainly setting yourself up for layoWs in the future, even if\\nyou',\n",
              " 'oo much\\nmoney:\\n• Hiring too many people — slows everything down and makes\\nit much harder for you to react and change. You are almost\\ncertainly setting yourself up for layoWs in the future, even if\\nyou are successful, because you probably won’t accurately\\nallocate the hiring among functions for what you will really\\nneed as your business grows.\\n• Lazy management culture — it is easy for a management\\nculture to get set where the manager’s job is simply to hire\\npeople, and then every other aspect of management suWers,\\nwith potentially disastrous long-term consequences to\\nmorale and eWectiveness.\\n• Engineering team bloat — another side eWect of hiring too\\nmany people; it’s very easy for engineering teams to get too\\nlarge, and it happens very fast. And then the “Mythical Man\\nMonth” eWect kicks in and everything slows to a crawl, your\\nbest people get frustrated and quit, and you’re in huge\\ntrouble.\\n• Lack of focus on product and customers — it’s a lot easier to\\nnot be completely obsessed with',\n",
              " 'n and everything slows to a crawl, your\\nbest people get frustrated and quit, and you’re in huge\\ntrouble.\\n• Lack of focus on product and customers — it’s a lot easier to\\nnot be completely obsessed with your product and your\\ncustomers when you have a lot of money in the bank and\\ndon’t have to worry about your doors closing imminently.\\n• Too many salespeople too soon — out selling a product that\\nisn’t quite ready yet, hasn’t yet achieved Product/Market Fit\\n— alienating early adopters and making it much harder to go\\nback when the product does get right.\\n• Product schedule slippage — what’s the urgency? We have all\\n46 The Pmarca Blog Archives\\nthis cash! Creating a golden opportunity for a smaller,\\nscrappier startup to come along and kick your rear.\\nSo what should you do if you do raise a lot of money?\\nAs my old boss Jim Barksdale used to say, the main thing is to keep\\nthe main thing the main thing — be just as focused on product and\\ncustomers when you raise a lot of money as you would be if',\n",
              " ' money?\\nAs my old boss Jim Barksdale used to say, the main thing is to keep\\nthe main thing the main thing — be just as focused on product and\\ncustomers when you raise a lot of money as you would be if you\\nhadn’t raised a lot of money.\\nEasy to say, hard to do, but worth it.\\nContinue to run as lean as you can, bank as much of the money\\nas possible, and save it for a rainy day — or a nuclear winter.\\nTell everyone inside the company, over and over and over, until\\nthey can’t stand it anymore, and then tell them some more,\\nthat raising money does not count as an accomplishment and that you\\nhaven’t actually done anything yet other than raise the stakes\\nand increase the pressure.\\nIllustrate that point by staying as scrappy as possible on material\\nitems — oZce space, furniture, etc. The two areas to splurge,\\nin my opinion, are big-screen monitors and ergonomic oZce\\nchairs. Other than that, it should be Ikea all the way.\\nThe easiest way to lose control of your spending when you raise\\ntoo much mo',\n",
              " 'o splurge,\\nin my opinion, are big-screen monitors and ergonomic oZce\\nchairs. Other than that, it should be Ikea all the way.\\nThe easiest way to lose control of your spending when you raise\\ntoo much money is to hire too many people. The second easiest\\nway is to pay people too much. Worry more about the Xrst one\\nthan the second one; more people multiply spending a lot faster\\nthan a few raises.\\nGenerally speaking, act like you haven’t raised nearly as much money\\nas you actually have — in how you talk, act, and spend.\\nIn particular, pay close attention to deadlines. The easiest thing to\\ngo wrong when you raise a lot of money is that suddenly things\\ndon’t seem so urgent anymore. Oh, they are. Competitors still\\nlurk behind every bush and every tree, metaphorically speaking. Keeping moving fast if you want to survive.\\nThere are certain startups that raised an excessive amount of money,\\nPart 6: How much funding is too little? Too much? 47\\nproceeded to spend it like drunken sailors, and went on',\n",
              " \"f you want to survive.\\nThere are certain startups that raised an excessive amount of money,\\nPart 6: How much funding is too little? Too much? 47\\nproceeded to spend it like drunken sailors, and went on to become\\nhugely successful. Odds are, you’re not them. Don’t bet your company on it.\\nThere are a lot more startups that raised an excessive amount of\\nmoney, burned through it, and went under.\\nRemember Geocast? General Magic? Microunity? HAL? Trilogy\\nSystems?\\nExactly.\\n48 The Pmarca Blog Archives\\nPart 7: Why a startup's initial\\nbusiness plan doesn't matter that\\nmuch\\nA startup’s initial business plan doesn’t matter that much,\\nbecause it is very hard to determine up front exactly what combination of product and market will result in success.\\nBy deXnition you will be doing something new, in a world that is\\na very uncertain place. You are simply probably not going to know\\nwhether your initial idea will work as a product and a business, or\\nnot. And you will probably have to rapidly evolve your \",\n",
              " 'ld that is\\na very uncertain place. You are simply probably not going to know\\nwhether your initial idea will work as a product and a business, or\\nnot. And you will probably have to rapidly evolve your plan —\\npossibly every aspect of it — as you go.\\n(The military has a saying that expresses the same concept —\\n“No battle plan ever survives contact with the enemy.” In this\\ncase, your enemy is the world at large.)\\nIt is therefore much more important for a startup to aggressively\\nseek out a big market, and product/market Ft within that market, once\\nthe startup is up and running, than it is to try to plan out what\\nyou are going to do in great detail ahead of time.\\nThe history of successful startups is quite clear on this topic.\\nNormally I would simply point to Microsoa, which started as\\na programming tools company before IBM all but forced Bill\\nGates to go into the operating system business, or Oracle, which\\nwas a consultancy for the CIA before Larry Ellison decided to\\nproductize the relation',\n",
              " 'ming tools company before IBM all but forced Bill\\nGates to go into the operating system business, or Oracle, which\\nwas a consultancy for the CIA before Larry Ellison decided to\\nproductize the relational database, or Intel, which was a much\\nsmaller company focused on the memory chip market until the\\nJapanese onslaught of the mid-80’s forced Andy Grove to switch\\nfocus to CPUs.\\nHowever, I’ve recently been reading Randall Stross’s marvelous\\nbook about Thomas Edison, The Wizard of Menlo Park.\\nEdison’s Xrst commercially viable breakthrough invention was\\nthe phonograph — the forerunner to what you kids know as\\nthe record player, the turntable, the Walkman, the CD player,\\nand the IPod. Edison went on, of course, to become one of the\\ngreatest inventors and innovators of all time.\\nAs our story begins, Edison, an unknown inventor running his\\nown startup, is focused on developing better hardware for telegraph operators. He is particularly focused on equipment for\\ntelegraph operators to be able to ',\n",
              " 's, Edison, an unknown inventor running his\\nown startup, is focused on developing better hardware for telegraph operators. He is particularly focused on equipment for\\ntelegraph operators to be able to send voice messages over telegraph lines.\\nCue the book:\\nThe day aaer Edison had noted the idea for recording voice messages received by a telegraphy oZce, he came up with a variation.\\nThat evening, on 18 July 1877, when [his lab’s] midnight dinner\\nhad been consumed… [Edison] turned around to face [his assistant\\nCharles Batchelor] and casually remarked, “Batch, if we had a point\\non this, we could make a record of some material which we could\\naaerwards pull under the point, and it would give us the speech\\nback.”\\nAs soon as Edison had pointed it out, it seemed so obvious that they\\ndid not pause to appreciate… the suggestion. Everyone jumped up\\nto rig a test… within an hour, they had the gizmo set up on the\\ntable… Edison sat down, leaned into the mouthpiece… [and] delivered the stock phrase th',\n",
              " 'o appreciate… the suggestion. Everyone jumped up\\nto rig a test… within an hour, they had the gizmo set up on the\\ntable… Edison sat down, leaned into the mouthpiece… [and] delivered the stock phrase the lab used to test telephone diaphragms:\\n“Mary had a little lamb.”\\n…Batchelor reinserted the beginning of the [strip on which the\\nphrase had been recorded]… out came “ary ad ell am.” “It was not\\nXne talking,” Batchelor recalled, “but the shape of it was there.”\\nThe men celebrated with a whoop, shook hands with one another,\\nand worked on. By breakfast the following morning, they had\\nsucceeded in getting clear articulation from waxed paper, the Xrst\\nrecording medium — in the Xrst midnight recording session.\\n50 The Pmarca Blog Archives\\n…The discovery was treated suprisingly casually in the lab’s notebooks…\\nIt was a singular moment in the modern history of invention, but,\\nin the years that would follow, Edison would never tell the story the\\nway it actually unfolded that summer, always moving t',\n",
              " 'notebooks…\\nIt was a singular moment in the modern history of invention, but,\\nin the years that would follow, Edison would never tell the story the\\nway it actually unfolded that summer, always moving the events\\nfrom July 1877 to December. We may guess the reason why: in July,\\nhe and his assistants failed to appreciate what they had discovered.\\nAt the time, they were working feverishly to develop a set of working telephones to show their best prospect… Western Union… There\\nwas no time to pause and reYect on the incidental invention of what\\nwas the Xrst working model of the phonograph…\\nThe invention continued to be labeled in the notebooks with the\\nbroader rubric “speaking telegraph”, reYecting the assumption that\\nit would be put to use in the telegraph oZce, recording messages.\\nAn unidentiXed staW member draw up a list of possible names\\nfor the machine, which included: tel-autograph, tel-autophone,\\n“chronophone = time-announcer = speaking clock”, “didaskophone\\n= teaching speaker = portab',\n",
              " 'd staW member draw up a list of possible names\\nfor the machine, which included: tel-autograph, tel-autophone,\\n“chronophone = time-announcer = speaking clock”, “didaskophone\\n= teaching speaker = portable teacher”, “glottophone = language\\nsounder or speaker”, “climatophone = weather announcer”, “klangophone = bird cry sounder”, “hulagmophone = barking sounder”…\\n…In October 1877, [Edison] wrote his father that he was “at present\\nvery hard up for cash,” but if his “speaking telegraph” was successful, he would receive an advance on royalties. The commercial\\npotential of his still-unnamed recording apparatus remained out of\\nsight…\\n[A description of the phonograph in ScientiXc American in early\\nNovember] set oW a frenzy in America and Europe. The New York\\nSun was fascinated by the metaphysical implications of an invention that could play “echoes from dead voices”. The New York\\nTimes predicted [in an eerie foreshadowing of their bizarre coverage of the Internet in the mid-1990’s] that a large ',\n",
              " \"plications of an invention that could play “echoes from dead voices”. The New York\\nTimes predicted [in an eerie foreshadowing of their bizarre coverage of the Internet in the mid-1990’s] that a large business would\\ndevelop in “bottled sermons”, and wealthy connoisseurs would take\\npride in keeping “a well-stocked oratorical cellar.”\\n…Such was the authority of ScientiXc American’s imprimatur that\\nall of this extraordinary attention was lavished not on the Xrst\\nworking phonograph made for public inspection, but merely a\\ndescription supplied by Edison’s assistant.\\n…By late November, Edison and his staW had caught onto the\\nphonograph’s commercial potential as a gadget for entertainment…\\na list of possible uses for the phonograph was noted [by Edison\\nand his staW], assembled apparently by free association: speaking\\ntoys (dogs, reptiles, humans), whistling toy train engines, music\\nPart 7: Why a startup's initial business plan doesn't matter that much 51\\nboxes, clocks and watches that announce\",\n",
              " \"ssociation: speaking\\ntoys (dogs, reptiles, humans), whistling toy train engines, music\\nPart 7: Why a startup's initial business plan doesn't matter that much 51\\nboxes, clocks and watches that announced the time. There was\\neven an inkling of the future importance of personal music collections, here described as the machine for the whole family to\\nenjoy, equipped with a thousand [music recordings], “giving endless amusement.”\\n…The Xrst actual model, however, remained to be built… On 4\\nDecember 1877, Batchelor’s diary laconically noted, “[staW member\\nJohn Kruesi] made phonograph today”; it received no more notice\\nthan the other entry, “working on speaking tel”, the invention [for\\ntelegraph operators] that continued to be at the top of the laboratory’s research agenda…\\n…On 7 December 1877, [Edison] walked into the New York oZce of\\nScientiXc American, placed a small machine on the editor’s desk,\\nand with about a dozen people gathered around, turned the crank.\\n“How do you do?” asked the mach\",\n",
              " 'ison] walked into the New York oZce of\\nScientiXc American, placed a small machine on the editor’s desk,\\nand with about a dozen people gathered around, turned the crank.\\n“How do you do?” asked the machine, introducing itself crisply.\\n“How do you like the phonograph?” It itself was feeling quite well,\\nit assured its listeners, and then cordially bid everyone a good\\nnight…\\n…Having long worked within the world of telegraphic equipment,\\n[Edison] had been perfectly placed to receive the technical inspiration for the phonograph. But that same world, oriented to a handful of giant industrial customers, had nothing in common with the\\nconsumer marketplace…\\nThe story goes on and on — and you should read the book; it’s\\nall like this.\\nThe point is this:\\nIf Thomas Edison didn’t know what he had when he invented\\nthe phonograph while he thought he was trying to create better\\nindustrial equipment for telegraph operators…\\n…what are the odds that you — or any entrepreneur — is going\\nto have it all Xgured',\n",
              " 'invented\\nthe phonograph while he thought he was trying to create better\\nindustrial equipment for telegraph operators…\\n…what are the odds that you — or any entrepreneur — is going\\nto have it all Xgured out up front?\\n52 The Pmarca Blog Archives\\nThe Pmarca Guide to Hiring\\nPart 8: Hiring, managing, promoting,\\nand Dring executives\\nOne of the most critical things a startup founder must do\\nis develop a top-notch executive team. This is a topic that could Xll a\\nwhole book, but in this post I will provide speciXc guidelines on\\nhow to hire, manage, promote, and Xre executives in a startup\\nbased on my personal observations and experiences.\\nFor the purposes of this post, deXnitions: An executive is\\na leader — someone who runs a function within the company\\nand has primary responsibility for an organization within the\\ncompany that will contribute to the company’s success or failure. The diWerence between an executive and a manager is that\\nthe executive has a higher degree of latitude to organize, ma',\n",
              " 'tion within the\\ncompany that will contribute to the company’s success or failure. The diWerence between an executive and a manager is that\\nthe executive has a higher degree of latitude to organize, make\\ndecisions, and execute within her function than a manager. The\\nmanager may ask what the right thing to do is; the executive should\\nknow.\\nThe general theory of executives, like managers, is, per Andy\\nGrove: the output of an executive is the output of her organization.\\nTherefore, the primary task of an executive is to maximize the\\noutput of her organization. However, in a startup, a successful\\nexecutive must accomplish three other critical tasks simultaneously:\\n• Build her organization– typically when an executive arrives\\nor is promoted into her role at a startup, she isn’t there to be\\na caretaker; rather she must build her organization, oaen\\nfrom scratch. This is a sharp diWerence from many big\\ncompany executives, who can spend their entire careers\\nrunning organizations other people buil',\n",
              " 'taker; rather she must build her organization, oaen\\nfrom scratch. This is a sharp diWerence from many big\\ncompany executives, who can spend their entire careers\\nrunning organizations other people built — oaen years or\\ndecades earlier.\\n• Be a primary individual contributor– a startup executive\\nmust “roll up her sleeves” and produce output herself. There\\nare no shortage of critical things to be done at a startup, and\\nan executive who cannot personally produce while\\nsimultaneously building and running her organization\\ntypically will not last long. Again, this is a sharp diWerence\\nfrom many big companies, where executives oaen serve\\nmore as administrators and bureaucrats.\\n• Be a team player– a startup executive must take personal\\nresponsibility for her relationships with her peers and people\\nthroughout the startup, in all functions and at all levels. Big\\ncompanies can oaen tolerate internal rivalries and warfare;\\nstartups cannot.\\nBeing a startup executive is not an easy job. The rewards ar',\n",
              " 'hroughout the startup, in all functions and at all levels. Big\\ncompanies can oaen tolerate internal rivalries and warfare;\\nstartups cannot.\\nBeing a startup executive is not an easy job. The rewards are\\nsubstantial — the ability to contribute directly to the startups’s\\nsuccess; the latitude to build and run an organization according\\nto her own theories and principles; and a meaningful equity\\nstake that can lead to personal Xnancial independence if the\\nstartup succeeds — but the responsibilities are demanding and\\nintense.\\nHiring\\nFirst, if you’re not sure whether you need an executive for a function,\\ndon’t hire one.\\nStartups, particularly well-funded startups, oaen hire executives\\ntoo early. Particularly before a startup has achieved product/\\nmarket Xt, it is oMen better to have a highly motivated manager or\\ndirector running a function than an executive.\\nHiring an executive too quickly can lead to someone who is\\nreally expensive, sitting there in the middle of the room, doing\\nvery little.',\n",
              " 'vated manager or\\ndirector running a function than an executive.\\nHiring an executive too quickly can lead to someone who is\\nreally expensive, sitting there in the middle of the room, doing\\nvery little. Not good for the executive, not good for the rest of\\nPart 8: Hiring, managing, promoting, and Dring executives 55\\nthe team, not good for the burn rate, and not good for the company.\\nHire an executive only when it’s clear that you need one: when an\\norganization needs to get built; when hiring needs to accelerate;\\nwhen you need more processes and structure and rigor to how\\nyou do things.\\nSecond, hire the best person for the next nine months, not the next\\nthree years.\\nI’ve seen a lot of startups overshoot on their executive hires.\\nThey need someone to build the soaware development team\\nfrom four people to 30 people over the next nine months, so\\nthey hire an executive from a big company who has been running 400 people. That is usually death.\\nHire for what you need now — and for roughly the ne',\n",
              " ' people to 30 people over the next nine months, so\\nthey hire an executive from a big company who has been running 400 people. That is usually death.\\nHire for what you need now — and for roughly the next nine\\nmonths. At the very least, you will get what you need now, and\\nthe person you hire may well be able to scale and keep going for\\nyears to come.\\nIn contrast, if you overhire — if you hear yourself saying, “this\\nperson will be great when we get bigger” — you are most likely\\nhiring someone who, best case, isn’t that interested in doing\\nthings at the scale you need, and worst case, doesn’t know how.\\nThird, whenever possible, promote from within.\\nGreat companies develop their own executives. There are several reasons for this:\\n• You get to develop your best people and turn them into\\nexecutives, which is great for both them and you — this is the\\nsingle best, and usually the only, way to hold onto great\\npeople for long periods of time.\\n• You ensure that your executives completely know and\\n',\n",
              " 'es, which is great for both them and you — this is the\\nsingle best, and usually the only, way to hold onto great\\npeople for long periods of time.\\n• You ensure that your executives completely know and\\nunderstand your company culture, strategy, and ethics.\\n• Your existing people are the “devil you know” — anyone new\\ncoming from outside is going to have Yaws, oaen really\\nserious ones, but you probably won’t Xgure out what they are\\n56 The Pmarca Blog Archives\\nuntil aaer you’ve hired them. With your existing people, you\\nknow, and you minimize your odds of being shocked and\\nappalled.\\nOf course, this isn’t always possible. Which segues us directly\\ninto…\\nFourth, my list of the key things to look at, and for, when evaluating executive candidates:\\n• Look for someone who is hungry and driven– someone who\\nwants a shot at doing “their thing”. Someone who has been an\\nup and comer at a midsized company but wants a shot at\\nbeing a primary executive at a startup can be a great catch.\\n• Flip side of tha',\n",
              " ' who\\nwants a shot at doing “their thing”. Someone who has been an\\nup and comer at a midsized company but wants a shot at\\nbeing a primary executive at a startup can be a great catch.\\n• Flip side of that: beware people who have “done it\\nbefore”.Sometimes you do run into someone who has been VP\\nEngineering at four companies and loves it and wants to do it\\nat a Xah company. More oaen, you will be dealing with\\nsomeone who is no longer hungry and driven. This is a very,\\nvery big problem to end up with — be very careful.\\n• Don’t disqualify someone based on ego or cockiness– as long as\\nshe’s not insane. Great executives are high-ego — you want\\nsomeone driven to run things, driven to make decisions,\\nconXdent in herself and her abilities. I don’t mean loud and\\nobnoxious, I mean assured and determined, bleeding over\\ninto cocky. If a VC’s ideal investment is a company that will\\nsuccceed without him, then your ideal executive hire is someone who\\nwill succeed without you.\\n• Beware hiring a big compa',\n",
              " 'd, bleeding over\\ninto cocky. If a VC’s ideal investment is a company that will\\nsuccceed without him, then your ideal executive hire is someone who\\nwill succeed without you.\\n• Beware hiring a big company executive for a startup.The\\nexecutive skill sets required for a big company vs a startup\\nare very Even great big company executives frequently\\nhave no idea what to do once they arrive at a startup.\\n• In particular, really beware hiring an executive from an incredibly\\nsuccessful big company. This is oaen very tempting — who\\nwouldn’t want to bring onboard someone who sprinkles\\nsome of that IBM (in the 80’s), Microsoa (in the 90’s), or\\nGoogle (today) fairy dust on your startup? The issue is that\\npeople who have been at an incredibly successful big\\ncompany oaen cannot function in a normal, real world,\\nPart 8: Hiring, managing, promoting, and Dring executives 57\\ncompetitive situation where they don’t start every day with\\n80% market share. Back in the 80’s, you oaen heard, “never\\nhire anyone ',\n",
              " ' world,\\nPart 8: Hiring, managing, promoting, and Dring executives 57\\ncompetitive situation where they don’t start every day with\\n80% market share. Back in the 80’s, you oaen heard, “never\\nhire anyone straight out of IBM — Xrst, let them go\\nsomewhere else and fail, and then hire them”. Believe it.\\n• This probably goes without saying, but look for a pattern of\\noutput– accomplishment. Validate it by reference checking\\npeers, reports, and bosses. Along the way, reference check\\npersonality and teamwork, but look Xrst and foremost for a\\npattern of output.\\nFiah, by all means, use an executive recruiter, but for sourcing, not\\nevaluation.\\nThere are some executive recruiters who are actually really\\ngood at evaluation. Others are not. It’s beside the point. It’s your\\njob to evaluate and make the decision, not the recruiter’s.\\nI say this because I have never met a recruiter who lacks conXdence in his ability to evaluate candidates and pass judgment on\\nwho’s right for a given situation. This can lu',\n",
              " 'sion, not the recruiter’s.\\nI say this because I have never met a recruiter who lacks conXdence in his ability to evaluate candidates and pass judgment on\\nwho’s right for a given situation. This can lull a startup founder\\ninto relying on the recruiter’s judgment instead of really digging\\nin and making your own decision. Betting that your recruiter is\\ngreat at evaluation is not a risk you want to take. You’re the one\\nwho has to Xre the executive if it doesn’t work out.\\nSixth, be ready to pay market compensation, including more cash\\ncompensation than you want, but watch for red Gags in the compensation discussion.\\nYou want someone focused on upside — on building a company. That means, a focus on their stock option package Xrst\\nand foremost.\\nWatch out for candidates who want egregious amounts of cash,\\nhigh bonuses, restricted stock, vacation days, perks, or — worst\\nof all — guaranteed severance. A candiate who is focused on\\nthose things, as opposed to the option package, is not ready to\\ndo',\n",
              " 'of cash,\\nhigh bonuses, restricted stock, vacation days, perks, or — worst\\nof all — guaranteed severance. A candiate who is focused on\\nthose things, as opposed to the option package, is not ready to\\ndo a startup.\\nOn a related note, be careful about option accceleration in the\\nevent of change of control. This is oaen reasonable for support\\n58 The Pmarca Blog Archives\\nfunctions such as Xnance, legal, and HR where an acquirer\\nwould most likely not have a job for the startup executive in\\nany of those functions. But this is not reasonable, in my view,\\nfor core functions such as engineering, product management,\\nmarketing, or sales. You don’t want your key executives focused\\non selling the company — unless of course you want them\\nfocused on selling the company. Make your acceleration decisions accordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO w',\n",
              " 'ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — cons',\n",
              " 'en has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — consciously or subconsciously. Don’t let it happen to you — make sure the person you\\nhire into that role is way better than you used to be.\\nEighth, recognize that hiring an executive is a high-risk proposition.\\nYou oaen see a startup with a screwed up development process,\\nbut “when we get our VP of Engineering onboard, everything\\nwill get Xxed”. Or a startup that is missing its revenue targets, but\\n“when we get our VP of sales, reveue will take oW”.\\nHere’s the problem: in my experience, if you know what you’re\\ndoing, the odds of a given executive hire working out will be about\\nPart 8: Hiring, managing, promoting, and Dring executives 59\\n50/50. That is, about 50% of the time you’ll screw up and ultimately have to replace the person. (If you don’t know what\\nyou’re doing, your failure rate will b',\n",
              " 'ging, promoting, and Dring executives 59\\n50/50. That is, about 50% of the time you’ll screw up and ultimately have to replace the person. (If you don’t know what\\nyou’re doing, your failure rate will be closer to 100%.)\\nWhy? People are people. People are complicated. People have\\nYaws. You oaen don’t know what those Yaws are until aaer you\\nget to know them. Those Yaws are oaen fatal in an executive\\nrole. And more generally, sometimes the Xt just isn’t there.\\nThis is why I’m so gung ho on promoting from within. At least\\nthen you know what the Yaws are up front.\\nManaging\\nFirst, manage your executives.\\nIt’s not that uncommon to see startup founders, especially Xrsttimers, who hire executives and are then reluctant to manage\\nthem.\\nYou can see the thought process: I just hired this really great, really\\nexperienced VP of Engineering who has way more experience running\\ndevelopment teams than I ever did — I should just let him go do his\\nthing!\\nThat’s a bad idea. While respecting someone’s experi',\n",
              " 'really\\nexperienced VP of Engineering who has way more experience running\\ndevelopment teams than I ever did — I should just let him go do his\\nthing!\\nThat’s a bad idea. While respecting someone’s experience and\\nskills, you should nevertheless manage every executive as if she\\nwere a normal employee. This means weekly 1:1’s, performance\\nreviews, written objectives, career development plans, the whole\\nnine yards. Skimp on this and it is very easy for both your relationship with her and her eWectiveness in the company to skew\\nsideways.\\nThis even holds if you’re 22 and she’s 40, or 50, or 60! Don’t be\\nshy, that will just scare her — and justiXably so.\\nSecond, give your executives the latitude to run their organizations.\\nThis is the balancing act with the previous point, but it’s equally\\nimportant. Don’t micromanage.\\nThe whole point of having an executive is to have someone who\\n60 The Pmarca Blog Archives\\ncan Xgure out how to build and run an organization so that you\\ndon’t have to. Manage her,',\n",
              " '. Don’t micromanage.\\nThe whole point of having an executive is to have someone who\\n60 The Pmarca Blog Archives\\ncan Xgure out how to build and run an organization so that you\\ndon’t have to. Manage her, understand what she is doing, be\\nvery clear on the results you expect, but let her do the job.\\nHere’s the key corollary to that: if you want to give an executive full\\nlatitude, but you’re reluctant to do so because you’re not sure she can\\nmake it happen, then it’s probably time to Fre her.\\nIn my experience it’s not that uncommon for a founder or CEO\\nto be uncomfortable — sometimes only at a gut level — at really\\ngiving an executive the latitude to run with the ball. That is a\\nsureXre signal that the executive is not working out and probably needs to be Xred. More on that below.\\nThird, ruthlessly violate the chain of command in order to gather data.\\nI don’t mean going around telling people under an executive\\nwhat to do without her knowing about it. I mean, ask questions,\\ncontinually, at al',\n",
              " 'sly violate the chain of command in order to gather data.\\nI don’t mean going around telling people under an executive\\nwhat to do without her knowing about it. I mean, ask questions,\\ncontinually, at all levels of the organization. How are things\\ngoing? What do you think of the new hires? How oaen are you\\nmeeting with your manager? And so on.\\nYou never want the bulk of your information about a function\\ncoming from the executive running that function. That’s the\\nbest way to be completely and utterly surprised when everything blows up.\\nHere’s the kicker: a great executive never minds when the CEO talks\\nto people in her organization. In fact, she loves it, because it means\\nthe CEO just hears more great things about her.\\nIf you have an executive who doesn’t want you to talk to people\\nin her organization, you have a bad executive.\\nPromoting\\nThis will be controversial, but I am a big fan of promoting talented\\npeople as fast as you can — promoting up and comers into executive roles, and promoti',\n",
              " 'nization, you have a bad executive.\\nPromoting\\nThis will be controversial, but I am a big fan of promoting talented\\npeople as fast as you can — promoting up and comers into executive roles, and promoting executives into bigger and broader\\nresponsibilities.\\nPart 8: Hiring, managing, promoting, and Dring executives 61\\nYou can clearly overdo this — you can promote someone before\\nthey are ready and in the worst case, completely screw up their\\ncareer. (Seen it. Done it.) You can also promote someone to their\\nlevel of incompetence — the Peter Principle. (Seen it. Done it.)\\nHowever, life is short, startups move fast, and you have stuE to get\\ndone. You aren’t going to have the privilege of working with\\nthat many great, talented, high-potential people in your career.\\nWhen you Xnd one, promote her as fast as you can. Great for\\nher, great for the company, and great for you.\\nThis assumes you are properly training and managing her along the\\nway. That is lea as an exercise for the reader.\\nThe surest ',\n",
              " ' fast as you can. Great for\\nher, great for the company, and great for you.\\nThis assumes you are properly training and managing her along the\\nway. That is lea as an exercise for the reader.\\nThe surest sign someone is ready for promotion is when they’re doing\\na great job running their current team. Projects are getting done,\\nteam morale is good, new hires are top quality, people are\\nhappy. Time to promote some people into new challenges.\\nI’m a Xrm believer that most people who do great things are doing\\nthem for the Frst time. Returning to my theory of hiring, I’d rather\\nhave someone all Xred up to do something for the Xrst time\\nthan someone who’s done it before and isn’t that excited to do it\\nagain. You rarely go wrong giving someone who is high potential the shot.\\nThis assumes you can tell the high potential people apart from everyone\\nelse. That too is lea as an exercise for the reader.\\nFiring\\nFirst, recognize the paradox of deciding to Fre an executive.\\nThe paradox works like this:\\nIt ',\n",
              " 'the high potential people apart from everyone\\nelse. That too is lea as an exercise for the reader.\\nFiring\\nFirst, recognize the paradox of deciding to Fre an executive.\\nThe paradox works like this:\\nIt takes time to gather data to evaluate an executive’s performance. You can’t evaluate an executive based on her own output, like a normal employee — you have to evaluate her based\\non the output of her organization. It takes time for her to build\\nand manage her organization to generate output. Therefore, it\\n62 The Pmarca Blog Archives\\ntakes longer to evaluate the performance of an executive than a normal\\nemployee.\\nBut, an executive can cause far more damage than a normal\\nemployee. A normal employee doesn’t work out, Xne, replace\\nhim. An executive doesn’t work out, it can — worst case — permanently cripple her function and sometimes the entire company. Therefore, it is far more important to Fre a bad executive as fast\\nas possible, versus a normal employee.\\nSolution? There isn’t one. It’s a pe',\n",
              " 'ly cripple her function and sometimes the entire company. Therefore, it is far more important to Fre a bad executive as fast\\nas possible, versus a normal employee.\\nSolution? There isn’t one. It’s a permanent problem.\\nI once asked Andy Grove, one of the world’s all-time best CEOs,\\nabout this. He said, you always Xre a bad executive too late. If\\nyou’re really good, you’ll Xre her about three months too late.\\nBut you’ll always do it too late. If you did it fast enough that it\\nwasn’t too late, you wouldn’t have enough data, and you’d risk\\nbeing viewed as arbitrary and capricious by the rest of the organization.\\nSecond, the minute you have a bad feeling in your gut, start gathering\\ndata.\\nBack to the point on ruthlessly violating the chain of command\\n— get to it. Talk to everyone. Know what’s going on. Unless\\nyou’re paranoid — and, shockingly, I have met paranoid\\nfounders and CEOs, and not counting Andy Grove — you need\\nto gather the data because you’re going to need to Xre the executive — i',\n",
              " 'ing on. Unless\\nyou’re paranoid — and, shockingly, I have met paranoid\\nfounders and CEOs, and not counting Andy Grove — you need\\nto gather the data because you’re going to need to Xre the executive — if you’re good, in about three months.\\nIn the meantime, of course do everything you can to coach and\\ndevelop and improve the executive. If it works out, that’s great.\\nIf not, get ready.\\nA few speciXc things I think are critical to look for:\\n• Is the executive hiring?If there are open headcount slots and\\nnobody’s coming in the door, you have a problem. Just as bad\\nis when the new hires aren’t very good — when they’re\\nbringing down the average quality of the organization.\\n• Is the executive training and developing her people?Oaen in a\\nPart 8: Hiring, managing, promoting, and Dring executives 63\\nstartup, an executive is hired to take over a function that’s\\nalready been started, at least in rudimentary form. The\\npeople in that function should be noticeably better at their\\njobs, and highly respe',\n",
              " 'tartup, an executive is hired to take over a function that’s\\nalready been started, at least in rudimentary form. The\\npeople in that function should be noticeably better at their\\njobs, and highly respectful of the executive’s skills, within the\\nXrst several months at the very least. If not, you have a\\nproblem.\\n• What do the other executives think?Great executives are oaen\\nimperfect but their peers always respect them. If your other\\nexecutives are skeptical of a new executive aaer the Xrst few\\nmonths, you have a problem.\\n• Is it painful for you to interact with the executive?Do you try to\\navoid or cancel your 1:1’s? Does talking to her give you a\\nheadache? Do you oaen not understand what point she’s\\ntrying to make or why she’s focused on such an odd issue? If\\nthe answer to any of these questions is yes, you have a\\nproblem.\\nThird, Fre crisply.\\nFiring an executive sucks. It’s disruptive to the organization. It\\ncreates a lot of work for you — not least of which is you’ll have\\nto go Xnd some',\n",
              " 'ions is yes, you have a\\nproblem.\\nThird, Fre crisply.\\nFiring an executive sucks. It’s disruptive to the organization. It\\ncreates a lot of work for you — not least of which is you’ll have\\nto go Xnd someone else for the job. And, it risks making you\\nlook bad, since you’re the one who hired the person in the Xrst\\nplace.\\nAnd it always seems to happen at a critical time in your startup’s\\nlife, when the last thing you need is a distraction like this.\\nNevertheless, the only thing to do is do it, do it professionally,\\nmake clear to the organization what will happen next, and get\\non down the road.\\nIn my opinion the two most common mistakes people make\\nwhen they Xre executives both fall in the category of pulling\\none’s punches, and I highly recommend avoiding them:\\n• Long transition periods — tempting, but counterproductive.\\nConfusing, demoralizing, and just plain weird. Instead, make\\na clean break, put a new person in charge — even if only on\\nan acting basis — and get moving.\\n64 The Pmarca Blog ',\n",
              " 'g, but counterproductive.\\nConfusing, demoralizing, and just plain weird. Instead, make\\na clean break, put a new person in charge — even if only on\\nan acting basis — and get moving.\\n64 The Pmarca Blog Archives\\n• Demotion as an alternative to Xring (or, alternately, “I know,\\nwe’ll hire her a boss!”). Hate it. Great people don’t deal well\\nwith getting demoted. There is an occasional exception.\\nUnless you are positive you have such an exception, skip it,\\nand move directly to the conclusion.\\nFourth, don’t feel guilty.\\nYou’re not beheading anyone.\\nAnyone who got a job as an executive at a startup is going to\\nhave an easy time getting the next job. Aaer all, she can always\\npaint you as a crazy founder, or inept CEO.\\nMore oaen than not, when you Xre an executive, you are doing\\nher a favor — you are giving her a chance to Xnd a better Xt\\nin a diWerent company where she will be more valued, more\\nrespected, and more successful. This sounds mushy, but I mean\\nit. And if she can’t, then she has a mu',\n",
              " 'e giving her a chance to Xnd a better Xt\\nin a diWerent company where she will be more valued, more\\nrespected, and more successful. This sounds mushy, but I mean\\nit. And if she can’t, then she has a much deeper problem and\\nyou just dodged a huge bullet.\\nAnd on that cheery note, good luck!\\nCounterpoint: Ben Horowitz on\\nmicromanagement\\n[This is a guest post from my business partner Ben Horowitz, reacting\\nto my recent post about hiring, managing, promoting, and Fring executives. I have italicized the parts where he really tears into me for your\\nadded humor value.]\\nWhile I enjoyed Marc’s post on hiring and Xring executives, I\\nthink that he unfairly dissed micromanagement.\\nHere’s why.\\nEveryone knows that the hyper-controlling manager with the\\nsevere personality disorder who micromanages every crummy\\ndecision is no fun to work for. However, it is wrong to condemn\\nthe practice of micromanagement on that basis.\\nPart 8: Hiring, managing, promoting, and Dring executives 65\\nSpeciXcally, there are ',\n",
              " 'ummy\\ndecision is no fun to work for. However, it is wrong to condemn\\nthe practice of micromanagement on that basis.\\nPart 8: Hiring, managing, promoting, and Dring executives 65\\nSpeciXcally, there are times and situations where micromanaging executives is not just ok, but also the right thing to do.\\nAndy Grove has an excellent explanation of this in his classic\\nbook High Output Management, where he describes a concept\\ncalled “Task Relevant Maturity”. Andy explains that employees\\nwho are immature in a given task require detailed training and\\ninstruction. They need to be micromanaged. On the other\\nhand, if an employee is relatively mature in a task, then it is\\ncounterproductive and annoying to manage the details of their\\nwork.\\nThis is also true when managing executives. Marc might think\\nthat he hires an executive because she has the experience and\\nknow-how to comprehensively do her job, so any detailed\\ninstruction would be unwise and unwarranted. Marc would be\\nwrong about that. It turns o',\n",
              " 'he hires an executive because she has the experience and\\nknow-how to comprehensively do her job, so any detailed\\ninstruction would be unwise and unwarranted. Marc would be\\nwrong about that. It turns out that even — and maybe especially\\n— executives are also immature in certain tasks.\\nIt is almost always the case that a new executive will be immature in their understanding of your market, your technology,\\nand your company — its personnel, processes, and culture. Will\\nthe new head of engineering at Ning walk in the door with\\nMarc’s understanding of the development process or the technology base? Would it be better for this new head of engineering\\nto make guesses and use her own best — not so good– judgment, or for Marc to review the Xrst say 20 decisions until the\\nnew exec is fully up to speed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen ',\n",
              " 'ed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
              " 'm. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\\nthem yet. When managing, it’s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
              " 'nYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly objective\\nto Xx it, or…\\n(b) Intensively micromanage her interactions until she learns\\nthe fundamental interpersonal skills required to be an eWective\\nexecutive.\\nI am arguing that doing (a) will likely result in weak performance. The reason is that she very likely has no idea how to be\\neWective with her peers. If somebody is an executive, it’s very\\nlikely that somewhere along the line somebody gave her feedback — perhaps abstractly — about all of her weaknesses. Yet\\nthe weakness remains. As a result, executives generally require\\nmore hands-on management than lower level employees to\\nimprove weak areas.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and you’ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring execut',\n",
              " \"s.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and you’ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring executives 67\\nPart 9: How to hire a professional\\nCEO\\nDon’t.\\nIf you don’t have anyone on your founding team who is capable\\nof being CEO, then sell your company — now.\\nHow to hire the best people you've\\never worked with\\nThere are many aspects to hiring great people, and various people smarter than me have written extensively on the topic.\\nSo I’m not going to try to be comprehensive.\\nBut I am going to relay some lessons learned through hard\\nexperience on how to hire the best people you’ve ever worked\\nwith — particularly for a startup.\\nI’m going to cover two key areas in this post:\\n• Criteria: what to value when evaluating candidates.\\n• And process: how to actually run the hiring process, and if\\nnecessary the aaermath of making a mistake.\\nCriteria 7rst\\nLots of people will tell you to hire for intell\",\n",
              " 'alue when evaluating candidates.\\n• And process: how to actually run the hiring process, and if\\nnecessary the aaermath of making a mistake.\\nCriteria 7rst\\nLots of people will tell you to hire for intelligence.\\nEspecially in this industry.\\nYou will read, hire the smartest people out there and your company’s success is all but guaranteed.\\nI think intelligence, per se, is highly overrated.\\nSpeciXcally, I am unaware of any actual data that shows a cor-\\nrelation between raw intelligence, as measured by any of the\\nstandard metrics (educational achievement, intelligence tests, or\\nskill at solving logic puzzles) and company success.\\nNow, clearly you don’t want to hire dumb people, and clearly\\nyou’d like to work with smart people.\\nBut let’s get speciXc.\\nMost of the lore in our industry about the role of intelligence\\nin company success comes from two stratospherically successful companies — Microsoa, and now Google — that are famous\\nfor hiring for intelligence.\\nMicrosoa’s metric for intelligence w',\n",
              " ' of intelligence\\nin company success comes from two stratospherically successful companies — Microsoa, and now Google — that are famous\\nfor hiring for intelligence.\\nMicrosoa’s metric for intelligence was the ability to solve logic\\npuzzles.\\n(I don’t know if the new, MBA-heavy Microsoa still does this,\\nbut I do know this is how Microsoa in its heyday worked.)\\nFor example, a classic Microsoa interview question was: “Why\\nis a manhole cover round?”\\nThe right answer, of course, is, “Who cares? Are we in the manhole business?”\\n(Followed by twisting in your chair to look all around, getting\\nup, and leaving.)\\nGoogle, on the other hand, uses the metric of educational\\nachievement.\\nHave a PhD? Front of the line. Masters? Next. Bachelor’s? Go to\\nthe end.\\nIn apparent direct contraction to decades of experience in the\\ncomputer industry that PhD’s are the hardest people to motivate to ship commercially viable products — with rare exception. (Hi, Tim! Hi, Diego!)\\nNow, on the one hand, you can’t question',\n",
              " 'rience in the\\ncomputer industry that PhD’s are the hardest people to motivate to ship commercially viable products — with rare exception. (Hi, Tim! Hi, Diego!)\\nNow, on the one hand, you can’t question the level of success of\\neither company.\\n70 The Pmarca Blog Archives\\nMaybe they’re right.\\nBut maybe, just maybe, their success had a lot to do with other\\nfactors — say, huge markets, extreme aggressiveness, right time/\\nright place, key distribution deals, and at least in one case, great\\nproducts.\\nBecause here’s the problem: I’m not aware of another Microsoa\\nthat’s been built by hiring based on logic puzzles. And I’m not\\naware of another Google that’s been built by hiring PhD’s.\\nSo maybe there are other hiring criteria that are equally, or\\nmore, important.\\nHere’s what I think those criteria are.\\nDrive\\nI deXne drive as self-motivation — people who will walk right\\nthrough brick walls, on their own power, without having to be\\nasked, to achieve whatever goal is in front of them.\\nPeople with dri',\n",
              " \".\\nDrive\\nI deXne drive as self-motivation — people who will walk right\\nthrough brick walls, on their own power, without having to be\\nasked, to achieve whatever goal is in front of them.\\nPeople with drive push and push and push and push and push\\nuntil they succeed.\\nWinston Churchill aaer the evacuation of Dunkirk:\\n“We shall not Yag or fail. We shall go on to the end, we shall\\nXght in France, we shall Xght on the seas and oceans, we shall\\nXght with growing conXdence and growing strength in the air,\\nwe shall defend our Island, whatever the cost may be, we shall\\nXght on the beaches, we shall Xght on the landing grounds, we\\nshall Xght in the Xelds and in the streets, we shall Xght in the\\nhills; we shall never surrender.”\\nThat’s what you want.\\nSome people have it and some people don’t.\\nOf the people who have it, with some of them it comes from\\nguilt, oaen created by family pressure.\\nHow to hire the best people you've ever worked with 71\\nWith others, it comes from a burning desire to make it b\",\n",
              " \"ple who have it, with some of them it comes from\\nguilt, oaen created by family pressure.\\nHow to hire the best people you've ever worked with 71\\nWith others, it comes from a burning desire to make it big.\\nWith others, it comes from being incredibly Type A.\\nWhatever… go with it.\\nDrive is independent of educational experience, grade point\\naverages, and socioeconomic background.\\n(But Marc, isn’t a 4.0 GPA a sure sign of drive? Well, it’s a sign\\nthat the person is driven to succeed on predeXned tests with\\nclear criteria and a grader — in an environment where the student’s parents are oaen paying a lot of money for the privilege\\nof having their child take the tests. That may or may not be the\\nsame thing as being driven to succeed in the real world.)\\nDrive is even independent of prior career success.\\nDriven people don’t tend to stay long at places where they can’t\\nsucceed, and just because they haven’t succeeded in the wrong\\ncompanies doesn’t mean they won’t succeed at your company\\n— if they’\",\n",
              " 'ess.\\nDriven people don’t tend to stay long at places where they can’t\\nsucceed, and just because they haven’t succeeded in the wrong\\ncompanies doesn’t mean they won’t succeed at your company\\n— if they’re driven.\\nI think you can see drive in a candidate’s eyes, and in a candidate’s background.\\nFor the background part, I like to see what someone has done.\\nNot been involved in, or been part of, or watched happen, or\\nwas hanging around when it happened.\\nI look for something you’ve done, either in a job or (oaen better\\nyet) outside of a job.\\nThe business you started and ran in high school.\\nThe nonproXt you started and ran in college.\\nIf you’re a programmer: the open source project to which\\nyou’ve made major contributions.\\nSomething.\\nIf you can’t Xnd anything — if a candidate has just followed the\\n72 The Pmarca Blog Archives\\nrules their whole lives, showed up for the right classes and the\\nright tests and the right career opportunities without achieving\\nsomething distinct and notable, relative',\n",
              " 'e\\n72 The Pmarca Blog Archives\\nrules their whole lives, showed up for the right classes and the\\nright tests and the right career opportunities without achieving\\nsomething distinct and notable, relative to their starting point —\\nthen they probably aren’t driven.\\nAnd you’re not going to change them.\\nMotivating people who are fundamentally unmotivated is not\\neasy.\\nBut motivating people who are self-motivated is wind at your\\nback.\\nI like speciXcally looking for someone for which this job is their\\nbig chance to really succeed.\\nFor this reason, I like hiring people who haven’t done the speciXc job before, but are determined to ace it regardless.\\nI also like speciXcally looking for someone who comes from\\nsome kind of challenging background — a diZcult family situation, say, or someone who had to work his/her way through\\nschool — who is nevertheless on par with his/her more fortunate peers in skills and knowledge.\\nFinally, beware in particular people who have been at highly\\nsuccessful companies',\n",
              " \"ork his/her way through\\nschool — who is nevertheless on par with his/her more fortunate peers in skills and knowledge.\\nFinally, beware in particular people who have been at highly\\nsuccessful companies.\\nPeople used to say, back when IBM owned the industry: never\\nhire someone straight out of IBM. First, let them go somewhere\\nelse and fail. Then, once they’ve realized the real world is not\\nlike IBM, hire them and they’ll be great.\\nAnd remember, an awful lot of people who have been at hugely\\nsuccessful companies were just along for the ride.\\nCareer success is great to look for — but it’s critical to verify that\\nthe candidates out of hugely successful companies actually did\\nwhat they claim in their roles at those companies. And that they\\nreally get it, that the real world is a lot tougher than being IBM\\nin the 80’s, or Microsoa in the 90’s, or Google today.\\nHow to hire the best people you've ever worked with 73\\nCuriosity\\nCuriosity is a proxy for, do you love what you do?\\nAnyone who loves wh\",\n",
              " \"being IBM\\nin the 80’s, or Microsoa in the 90’s, or Google today.\\nHow to hire the best people you've ever worked with 73\\nCuriosity\\nCuriosity is a proxy for, do you love what you do?\\nAnyone who loves what they do is inherently intensely curious\\nabout their Xeld, their profession, their craa.\\nThey read about it, study it, talk to other people about it…\\nimmerse themselves in it, continuously.\\nAnd work like hell to stay current in it.\\nNot because they have to.\\nBut because they love to.\\nAnyone who isn’t curious doesn’t love what they do.\\nAnd you should be hiring people who love what they do.\\nAs an example, programmers.\\nSit a programmer candidate for an Internet company down and\\nask them about the ten most interesting things happening in\\nInternet soaware.\\nREST vs SOAP, the new Facebook API, whether Ruby on Rails\\nis scalable, what do you think of Sun’s new Java-based scripting\\nlanguage, Google’s widgets API, Amazon S3, etc.\\nIf the candidate loves their Xeld, they’ll have informed opinions\\non m\",\n",
              " 'er Ruby on Rails\\nis scalable, what do you think of Sun’s new Java-based scripting\\nlanguage, Google’s widgets API, Amazon S3, etc.\\nIf the candidate loves their Xeld, they’ll have informed opinions\\non many of these topics.\\nThat’s what you want.\\nNow, you might say, Marc, that’s great for a young kid who has a\\nlot of spare time to stay current, but what about the guy who has\\na family and only has time for a day job and can’t spend nights\\nand weekends reading blogs and staying that current?\\nWell, when you run into a person like that who isn’t current in\\ntheir Xeld, the other implication is that their day job isn’t keeping them current.\\n74 The Pmarca Blog Archives\\nIf they’ve been in that job for a while, then ask yourself, is the\\nkind of person you’re looking for really going to have tolerated\\nstaying in a day job where their skills and knowledge get stale,\\nfor very long?\\nReally?\\nRemember — because of the Internet, staying current in any\\nXeld no longer costs any money.\\nIn my experience, driv',\n",
              " 'taying in a day job where their skills and knowledge get stale,\\nfor very long?\\nReally?\\nRemember — because of the Internet, staying current in any\\nXeld no longer costs any money.\\nIn my experience, drive and curiosity seem to coincide pretty\\nfrequently.\\nThe easiest way to be driven is to be in a Xeld that you love, and\\nyou’ll automatically be curious.\\nEthics\\nEthics are hard to test for.\\nBut watch for any whiW of less than stellar ethics in any candidate’s background or references.\\nAnd avoid, avoid, avoid.\\nUnethical people are unethical by nature, and the odds of a\\nmetaphorical jailhouse conversion are quite low.\\nPriests, rabbis, and ministers should give people a second\\nchance on ethics — not hiring managers at startups.\\n‘NuW said.\\nOne way to test for an aspect of ethics — honesty — is to test for\\nhow someone reacts when they don’t know something.\\nPick a topic you know intimately and ask the candidate increasingly esoteric questions until they don’t know the answer.\\nThey’ll either say th',\n",
              " \"st for\\nhow someone reacts when they don’t know something.\\nPick a topic you know intimately and ask the candidate increasingly esoteric questions until they don’t know the answer.\\nThey’ll either say they don’t know, or they’ll try to bullshit you.\\nGuess what. If they bullshit you during the hiring process,\\nthey’ll bullshit you once they’re onboard.\\nHow to hire the best people you've ever worked with 75\\nA candidate who is conXdent in his own capabilities and ethical\\n— the kind you want — will say “I don’t know” because they\\nknow that the rest of the interview will demonstrate their\\nknowledge, and they know that you won’t react well to being\\nbullshitted — because they wouldn’t react well either.\\nHow to run the hiring process\\nFirst, have a written hiring process.\\nWhatever your hiring process is — write it down, and make sure\\neveryone has a copy of it, on paper.\\nIt’s continually shocking how many startups have a random hiring process, and as a result hire apparently randomly.\\nSecond, do bas\",\n",
              " 's — write it down, and make sure\\neveryone has a copy of it, on paper.\\nIt’s continually shocking how many startups have a random hiring process, and as a result hire apparently randomly.\\nSecond, do basic skills tests.\\nIt’s amazing how many people come in and interview for jobs\\nwhere their resume says they’re qualiXed, but ask them basic\\nquestions about how to do things in their domain, and they Yail.\\nFor example, test programmers on basic algorithms — linked\\nlists, binary searches.\\nJust in pseudocode — it doesn’t matter if they know the relevant\\nJava library calls.\\nIt does matter if they are unable to go up to the whiteboard and\\nwork their way through something that was covered in their Xrst\\nalgorithms course.\\nA lot of people come in and interview for programming jobs\\nwho, at their core, can’t program.\\nAnd it’s such a breath of fresh air when you get someone who\\njust goes, oh yeah, a linked list, sure, let me show you.\\nThe same principle applies to other Xelds.\\nFor a sales rep — have th',\n",
              " 'n’t program.\\nAnd it’s such a breath of fresh air when you get someone who\\njust goes, oh yeah, a linked list, sure, let me show you.\\nThe same principle applies to other Xelds.\\nFor a sales rep — have them sell you on your product all the way\\nto a closed deal.\\n76 The Pmarca Blog Archives\\nFor a marketing person — have them whiteboard out a launch\\nfor your new product.\\nThird, plan out and write down interview questions ahead of\\ntime.\\nI’m assuming that you know the right interview questions for\\nthe role — and frankly, if you don’t, you probably shouldn’t be\\nthe hiring manager for that position.\\nThe problem I’m addressing is: most people don’t know how to\\ninterview a candidate.\\nAnd even people who do know how, aren’t necessarily good at\\ncoming up with questions on the Yy.\\nSo just make sure you have questions planned out and assigned\\nto each interviewer ahead of time.\\nI do this myself — always enter the room with a list of questions\\npre-planned — because I don’t want to count on coming up with',\n",
              " \"ve questions planned out and assigned\\nto each interviewer ahead of time.\\nI do this myself — always enter the room with a list of questions\\npre-planned — because I don’t want to count on coming up with\\nthem on the Yy.\\nThe best part is that you can then iteratively reXne the questions\\nwith your team as you interview candidates for the position.\\nThis is one of the best ways for an organization to become really\\ngood at hiring: by iterating the questions, you’re reXning what\\nyour criteria are — and how you screen for those criteria.\\nFourth, pay attention to the little things during the interview\\nprocess.\\nYou see little hints of things in the interview process that blow\\nup to disasters of unimaginable proportions once the person is\\nonboard.\\nPerson never laughs? Probably hard to get along with.\\nPerson constantly interrupts? Egomaniac, run for the hills.\\nPerson claims to be good friends with someone you know but\\nHow to hire the best people you've ever worked with 77\\nthen doesn’t know what the \",\n",
              " \"erson constantly interrupts? Egomaniac, run for the hills.\\nPerson claims to be good friends with someone you know but\\nHow to hire the best people you've ever worked with 77\\nthen doesn’t know what the friend is currently doing? Bullshitter.\\nPerson gives nonlinear answers to simple questions? Complete\\ndisorganized and undisciplined on the job.\\nPerson drones on and on? Get ready for hell.\\nFiah, pay attention to the little things during the reference\\ncalls.\\n(You are doing reference calls, right?)\\nMost people soaball deXciencies in people they’ve worked with\\nwhen they do reference calls.\\n“He’s great, super-smart, blah blah blah, but…”\\n“Sometimes wasn’t that motivated” — the person is a slug,\\nyou’re going to have to kick their rear every morning to get\\nthem to do anything.\\n“Could sometimes be a little hard to get along with” — hugely\\nunpleasant.\\n“Had an easier time working with men than women” — raging\\nsexist.\\n“Was sometimes a little moody” — suWering from clinical\\ndepression, and unmedicate\",\n",
              " 'little hard to get along with” — hugely\\nunpleasant.\\n“Had an easier time working with men than women” — raging\\nsexist.\\n“Was sometimes a little moody” — suWering from clinical\\ndepression, and unmedicated.\\nYou get the picture.\\nSixth, Hx your mistakes fast… but not too fast.\\nIf you are super-scrupulous about your hiring process, you’ll\\nstill have maybe a 70% success rate of a new person really working out — if you’re lucky.\\nAnd that’s for individual contributors.\\nIf you’re hiring executives, you’ll probably only have a 50% success rate.\\n78 The Pmarca Blog Archives\\nThat’s life.\\nAnyone who tells you otherwise is hiring poorly and doesn’t\\nrealize it.\\nMost startups in my experience are undisciplined at Xxing hiring mistakes — i.e., Xring people who aren’t working out.\\nFirst, realize that while you’re going to hate Xring someone,\\nyou’re going to feel way better aaer the fact than you can currently imagine.\\nSecond, realize that the great people on your team will be happy\\nthat you’ve done it — th',\n",
              " '’re going to hate Xring someone,\\nyou’re going to feel way better aaer the fact than you can currently imagine.\\nSecond, realize that the great people on your team will be happy\\nthat you’ve done it — they knew the person wasn’t working out,\\nand they want to work with other great people, and so they’ll\\nbe happy that you’ve done the right thing and kept the average\\nhigh.\\n(The reason I say “not too fast” is because your great people are watching to see how you Xre people, and if you do it too\\nfast you’ll be viewed as arbitrary and capricious — but trust me,\\nmost startup managers do not have this problem, they have the\\nopposite problem.)\\nThird, realize that you’re usually doing the person you’re Xring\\na favor — you’re releasing them from a role where they aren’t\\ngoing to succeed or get promoted or be valued, and you’re giving them the opportunity to Xnd a better role in a diWerent company where they very well might be an incredible star.\\n(And if they can’t, were they really the kind of perso',\n",
              " \"or be valued, and you’re giving them the opportunity to Xnd a better role in a diWerent company where they very well might be an incredible star.\\n(And if they can’t, were they really the kind of person you\\nwanted to hire in the Xrst place?)\\nOne of the good things about our industry is that there are\\nfrequently lots of new jobs being created and so you’re almost\\nnever pushing someone out onto the street — so don’t feel that\\nyou’re dooming their families to the poorhouse, because you\\naren’t.\\nYou’re not that important in their lives.\\nHow to hire the best people you've ever worked with 79\\nI can name a number of people I’ve Xred or participated in Xring who have gone on to be quite successful at other companies.\\nThey won’t necessarily talk to me anymore, though :-).\\nFinally, although this goes without saying: value the hell out of\\nthe great people you do have on your team. Given all of the\\nabove, they are incredibly special people.\\n80 The Pmarca Blog Archives\\nThe Pmarca Guide to Big\\nCompani\",\n",
              " 'without saying: value the hell out of\\nthe great people you do have on your team. Given all of the\\nabove, they are incredibly special people.\\n80 The Pmarca Blog Archives\\nThe Pmarca Guide to Big\\nCompanies\\nPart 1: Turnaround!\\nSo you’ve been hired/promoted/brought out of retirement to\\nbecome CEO of and turn around your NASDAQ/NYSE/LSElisted 5,000+ employee soMware/semiconductor/media company\\nthat’s recently been getting trounced by competitors, brutalized\\nby the press, and savaged in the stock market.\\nHere’s your turnaround plan in 9 easy steps.\\nStep 1: Go dark and execute.\\nYour predecessor in the CEO job inevitably spent way too much\\ntime explaining to reporters, investors, analysts, and anyone else\\nwho would listen why your company was actually doing just Xne\\nand how brighter times were just around the corner as your\\ncompetitive position deteriorated and your Xnancial results fell\\napart, and nobody believed it anyway.\\nMoney talks, hype walks — when you’re hitting your numbers,\\neveryone t',\n",
              " ' around the corner as your\\ncompetitive position deteriorated and your Xnancial results fell\\napart, and nobody believed it anyway.\\nMoney talks, hype walks — when you’re hitting your numbers,\\neveryone thinks you’re a genius and believes everything you\\nsay, no matter how silly. When you’re not hitting your numbers,\\neveryone thinks you’re a moron and won’t believe anything you\\nsay, no matter how true.\\nSo go dark, focus on the business, and don’t talk publicly for at\\nleast six months.\\nStep 2: But Frst, throw your predecessor completely under the bus.\\nCan’t forget this one! Tell Wall Street that your predecessor\\nwas such an incredibly dim bulb that in retrospect you can’t\\neven understand how he got past security and into the building,\\nmuch less was picked to be CEO. He completely fouled the\\nXnancials and sabotaged the business and as a result, earnings\\nfor the next several quarters are going to come in way below\\nexpectations.\\nThe fun part about this one is that your stock won’t even drop\\nbec',\n",
              " 'cials and sabotaged the business and as a result, earnings\\nfor the next several quarters are going to come in way below\\nexpectations.\\nThe fun part about this one is that your stock won’t even drop\\nbecause everyone has already Xgured that out.\\nStep 3: Identify the 3-5 things that are working surprisingly well in\\nyour business, and double down on those.\\nAny big company, no matter how moribund and poorly run, has\\na number of products and projects that are going better than\\nexpected — and usually come as a complete surprise.\\nDrawing on Peter Drucker’s classic admonition to “focus on\\nopportunities, not problems”, Xgure out what these surprise successes are and double down on them.\\nPromote their general managers, elevate their business units in\\nthe organization, give them more funding, and get out of the\\nway.\\nStep 4: Identify the 3-5 things that are consuming a lot of money and\\ntime and yet going nowhere, and kill those.\\nA good starting point is your predecessor’s pet projects — line\\n‘em up ',\n",
              " 'ut of the\\nway.\\nStep 4: Identify the 3-5 things that are consuming a lot of money and\\ntime and yet going nowhere, and kill those.\\nA good starting point is your predecessor’s pet projects — line\\n‘em up and shoot ‘em.\\nFrankly, they don’t even have to be consuming that much\\nmoney. They’re almost certainly consuming time and management bandwidth, and they need to go.\\nYou can also consider this a warmup exercise for Step 5.\\nStep 5: Lay oE a third of the workforce.\\nHere’s why:\\nHistory shows that you’re going to have to ultimately do it anyPart 1: Turnaround! 83\\nway, either via death of a thousand cuts (or six to eight distinct\\nrounds of layoWs), or all at once.\\nSo do it all at once.\\nA company that requires a turnaround has, in all likelihood, hired\\ntoo many people for the size of the business opportunity it actually\\nhas. This impairs proXtability, driving away investors and submergingthestockpriceatpreciselythetimethecompanyneedsahealthy\\nacquisition currency; this demotivates your great peopl',\n",
              " 'ortunity it actually\\nhas. This impairs proXtability, driving away investors and submergingthestockpriceatpreciselythetimethecompanyneedsahealthy\\nacquisition currency; this demotivates your great people by surrounding them by too many mediocre people and too much\\nbureaucracy; and this slows everything in your company to a crawl\\nbecause there are simply too many people running around who have to talk\\nabout everything before anything gets done.\\nGrit your teeth, oWer the most generous severance and assistance packages you possibly can, and get it done.\\nYour ability to continue to employ the other two-thirds of your\\npeople is at stake.\\nStep 6: Reduce layers, then promote up and comers and put them\\nclearly in charge.\\nA company that requires a turnaround has, in all likelihood, too\\nmany layers of management. Nuke as many of them as you can.\\nThen develop a list of your top 20 or 30 up and comers —\\nstrong, sharp, aggressive, ambitious director- or VP-level managers who want to succeed and want ',\n",
              " ' management. Nuke as many of them as you can.\\nThen develop a list of your top 20 or 30 up and comers —\\nstrong, sharp, aggressive, ambitious director- or VP-level managers who want to succeed and want your company to succeed.\\nAnd promote them, and put them in sole charge of clearly identiXed teams and missions. (And give them big ol’ fresh option\\npackages.)\\nAs CEO, you should only have at most one executive between\\nyou and these 20 or 30 up and comers once you are done promoting them and putting them in charge of their teams and\\nmissions.\\nIf you don’t know who those top 20 or 30 up and comers are, if\\nyou don’t promote them, if you don’t put them clearly in charge\\nof the things that matter, or if you have more than one layer of\\n84 The Pmarca Blog Archives\\nmanagement between you and them when you’re done, you’re\\nprobably doomed.\\nStep 7: Figure out the single most important thing your company has\\nto win at, and put your single best person in charge of winning at it.\\n‘NuW said.\\nStep 8: Look',\n",
              " ' you’re done, you’re\\nprobably doomed.\\nStep 7: Figure out the single most important thing your company has\\nto win at, and put your single best person in charge of winning at it.\\n‘NuW said.\\nStep 8: Look at the market, Fgure out 3-5 new areas in which your\\ncompany is not currently playing or winning, but are clearly going to\\ngrow a lot — and acquire the best company in each of those areas.\\nHere you’re looking for growth — for products, trends, perhaps\\nphenomena outside but adjacent to your current products and\\nmarkets, that are going to grow a lot in the next few years.\\nYou have to acquire, because if you’re in a turnaround situation,\\nyou aren’t going to have the time or bandwidth to build them\\nin-house — unless you’re the very rare exception.\\nWhen you do acquire, you’re going to have to pay up, because\\nnew things that are growing really fast in growth markets are\\nalways expensive — whether private or public — especially\\ncompared to the PE multiple of a big company in turnaround.\\nSo here’',\n",
              " 'up, because\\nnew things that are growing really fast in growth markets are\\nalways expensive — whether private or public — especially\\ncompared to the PE multiple of a big company in turnaround.\\nSo here’s hoping you did a great job at Step 5.\\nStep 9: In six months, relaunch the company with a single, crisp,\\ncoherent message and strategy.\\nThen go dark again and go right back to work.\\nOf course, there’s more to being the CEO of a turnaround than\\nthese 9 steps. There are a thousand other things you’re going to\\nhave to do. But these are the 9 most important.\\nTo quote the great Tommy Lasorda: “This fucking job ain’t that\\nfucking easy.”\\nAppendix for media companies only:\\nStep 10: For God’s sake, stop suing your customers.\\nPart 1: Turnaround! 85\\nPart 2: Retaining great people\\nThis post is about retaining great people, particularly at big companies in industries like technology, where stock options matter\\nand where people can relatively easily move from one company\\nto another.\\nActually, I lied. T',\n",
              " 'ining great people, particularly at big companies in industries like technology, where stock options matter\\nand where people can relatively easily move from one company\\nto another.\\nActually, I lied. This post isn’t really about retention at all.\\nIt’s about winning.\\nLet me explain:\\nCompanies that are winning — even really big, old ones — never\\nhave a retention problem. Everyone wants to stay, and when someone does leave, it’s really easy to get someone great to take her\\nplace.\\nCompanies that have a retention problem usually have a winning problem. Or rather, a “not winning” problem.\\nThe typical case is a company that used to be a hot growth company, but the growth has Yattened out, causing the stock to tank\\nand everyone to be in a bad mood. Or, alternately, an older\\nbig company that did really well for a while but more recently\\nhasn’t been doing so well, causing the stock to tank and everyone to be in a bad mood.\\nIn other words, a company in transition — from winning at one\\npoint, to no',\n",
              " 'lly well for a while but more recently\\nhasn’t been doing so well, causing the stock to tank and everyone to be in a bad mood.\\nIn other words, a company in transition — from winning at one\\npoint, to not winning now.\\nThe only way a company in that situation can retain great people is to start winning again.\\nGreat people want to work at a winner.\\nAll the raises, perks, and HR-sponsored “company values” draaing sessions in the world won’t help you retain great people if\\nyou’re not winning — not even the $6,000 heated Japanese toilets in all the restrooms, the $30,000 Olympic lap pool out back,\\nand the free $4 bottles of organic orange juice in all the snack\\nrooms.\\nThis seems deeply unfair when you’re going through it, because\\nwhen you’re not winning, that’s exactly when you need all those\\ngreat people the most!\\nOh well. That’s the price we pay for living in a society where\\njobs aren’t for life anymore.\\nSo the right question is, how can we start\\nwinning again?\\nIn this post, I’m going to pun',\n",
              " 't people the most!\\nOh well. That’s the price we pay for living in a society where\\njobs aren’t for life anymore.\\nSo the right question is, how can we start\\nwinning again?\\nIn this post, I’m going to punt on large parts of the answer to\\nthat question, as follows:\\n• I already discussed the big company turnaround scenarioin\\nmy last post in this series, which describes how to take a big\\ncompany that is no longer winning and set it up to win again.\\n• A company that was a hot startup a few years ago and grew\\nfast but is now seeing growth slow has a slightly diWerent\\nproblem — your original product cycle has peaked and you\\nneed to Xnd a new product cycle. That’s your big challenge,\\nway beyond retention. But I’ll talk about that in a future post.\\nRetention follows from the steps you are taking to orchestrate\\nthe turnaround and/or get to the next big product cycle.\\nHaving adroitly sidestepped most of what you need to do, let\\nme now address some things you can do to help the retention\\nPart 2: Reta',\n",
              " 'chestrate\\nthe turnaround and/or get to the next big product cycle.\\nHaving adroitly sidestepped most of what you need to do, let\\nme now address some things you can do to help the retention\\nPart 2: Retaining great people 87\\nsituation while you are addressing the deeper issues required to\\nwin again.\\nFirst, don’t give up. I am particularly talking about the former hot\\nstartup that is now a large slow-growth company. It is easy to\\nsay, well, we’re not a startup anymore, we’re not a growth company anymore — now we’re a big company, and we need to\\nchange our culture and our methods to attract and retain the\\nkinds of people who like to work at big companies.\\nDoing that will make your situation far worse, by causing the\\nremaining great people you do have to abandon ship even\\nfaster. Who wants to work for a company that has given up on\\nhaving energy and drive and ambition? And then you will end\\nup with a staW that only knows how to be a big company —\\nthat only knows how to maintain something tha',\n",
              " 'rk for a company that has given up on\\nhaving energy and drive and ambition? And then you will end\\nup with a staW that only knows how to be a big company —\\nthat only knows how to maintain something that someone else\\nhas already built — which is death in any industry where things\\nchange all the time.\\nSecond, focus. In a technology company, focus on retaining the\\ngreat architects and managers. In other kinds of companies,\\nfocus on retaining the equivalent people — the people who are\\nthe magnets for retaining other great people and hiring more\\ngreat people.\\nYou have to retain the magnets — or at least a critical mass of\\nthem — because without them, you’re going to lose everyone\\nelse.\\nIf you bear down and focus on retaining the magnets, retaining\\neveryone else — for example, in a soaware company, the junior\\nprogrammers, the product managers, the user interface designers, the salespeople, the sales engineers, the marketing staW, and\\nso on — will be much easier.\\nThird, clean house. Any compan',\n",
              " 'y, the junior\\nprogrammers, the product managers, the user interface designers, the salespeople, the sales engineers, the marketing staW, and\\nso on — will be much easier.\\nThird, clean house. Any company with a retention problem probably also has an overstaZng problem and a mediocrity problem\\nand needs to Xx both of those at the same time.\\nIdentify and eliminate the jobs of the following categories of\\npeople:\\n88 The Pmarca Blog Archives\\n• People who are “vesting in peace” — so called “VIPs”. VIPs are\\na particular problem at the former hot startup that has\\nplateaued. They suck the life out of their environment and\\nhave to go.\\n• “Summertime soldiers” — people who only joined in the Xrst\\nplace because you were already successful and have no\\ninterest in really bearing down and applying themselves to a\\nchallenge. Again, a classic problem for the former hot\\nstartup. Look particularly hard at the people who joined in\\nthe two years following the IPO — some of them are\\nundoubtedly very hard worki',\n",
              " 's to a\\nchallenge. Again, a classic problem for the former hot\\nstartup. Look particularly hard at the people who joined in\\nthe two years following the IPO — some of them are\\nundoubtedly very hard working, but others are summertime\\nsoldiers and have to go.\\n• Mediocre performers — every company has some, unless\\nyou have been routinely Xring your bottom 10% every year,\\nand even then you probably have some.\\nTaking out the people who fall into these categories will make\\nyour remaining great people feel better immediately, and will\\nsave you a lot of money and stock options that you can reallocate to better purposes — such as new compensation packages\\nfor your remaining great people.\\nFourth, promote your best people — especially into the jobs vacated\\nby the more senior of the people you just Xred — and give them\\nvery interesting challenges.\\nThat is so fundamental that I’m not even going to discuss it further here.\\nFiMh, simplify and clarify your organizational structure to make sure\\nthat your ',\n",
              " '— and give them\\nvery interesting challenges.\\nThat is so fundamental that I’m not even going to discuss it further here.\\nFiMh, simplify and clarify your organizational structure to make sure\\nthat your best people have direct responsibility for their projects.\\nCompanies coming oW a period of signiXcant success usually\\nhave grown a lot and/or bought a lot of other companies, and\\ntypically have messy or overly complex org structures. This is a\\ngreat opportunity to clean that up — and in particular, to move\\nto an organizational model where each of your stars has clear,\\ndirect, and comprehensive responsibility for a critical mission.\\nPart 2: Retaining great people 89\\nNuke all matrices. Nuke all dual reporting structures. And nuke\\nas many shared services functions as you possibly can.\\nFor example, in a soaware company, break up the centralized\\ndocumentation group, QA group, build group, etc. and disperse\\nthose people into the individual product divisions. Give your\\nproduct division heads comp',\n",
              " ', in a soaware company, break up the centralized\\ndocumentation group, QA group, build group, etc. and disperse\\nthose people into the individual product divisions. Give your\\nproduct division heads complete responsibility for everything\\nthey need to ship great products — except for sales. And then in\\nsales, give your territory heads everything they need to kill their\\nnumbers.\\nA great general rule of thumb for this kind of organizational\\nredesign is that you want to tolerate overlap. So each product\\ndivision has its own QA team — so what? Your division heads —\\nwho are now your best people — will be able to move so much\\nfaster that way that it’s worth it. Plus, you saved so much money\\ntaking out the VIPs, summertime soldiers, and mediocre people\\nthat you’re still ahead on headcount expense.\\nRemember, it’s generally a good idea, once you do all of this\\nrestructuring, to end up with smaller team sizes than you had\\nbefore. By reducing the size of a team, and increasing the average quality lev',\n",
              " 'member, it’s generally a good idea, once you do all of this\\nrestructuring, to end up with smaller team sizes than you had\\nbefore. By reducing the size of a team, and increasing the average quality level within the team, you will usually speed things\\nup, while saving money.\\nFinally, be sure to take out layers — especially at the top of the\\ncompany. The best people who are now running all the key projects and divisions should be no more than one layer away from\\nthe CEO, and usually that means you can take out at least one\\nlayer, maybe two, and (shudder) maybe even more.\\nSixth, put the recruiters to work, aggressively — but don’t rely on them\\nfor everything.\\nNotably, for the really critical open jobs, go out and recruit the\\nright person yourself, or better yet promote from within.\\nAnd here’s a neat trick that actually works. Go out and re-recruit\\nthe best people who already lea. Some of them have since\\ndiscovered that the grass isn’t actually greener at whatever\\nmediocre startup they join',\n",
              " 'a neat trick that actually works. Go out and re-recruit\\nthe best people who already lea. Some of them have since\\ndiscovered that the grass isn’t actually greener at whatever\\nmediocre startup they joined or whatever other big company\\n90 The Pmarca Blog Archives\\nthey jumped to. Give them fat packages against the new mission\\nand get them back.\\nSeventh, ramp up college recruiting. This will be very important for\\nyou over the next couple years. College recruiting is the best\\nway to get a bunch of new Xred up people into your company\\nwho are hungry and who don’t realize quite how badly you suck.\\n(That was a joke.)\\nEighth, communicate within — tell everyone in your company\\nclearly and unambiguously, we are here to win and here’s how\\nwe’re going to do it. It won’t be easy, but we can do it and we will do\\nit, and we will have amazing stories to tell our grandchildren.\\nYou don’t need to be certain of all the answers! Colin Powell\\nsays, “You know you’re a good leader when people follow you, if\\non',\n",
              " 'will do\\nit, and we will have amazing stories to tell our grandchildren.\\nYou don’t need to be certain of all the answers! Colin Powell\\nsays, “You know you’re a good leader when people follow you, if\\nonly out of curiosity.” So project boldness, and have that glint in\\nyour eye where people know you’re up to something big.\\nNinth, shake things up. Directly on Powell’s curiosity point —\\nchange the story to something new. Overhaul the organization,\\nmove people around, Xre people, promote other people, cancel\\nproducts, double down on other products, do some acquisitions,\\ncut some big deals, do some spinoWs, whatever — but change\\nthe story. Reintroduce curiosity.\\nWhen all else fails, do a “shake and bake” — do a big transformative deal that you’re not sure will work but which you think\\nhas a real shot. That will at the very least inject energy back into\\nthe situation.\\nI am being deliberately cavalier about this tactic, especially the\\n“shake and bake” part. You can easily destroy your company\\nwi',\n",
              " 'hot. That will at the very least inject energy back into\\nthe situation.\\nI am being deliberately cavalier about this tactic, especially the\\n“shake and bake” part. You can easily destroy your company\\nwith this kind of move.\\nBut — and this is a very important but — a company in crisis\\noaen has a severe narrative or “story” problem that accompanies its business problems, and it can be hard to get people\\ninside and outside the company motivated to reengage without\\nyou forcing a dramatic change to the story in some fundamental\\nway.\\nPart 2: Retaining great people 91\\nStories don’t change by themselves. Change the story.\\nNext topic: how to talk a great person out\\nof going to a startup\\nI’m assuming based on all of the above that if you do the blocking and tackling right, you’re going to be able to convince a lot\\nof your great people to not go to a diWerent big company.\\nTalking someone out of going to a startup is a separate challenge.\\nSomeone usually wants to move from a big company to a\\nstartup',\n",
              " 'onvince a lot\\nof your great people to not go to a diWerent big company.\\nTalking someone out of going to a startup is a separate challenge.\\nSomeone usually wants to move from a big company to a\\nstartup for one or more of the following four reasons:\\nFirst, she wants to build a new company instead of being a caretaker in\\nsomeone else’s company.\\nYou can talk someone out of this if you can show her how, if\\nshe stays with you for two more years, you are going to concretely better train and prepare her to kick butt at a startup when\\nshe does make such a move — particularly if she is on the management track.\\nGive her a promotion, a big new job with a big new challenge,\\nand clear responsibility. And tell her that if she kicks butt at this\\nmore signiXcant challenge, she’ll not only be better prepared to\\nbe out on her own, but you will personally give her glowing recommendations and help her Xnd the perfect startup or the perfect VC for her — in two years.\\nThen, two years later, you can do the sa',\n",
              " 'd to\\nbe out on her own, but you will personally give her glowing recommendations and help her Xnd the perfect startup or the perfect VC for her — in two years.\\nThen, two years later, you can do the same thing again.\\nI believe you are doing someone a huge favor when you do this.\\nMost startups aren’t very good and have no prospect of real success, and most big company people aren’t very good at picking\\n‘em, although they never believe that. And by staying, your great\\nperson is gaining incredibly valuable experience by taking on\\nnew challenges and new responsibilities at your company that\\nwill help her succeed and Yourish down the road wherever she\\nultimately decides to go.\\n92 The Pmarca Blog Archives\\nIt’s not that unusual to see a young superstar division head or\\nsenior vice president at a big company who has been promoted\\nrapidly over the last several years who has also been periodically\\non the verge of going to a startup and stayed each time for a new\\nchallenge instead. And there she i',\n",
              " 'ig company who has been promoted\\nrapidly over the last several years who has also been periodically\\non the verge of going to a startup and stayed each time for a new\\nchallenge instead. And there she is, running 100 or 500 or 2,000\\npeople and doing incredibly well in her career. Win/win.\\nNow, if you’re not willing to promote ‘em, that’s another story.\\nSecond, she has a killer idea for a startup, or has fallen in love with a\\nstartup’s killer idea.\\nIn this case, you’re probably better oW letting her go to the\\nstartup.\\nThis isn’t the case nearly as oaen as you’d think. Some of the\\nstartups I’ve seen great people join — including very recently —\\nboggle my brain at how bad the ideas were.\\nThird, she wants the Fnancial upside of a startup.\\nVisions of being the next Larry Page (Larriett Page?) are dancing\\nin her head.\\nYou can oaen defeat this by simply explaining the realities of\\nthe compensation package she’s being oWered.\\nExplain to her how her options will likely be worthless when\\nthe start',\n",
              " ' dancing\\nin her head.\\nYou can oaen defeat this by simply explaining the realities of\\nthe compensation package she’s being oWered.\\nExplain to her how her options will likely be worthless when\\nthe startup fails, how small a percentage of the company she’s\\nactually being oWered, how much she’s going to be diluted by\\nfuture Xnancing rounds, how far below market her package is\\noverall, and how bad the medical and dental beneXts are for her\\nkids. And if she hasn’t changed her mind by that point, tell her a\\ncramdown round story.\\nFourth, she can’t function in a large environment. It’s too frustrating,\\ntoo boring, too many rules, too much management, whatever.\\nIn this case, you are probably also better oW letting her go to the\\nstartup.\\nThings not to do when trying to retain great people:\\nPart 2: Retaining great people 93\\nNow we’re getting into personal opinion, but for what it’s\\nworth…\\nDon’t create a new group or organization within your company whose\\njob is “innovation”. This takes various for',\n",
              " 'taining great people 93\\nNow we’re getting into personal opinion, but for what it’s\\nworth…\\nDon’t create a new group or organization within your company whose\\njob is “innovation”. This takes various forms, but it happens reasonably oaen when a big company gets into product trouble,\\nand it’s hugely damaging.\\nHere’s why:\\nFirst, you send the terrible message to the rest of the organization that they’re not supposed to innovate.\\nSecond, you send the terrible message to the rest of the organization that you think they’re the B team.\\nThat’s a one-two punch that will seriously screw things up.\\nInstead, focus on boosting the innovation culture of the entire\\ncompany.\\nDon’t do arbitrary large spot bonuses or restricted stock grants to try to\\ngive a small number of people huge Fnancial upside.\\nAn example is the Google Founders’ Awards program, which\\nGoogle has largely stopped, and which didn’t work anyway.\\nIt sounds like a great idea at the time, but it causes a severe\\nbacklash among both the norma',\n",
              " 'le is the Google Founders’ Awards program, which\\nGoogle has largely stopped, and which didn’t work anyway.\\nIt sounds like a great idea at the time, but it causes a severe\\nbacklash among both the normal people who don’t get it (who\\nfeel like they’re the B team) and the great people who don’t get it\\n(who feel like they’ve been screwed).\\nClosing thought\\nIn general, the intangibles that keep great people are: the quality\\nof the people they’re working with, the interestingness level of\\ntheir projects, and whether they are learning and growing.\\nThe tangibles are: winning, and a high stock price.\\nAs the leader, you have to really believe that you can get your company back to winning andtherefore back to ahighstockprice.\\n94 The Pmarca Blog Archives\\nIf not, you should sell the company.\\nPart 2: Retaining great people 95\\nThe Pmarca Guide to\\nCareer, Productivity, and\\nSome Other Things\\nIntroduction\\nIn real life — as opposed to blogging — one of my least favorite\\nthings to do is give career planning',\n",
              " 'ining great people 95\\nThe Pmarca Guide to\\nCareer, Productivity, and\\nSome Other Things\\nIntroduction\\nIn real life — as opposed to blogging — one of my least favorite\\nthings to do is give career planning advice. Most people who\\nsay they want career planning advice aren’t actually looking for\\nadvice — they just want validation of the path they have already\\nchosen. Because of that, giving someone career planning advice\\nis one of the surest ways to end up feeling like an a******.\\nHowever, as with so many other things, career planning is a\\ntopic about which I have plenty of opinions. And since I started\\nthis blog, I’ve received a lot of questions from people who are\\nlooking for career planning advice. So, this series of posts will\\npresent my opinions on career planning in today’s world.\\nDisclaimers:\\n• These posts are aimed at high-potential people who want to\\nexcel throughout their careers and make a signiXcant impact\\non their Xelds and the world. These posts are not appropriate\\nfor people fo',\n",
              " 'rs:\\n• These posts are aimed at high-potential people who want to\\nexcel throughout their careers and make a signiXcant impact\\non their Xelds and the world. These posts are not appropriate\\nfor people for whom work/life balance is a high priority or\\nfor whom lifestyle is particularly important — if that’s you,\\nthere are plenty of existing career planning resources for you\\nalready!\\n• My background is biased towards high-tech companies and\\nSilicon Valley, and my advice will be most relevant to people\\nentering either my industry or other industries that are like\\nmy industry — fast moving, rapidly changing, and\\ncharacterized by lots of new companies and lots of\\nopportunity for new people. Some of this advice may be\\napplicable to people entering other kinds of Xelds — but I\\nwouldn’t know and I won’t guarantee it.\\n• I’ll use a lot of words like “ambition” and “promotion” and\\n“gaining more responsibility”. It may seem like I’m talking\\nabout moving up through the management ranks and\\nmanaging mor',\n",
              " '’t guarantee it.\\n• I’ll use a lot of words like “ambition” and “promotion” and\\n“gaining more responsibility”. It may seem like I’m talking\\nabout moving up through the management ranks and\\nmanaging more and more people, but my intention is for all\\nof this advice to be equally relevant to people in purely\\ntechnical careers, such as soaware programming. If you\\naren’t interesting in managing people, then when I talk about\\npromotion and advancement, just think about getting\\nbroader latitude to work on or lead more complex technical\\nprojects, assuming more technical seniority within a\\ncompany, and the like.\\n• Everything that follows is purely personal opinion —\\nspeciXcally, these are the things I would want to know if I\\nwere entering college today. I’m sure there are many equally\\nvalid counterpoints to each of my points, and I look forward\\nto reading them on other people’s blogs!\\n98 The Pmarca Blog Archives\\nPart 1: Opportunity\\nThe Hrst rule of career planning: Do not plan your career.\\nThe wo',\n",
              " 'ints to each of my points, and I look forward\\nto reading them on other people’s blogs!\\n98 The Pmarca Blog Archives\\nPart 1: Opportunity\\nThe Hrst rule of career planning: Do not plan your career.\\nThe world is an incredibly complex place and everything is\\nchanging all the time. You can’t plan your career because you have\\nno idea what’s going to happen in the future. You have no idea what\\nindustries you’ll enter, what companies you’ll work for, what\\nroles you’ll have, where you’ll live, or what you will ultimately\\ncontribute to the world. You’ll change, industries will change,\\nthe world will change, and you can’t possibly predict any of it.\\nTrying to plan your career is an exercise in futility that will only\\nserve to frustrate you, and to blind you to the really signiXcant\\nopportunities that life will throw your way.\\nCareer planning = career limiting.\\nThe sooner you come to grips with that, the better.\\nThe second rule of career planning: Instead of planning your\\ncareer, focus on developing',\n",
              " 'ife will throw your way.\\nCareer planning = career limiting.\\nThe sooner you come to grips with that, the better.\\nThe second rule of career planning: Instead of planning your\\ncareer, focus on developing skills and pursuing opportunities.\\nI’ll talk a lot about skills development in the next post. But for\\nthe rest of this post, I’m going to focus in on the nature of opportunities.\\nOpportunities are key. I would argue that opportunities fall loosely\\ninto two buckets: those that present themselves to you, and\\nthose that you go out and create. Both will be hugely important\\nto your career.\\nOpportunities that present themselves to you are the consequence — at least partially — of being in the right place at the\\nright time. They tend to present themselves when you’re not\\nexpecting it — and oaen when you are engaged in other activities\\nthat would seem to preclude you from pursuing them. And\\nthey come and go quickly — if you don’t jump all over an opportunity, someone else generally will and it wi',\n",
              " ' you are engaged in other activities\\nthat would seem to preclude you from pursuing them. And\\nthey come and go quickly — if you don’t jump all over an opportunity, someone else generally will and it will vanish.\\nI believe a huge part of what people would like to refer to as\\n“career planning” is being continuously alert to opportunities\\nthat present themselves to you spontaneously, when you happen to be in the right place at the right time.\\n• A senior person at your Xrm is looking for someone young\\nand hungry to do the legwork on an important project, in\\naddition to your day job.\\n• Your former manager has jumped ship to a hot growth\\ncompany and calls you three months later and says, come\\njoin me.\\n• Or, a small group of your smartest friends are headed to\\nDenny’s at 11PM to discuss an idea for a startup — would you\\nlike to come along?\\nI am continually amazed at the number of people who are presented with an opportunity like one of the above, and pass.\\nThere’s your basic dividing line betw',\n",
              " 'r a startup — would you\\nlike to come along?\\nI am continually amazed at the number of people who are presented with an opportunity like one of the above, and pass.\\nThere’s your basic dividing line between the people who shoot up in\\ntheir careers like a rocket ship, and those who don’t — right there.\\nThe second bucket of opportunities are those you seek out and\\ncreate. A lot of what will follow in future posts in this series\\nwill discuss how to do that. However, let me say up front that I\\nam also continually amazed at the number of people who coast\\nthrough life and don’t go and seek out opportunities even when\\nthey know in their gut what they’d really like to do. Don’t be one\\nof those people. Life is way too short.\\nThe world is a very malleable place. If you know what you want,\\nand you go for it with maximum energy and drive and passion,\\n100 The Pmarca Blog Archives\\nthe world will oaen reconXgure itself around you much more\\nquickly and easily than you would think.\\nNow, I’m not proposing ',\n",
              " 'r it with maximum energy and drive and passion,\\n100 The Pmarca Blog Archives\\nthe world will oaen reconXgure itself around you much more\\nquickly and easily than you would think.\\nNow, I’m not proposing that you simply ping pong from\\nopportunity to opportunity randomly. You can have a strategy.\\nAnd here’s how I think that strategy should work.\\nPeople who manage money professionally don’t think about\\nindividual investments in isolation; they think of those investments as part of an overall portfolio. Each investment has its\\npotential return — the beneXt you get from owning it — and its\\npotential risks — the things that can go wrong. A portfolio, then,\\nis a set of investments, and the portfolio assumes the return and\\nrisk characteristics of all of the investments blended together.\\nViewed in that context, it is oaen logical to have individual\\ninvestments within a portfolio that are far more risky than one\\nwould normally Xnd comfortable — if the potential return is\\ngood enough. Or, investment',\n",
              " 't context, it is oaen logical to have individual\\ninvestments within a portfolio that are far more risky than one\\nwould normally Xnd comfortable — if the potential return is\\ngood enough. Or, investments that are far less risky and have far\\nless return potential than one would normally want — to protect one’s downside. The risk of any individual investment is not\\nimportant; what is important is how the risks — and the potential returns — of all of the investments combine in the overall\\nportfolio.\\nI believe you should look at your career as a portfolio of jobs/\\nroles/opportunities. Each job that you take, each role that you\\nchoose to Xll, each opportunity you pursue, will have a certain\\npotential return — the beneXts you can get from taking it,\\nwhether those beneXts come in the form of income, skill development, experience, geographic location, or something else.\\nEach job will also have a certain risk proXle — the things that\\ncould go wrong, from getting Xred for not being able to handle\\n',\n",
              " ', skill development, experience, geographic location, or something else.\\nEach job will also have a certain risk proXle — the things that\\ncould go wrong, from getting Xred for not being able to handle\\nthe job’s demands, to having to move somewhere you don’t\\nwant to, to the company going bankrupt, to the opportunity cost\\nof not pursuing some other attractive opportunity.\\nOnce you start thinking this way, you can think strategically\\nabout your career over its likely 50+ year timespan.\\nFor example, when you are just out of school and have a low\\nPart 1: Opportunity 101\\nburn rate and geographic Yexibility, you can take jobs with a\\ncertain return/risk proXle. If you get married and have kids, you\\nwill take jobs with a diWerent return/risk proXle. Later, when\\nyour kids grow up and you are once again free to move about\\nand you don’t have to worry about tuition payments and a\\nmortgage on a big house in a great school district, but you now\\nhave far more experience than you did when you were Xrst\\n',\n",
              " 'in free to move about\\nand you don’t have to worry about tuition payments and a\\nmortgage on a big house in a great school district, but you now\\nhave far more experience than you did when you were Xrst\\nstarting out, you can take jobs with a third return/risk proXle.\\nMost people do not think this way. They might occasionally\\nthink this way, but they don’t do it systematically. So when\\nan opportunity pops up, they evaluate it on a standalone basis\\n— “boy, it looks risky, I’m not sure I should do it”. What you\\nshould automatically do instead is put it in context with all of\\nthe other risks you are likely to take throughout your entire\\ncareer and decide whether this new opportunity Fts strategically into\\nyour portfolio.\\nLet’s dig into the concept of risk a little more.\\nI’m not talking about the form of risk that you think of when\\nyou think of stepping out into the crosswalk and getting run\\nover by an Escalade. I’m talking about the form of risk that\\nXnancial professionals deal with (see the ',\n",
              " 'form of risk that you think of when\\nyou think of stepping out into the crosswalk and getting run\\nover by an Escalade. I’m talking about the form of risk that\\nXnancial professionals deal with (see the classic book Against\\nthe Gods: The Remarkable Story of Risk for more on this), and\\nstartup entrepreneurs deal with, and that you deal with any time\\nyou make any decision. There are a set of potential downsides to\\nalmost any decision — but they can be analyzed, and oMen quantiFed,\\nand thereby brought under control.\\nWhich is important, because in life, there is generally no opportunity\\nwithout risk. Doing the legwork on that extra project for the\\nsenior person at your Xrm? You risk exhausting yourself and\\ndoing your day job poorly. Joining your former manager at\\nthat hot growth company? Maybe it tanks six months later and\\nthen your current employer won’t rehire you. Join those smart\\nfriends at Denny’s and start a new company with them? Maybe\\nit completely fails, and you have to explain why y',\n",
              " ' tanks six months later and\\nthen your current employer won’t rehire you. Join those smart\\nfriends at Denny’s and start a new company with them? Maybe\\nit completely fails, and you have to explain why you were so\\nfoolish at every job interview you do for the rest of your\\n102 The Pmarca Blog Archives\\nlife. There are always real and legitimate reasons why people oMen pass\\non opportunities — they see the risks and they wish to avoid them.\\nThe issue is that without taking risk, you can’t exploit any\\nopportunities. You can live a quiet and reasonably happy life,\\nbut you are unlikely to create something new, and you are\\nunlikely to make your mark on the world.\\nTo quote Aaron Brown — a legendary Morgan Stanley derivatives trader and poker expert — from his extraordinary\\nbook The Poker Face of Wall Street, when talking about hiring\\ntraders at an investment bank:\\nWhat I listen for is someone who really wanted something that\\ncould be obtained only through taking the risk, whether that risk\\nwas big',\n",
              " 'Street, when talking about hiring\\ntraders at an investment bank:\\nWhat I listen for is someone who really wanted something that\\ncould be obtained only through taking the risk, whether that risk\\nwas big or small.\\nIt’s not even important that she managed the risk skillfully; it’s only\\nimportant that she knew it was there, respected it, but took it anyway.\\nMost people wander through life, carelessly taking whatever risk\\ncrosses their path without compensation, but never consciously\\naccepting extra risk to pick up the money and other good things\\nlying all around them.\\nOther people reYexively avoid every risk or grab every loose dollar\\nwithout caution.\\nI don’t mean to belittle these strategies; I’m sure they make sense to\\nthe people who pursue them. I just don’t understand them myself.\\nI do know that none of these people will be successful traders.\\n…or successful at anything important in life.\\nSo, when you are presented with an opportunity, carefully\\nanalyze its risks, but:\\n• Do so within th',\n",
              " ' that none of these people will be successful traders.\\n…or successful at anything important in life.\\nSo, when you are presented with an opportunity, carefully\\nanalyze its risks, but:\\n• Do so within the context if your likely lifetime portfolio of\\nrisks…\\n• And do so realizing that taking risk can be a good thing when\\nit leads to pursuing the best opportunities.\\nPart 1: Opportunity 103\\nAll that said, here are some of my opinions about the kinds of\\nrisks you should take and when:\\n• When you are just out of school — and assuming that you are\\nrelatively free to move and have a low burn rate — is when\\nyou should optimize for the rate at which you can develop\\nskills and acquire experiences that will serve you well later.\\nYou should speciXcally take income risk in order to do that.\\nAlways take the job that will best develop your skills and give\\nyou valuable experiences, regardless of its salary. This is not\\nthe time in your career to play it safe.\\n• When you have family obligations — a spouse,',\n",
              " ' job that will best develop your skills and give\\nyou valuable experiences, regardless of its salary. This is not\\nthe time in your career to play it safe.\\n• When you have family obligations — a spouse, two and a half\\nkids, a dog, and a picket fence — that’s obviously the time to\\ncrank back on the income risk and instead take a little risk\\nthat you might not learn as much or advance as quickly or\\njoin that hot new startup. However, even this is not black and\\nwhite! In Silicon Valley, for example, it can still make a lot of\\nsense for a young parent to take a risk on a hot new startup\\nbecause it will usually be easy to get another job if the startup\\nfails — especially if one has developed more useful skills and\\nexperiences along the way.\\n• There may be times when you realize that you are\\ndissatisXed with your Xeld — you are working in enterprise\\nsoaware, for example, but you’d really rather be working on\\ngreen tech or in a consumer Internet company. Jumping\\nfrom one Xeld into another is al',\n",
              " 'tisXed with your Xeld — you are working in enterprise\\nsoaware, for example, but you’d really rather be working on\\ngreen tech or in a consumer Internet company. Jumping\\nfrom one Xeld into another is always risky because your\\nspeciXc skills and contacts are in your old Xeld, so you’ll have\\nless certainty of success in the new Xeld. This is almost always a\\nrisk worth taking– standing pat and being unhappy about it\\nhas risks of its own, particularly to your happiness. And it is\\nawfully hard to be highly successful in a job or Xeld in which\\none is unhappy.\\n• Likewise with geography risk. You start out in one city — say,\\nworking at a soaware company in Philadelphia — and you’re\\ndoing well. You get the opportunity to jump to a faster\\ngrowing soaware company in Silicon Valley. Should you take\\nthe risk of moving geographies — to a place where you don’t\\nknow anybody, and where the cost of living is higher? Almost\\n104 The Pmarca Blog Archives\\ncertainly — the additional risks of not having an exte',\n",
              " 'risk of moving geographies — to a place where you don’t\\nknow anybody, and where the cost of living is higher? Almost\\n104 The Pmarca Blog Archives\\ncertainly — the additional risks of not having an extensive\\npersonal network and of tolerating a lower standard of living\\nfor some period of time are almost certainly overcome by\\nthe upside of being at a better company, relocating yourself\\nto the heart of your industry, and setting yourself up to\\nexploit many more great opportunities over the next\\ndecades.\\n• Working for a big company is, I believe, much risker than it\\nlooks. I’ll talk about this more in the next post, but people\\nwho work at big companies are subject to impersonal layoWs\\nat any time, and oaen forego the opportunity to develop\\nskills and gain experiences as rapidly as they would at\\nsomeplace smaller and faster growing. And then Xve or ten\\nyears pass, and you realize your skills and experiences are\\nonly relevant for jobs at other big companies — and then you\\nhave a real problem.',\n",
              " '\\nsomeplace smaller and faster growing. And then Xve or ten\\nyears pass, and you realize your skills and experiences are\\nonly relevant for jobs at other big companies — and then you\\nhave a real problem.\\n• Finally, pay attention to opportunity cost at all times. Doing\\none thing means not doing other things. This is a form of risk\\nthat is very easy to ignore, to your detriment.\\nThose are just a few examples. You will run into speciXc return/\\nrisk situations that nobody can predict ahead of time. When you\\ndo, just sit down and tease apart the risks — and think hard about\\nwhether, in the context of your overall career portfolio, they are\\nreally so scary that they justify passing on the return potential\\nof a great opportunity. They oaen won’t.\\nOne more quote, this time from Nassim Nicholas Taleb in The\\nBlack Swan:\\nSeize any opportunity, or anything that looks like opportunity. They are rare, much rarer than you think…\\nMany people do not realize they are getting a lucky break in life\\nwhen they',\n",
              " 'in The\\nBlack Swan:\\nSeize any opportunity, or anything that looks like opportunity. They are rare, much rarer than you think…\\nMany people do not realize they are getting a lucky break in life\\nwhen they get it. If a big publisher (or a big art dealer or a movie\\nexecutive or a hotshot banker or a big thinker) suggests an appointment, cancel anything you have planned: you may not see such a\\nwindow open up again.\\nOf course, if you really are high-potential, you’re naturally\\ngoing to be seeking out risks in your career in order to maxPart 1: Opportunity 105\\nimize your level of achievement, so you’re thinking, c’mon,\\nAndreessen, get to the next point. For which, see the next post!\\n106 The Pmarca Blog Archives\\nPart 2: Skills and education\\n[Please read my opening disclaimers. Note especially that these\\nare only personal views; I am not trying to malign anyone else’s\\nchoice of career or education path. These are simply the\\nthings I would want to be told if I were entering college today.]\\nThis po',\n",
              " 'hese\\nare only personal views; I am not trying to malign anyone else’s\\nchoice of career or education path. These are simply the\\nthings I would want to be told if I were entering college today.]\\nThis post discusses skills acquisition throughout your lifetime,\\nincluding your formal education. So I will start with college and\\nmove on from there.\\nWhat should I study in college?\\nSome people argue that college will be your one chance in life\\nto pursue your passion — to spend four years doing nothing but\\nstudying whatever you love the most, whether that’s Renaissance literature or existential philosophy.\\nI disagree.\\nIf you intend to have an impact on the world, the faster you start\\ndeveloping concrete skills that will be useful in the real world, the\\nbetter — and your undergrad degree is a great place to start.\\nOnce you get into the real world and you’re primed for success, then you can pursue your passion.\\nA typical liberal arts degree will be almost useless on its own. So\\nyou usually won’t h',\n",
              " 'lace to start.\\nOnce you get into the real world and you’re primed for success, then you can pursue your passion.\\nA typical liberal arts degree will be almost useless on its own. So\\nyou usually won’t have the option of immediately entering the\\nworkforce in a high-impact way when you graduate, and you’ll\\nhave to go get a useful graduate degree.\\nAnd even if you are already planning to get a useful graduate\\ndegree, you are much better oW combining it with a substantive\\nundergraduate degree — thereby becoming a “double threat”.\\nMore on this in a bit.\\nWhich undergraduate degrees are useful in\\nthe real world?\\nTypically, those that have a technical element of some form —\\nthat teach you how to do something substantive.\\nEngineering degrees obviously qualify. The current myth that\\nengineering and computer science degrees are less useful\\nbecause all the jobs are going to India and China is silliness;\\nignore it.\\nHard science degrees — physics, chemistry — also clearly qualify, as do mathematics and',\n",
              " 'puter science degrees are less useful\\nbecause all the jobs are going to India and China is silliness;\\nignore it.\\nHard science degrees — physics, chemistry — also clearly qualify, as do mathematics and economics.\\nWhy do I take this stance?\\n• Technical degrees teach you how to do something diZcult\\nand useful that matters in the real world. Even if you don’t\\nend up actually doing what the degree teaches you how to do,\\ngoing through the experience of learning how to do it will\\nhelp you go through other serious learning experiences in\\nyour career. Complexity and diZculty will not faze you.\\n• Plus, technical degrees teach you how think like an engineer,\\na scientist, an economist, or a mathematician — how to use\\nreason, logic, and data. This is incredibly useful in the real\\nworld, which generally demands rigorous thinking on the\\npath to doing anything big.\\n• Plus, technical degrees indicate seriousness of purpose to\\nfuture employers and partners. You get coded right up front\\nas someone who is',\n",
              " ' demands rigorous thinking on the\\npath to doing anything big.\\n• Plus, technical degrees indicate seriousness of purpose to\\nfuture employers and partners. You get coded right up front\\nas someone who is intent on doing real things.\\nGraduating with a technical degree is like heading out into the real\\n108 The Pmarca Blog Archives\\nworld armed with an assault riGe instead of a dull knife. Don’t miss\\nthat opportunity because of some fuzzy romanticized view of\\nliberal arts broadening your horizons.\\nWhat graduate degrees are useful in the\\nreal world?\\nGenerally, if you have a useful undergrad degree, I think graduate degrees are overrated. You can usually hit the workforce in a\\nreal job with just an undergraduate degree and progress rapidly\\naccording to your own ability and energy from there.\\nOf course, you’re hearing this from someone who could barely\\nstand to stay in school long enough to Xnish undergrad, so take\\nthat for what it’s worth.\\nIf you don’t have a useful undergrad degree, then a use',\n",
              " 'rse, you’re hearing this from someone who could barely\\nstand to stay in school long enough to Xnish undergrad, so take\\nthat for what it’s worth.\\nIf you don’t have a useful undergrad degree, then a useful graduate degree is deFnitely a great idea. Business, math, economics,\\nscience — something practical, substantive.\\nQuite a few people in business have paired a liberal arts undergrad degree with an MBA. They seem to do just Xne. But I think\\nthat’s a missed opportunity — much better would be an MBA\\non top of an engineering or math undergraduate degree. People with that combination are invaluable, and there aren’t nearly\\nenough of them running around.\\nAs far as PhDs are concerned — some of my best friends have\\nPhDs. However, most of the people who have a huge impact\\non the world — outside of pure research and education — do\\nnot have PhDs. Draw from that whatever conclusion you think\\nmakes sense.\\nWhat college or university should I go to?\\nTry very very hard to go to one of the best college',\n",
              " 'pure research and education — do\\nnot have PhDs. Draw from that whatever conclusion you think\\nmakes sense.\\nWhat college or university should I go to?\\nTry very very hard to go to one of the best colleges or universities in the world for your chosen Xeld.\\nDon’t worry about being a small Fsh in a big pond — you want to\\nalways be in the best pond possible, because that’s how you will get\\nPart 2: Skills and education 109\\nexposed to the best people and the best opportunities in your\\nXeld.\\nIf you can’t start out in one of the top schools for your Xeld, then\\nwork your butt oW and get great grades and transfer as fast as\\nyou possibly can into a top school.\\nAnd if you can’t do that — if you end up getting your undergrad\\nat a school that’s not one of the top in your Xeld — then strongly\\nconsider pursuing a graduate degree in your Xeld at a great\\nschool for your Xeld.\\nIn this way, even if your only option is starting out at a community college, by the time you Xnish 4-6 years of education, you\\ncan ',\n",
              " 'ing a graduate degree in your Xeld at a great\\nschool for your Xeld.\\nIn this way, even if your only option is starting out at a community college, by the time you Xnish 4-6 years of education, you\\ncan vault yourself into the top tier of your Xeld.\\nWhat should I do while I’m in school?\\nI’m a huge fan of gaining practical experience in school by working during the school year, and then doing as many internships\\nand co-op programs as you can.\\nParticularly at research universities — where you want to be —\\nthere are lots of on-campus jobs that will give you highly valuable work experience. Take a job that will teach you something useful\\nand practical — the two obvious examples are working for a professor in your Xeld with an active research program who needs\\nundergrads to do some of the work, and being a staW member at\\na campus computer lab or research lab.\\nAnd then aggressively pursue internship and co-op programs — to\\nget real-world working experience at companies in your Xeld,\\nbefore you ',\n",
              " 'd being a staW member at\\na campus computer lab or research lab.\\nAnd then aggressively pursue internship and co-op programs — to\\nget real-world working experience at companies in your Xeld,\\nbefore you even graduate. Target the best companies in your\\nXeld, and go aaer the opportunities early and oaen.\\nIf you do this right, by the time you graduate even with just an\\nundergrad degree, you can have a year and a half of real working experience at high-quality companies plus another four\\nyears of practical experience from an on-campus job under\\nyour belt.\\n110 The Pmarca Blog Archives\\nPlus, you will be implicitly demonstrating to future employers how determined you are to succeed and how hard you are willing\\nto work.\\nIn contrast, almost any other way you can spend your time\\nwhile in school aside from getting reasonably good grades is a\\nmistake.\\nHow should I think about skills\\ndevelopment once I’m out of school?\\nYou should view graduating from school as just the beginning of\\nyour development of',\n",
              " 'rom getting reasonably good grades is a\\nmistake.\\nHow should I think about skills\\ndevelopment once I’m out of school?\\nYou should view graduating from school as just the beginning of\\nyour development of a whole portfolio of useful skills.\\nOne of the single best ways you can maximize the impact you\\nwill have on the world and the success you will have in your\\ncareer is by continuously developing and broadening your base of\\nskills.\\nMy favorite way of thinking about this is:\\nSeek to be a double/triple/quadruple threat.\\nScott Adams — the creator of Dilbert — nails it:\\nIf you want an average successful life, it doesn’t take much planning. Just stay out of trouble, go to school, and apply for jobs you\\nmight like. But if you want something extraordinary, you have two\\npaths:\\n• Become the best at one speciXc thing.\\n• Become very good (top 25%) at two or more things.\\nThe Xrst strategy is diZcult to the point of near impossibility. Few\\npeople will ever play in the NBA or make a platinum album. I don',\n",
              " 'speciXc thing.\\n• Become very good (top 25%) at two or more things.\\nThe Xrst strategy is diZcult to the point of near impossibility. Few\\npeople will ever play in the NBA or make a platinum album. I don’t\\nrecommend anyone even try.\\nThe second strategy is fairly easy. Everyone has at least a few areas\\nin which they could be in the top 25% with some eWort. In my case,\\nI can draw better than most people, but I’m hardly an artist. And\\nI’m not any funnier than the average standup comedian who never\\nmakes it big, but I’m funnier than most people. The magic is that\\nfew people can draw well and write jokes. It’s the combination of\\nthe two that makes what I do so rare. And when you add in my busiPart 2: Skills and education 111\\nness background, suddenly I had a topic that few cartoonists could\\nhope to understand without living it.\\n…Get a degree in business on top of your engineering degree, law\\ndegree, medical degree, science degree, or whatever. Suddenly\\nyou’re in charge, or maybe you’re startin',\n",
              " 'o understand without living it.\\n…Get a degree in business on top of your engineering degree, law\\ndegree, medical degree, science degree, or whatever. Suddenly\\nyou’re in charge, or maybe you’re starting your own company\\nusing your combined knowledge.\\nCapitalism rewards things that are both rare and valuable. You\\nmake yourself rare by combining two or more “pretty goods” until\\nno one else has your mix…\\nIt sounds like generic advice, but you’d be hard pressed to Xnd any\\nsuccessful person who didn’t have about three skills in the top 25%.\\nThe fact is, this is even the secret formula to becoming a CEO. All\\nsuccessful CEO’s are like this. They are almost never the best\\nproduct visionaries, or the best salespeople, or the best marketing people, or the best Xnance people, or even the best managers, but they are top 25% in some set of those skills, and\\nthen all of a sudden they’re qualiXed to actually run something\\nimportant.\\nYou can apply this principle to the degrees you can choose to\\nget in ',\n",
              " ', but they are top 25% in some set of those skills, and\\nthen all of a sudden they’re qualiXed to actually run something\\nimportant.\\nYou can apply this principle to the degrees you can choose to\\nget in school.\\nI already talked about combining an undergrad engineering\\ndegree with an MBA. I’ll hire as many of those people as I possibly can.\\nAn MBA plus a law degree can be a great combination — and\\nprobably far more useful than either of those degrees by themselves.\\nOr even combine two undergrad degrees — computer science\\nplus physics, say, or physics plus economics.\\nYou can also apply this principle to skills that you develop once\\nyou leave school.\\nLet me cite as examples Fve skills that you can develop once you\\nleave school that, in combination with your degree or degrees\\nand your other skills, can help maximize your potential:\\n112 The Pmarca Blog Archives\\nFirst, communication.\\nBack to Scott Adams:\\nI always advise young people to become good public speakers (top\\n25%). Anyone can do it wit',\n",
              " 's, can help maximize your potential:\\n112 The Pmarca Blog Archives\\nFirst, communication.\\nBack to Scott Adams:\\nI always advise young people to become good public speakers (top\\n25%). Anyone can do it with practice. If you add that talent to any\\nother, suddenly you’re the boss of the people who have only one\\nskill…\\nAt least one of the skills in your mixture should involve communication, either written or verbal.\\nThe great thing about communication is that most people are\\nterrible at it, because they never take it seriously as a skill to\\ndevelop.\\nThis is particularly true of engineers and technical people, who\\noaen quaintly believe that the world works logically and that\\npeople will automatically recognize the quality of things.\\nHa!\\nOf course, communication is critically important because it’s\\nhow you convey information and concepts to lots of people in\\nways that will cause them to change their behavior.\\nThis is one good argument for certain liberal arts undergrad\\ndegrees, such as English. ',\n",
              " 'how you convey information and concepts to lots of people in\\nways that will cause them to change their behavior.\\nThis is one good argument for certain liberal arts undergrad\\ndegrees, such as English. But you don’t need speciXc college\\ntraining to be a good communicator — you can learn communication many other ways, including by doing, by practicing, by\\ntaking classes (how about a class in standup comedy? I’m serious!), and by reading a lot. And communication in combination\\nwith some other useful skill is much more powerful than communication alone.\\nAn engineer or a Xnance person or a lawyer who can communicate is hugely more valuable than one who cannot.\\nAnd in the long run, you are going to have a very hard time ever\\nchanging the world if you can’t communicate really well.\\nSecond, management.\\nIf at all possible, learn how to manage people.\\nPart 2: Skills and education 113\\nThe best way is to learn from a great manager.\\nEarly in your career, make sure you are working for a great\\nmanager',\n",
              " 'ent.\\nIf at all possible, learn how to manage people.\\nPart 2: Skills and education 113\\nThe best way is to learn from a great manager.\\nEarly in your career, make sure you are working for a great\\nmanager — you’ll know her when you see her in action — and\\nthen ask her to teach you how to do it.\\nAnd then give it a shot — ask for and get responsibility for a\\nteam of people whom you manage.\\nEven if your career path won’t involve managing lots of people,\\nbeing able to manage will give you a highly valuable tool that\\nyou can pull out whenever you need it, instead of forcing you to\\nalways be reliant on other people to manage.\\nWorst case, you’ll understand a lot more about why companies\\nwork the way they do and why people are the way they are.\\nWhich is hugely helpful when you set about doing something\\nnew.\\nThird, sales.\\nLearn how to sell.\\nI don’t mean, learn how to sell someone a set of steak knives\\nthey don’t need — although I hear that can be quite an education by itself.\\nI mean, learn how to c',\n",
              " 'g\\nnew.\\nThird, sales.\\nLearn how to sell.\\nI don’t mean, learn how to sell someone a set of steak knives\\nthey don’t need — although I hear that can be quite an education by itself.\\nI mean, learn how to convince people that something is in their\\nbest interest to do, even when they don’t realize it up front.\\nThink of this as the art of being able to interact with people\\nsuch that they will do what you want, predictably and repeatedly, as long as you are making sense and oWering them something they should want.\\nThis is another terribly underrated skill, at least among people\\nwho aren’t professional salespeople. But it’s an incredibly general skill that can be helpful not only in your career but\\nthroughout your entire life. Knowing how to sell can also help\\nyou recruit, raise money, talk to investors, create business partnerships,\\ndeal with reporters and analysts, and more — even, God help you,\\nin your marriage and with your kids.\\n114 The Pmarca Blog Archives\\nSpending a year or more in an act',\n",
              " 'tors, create business partnerships,\\ndeal with reporters and analysts, and more — even, God help you,\\nin your marriage and with your kids.\\n114 The Pmarca Blog Archives\\nSpending a year or more in an actual salesforce can be a superb\\nidea even if you have no intention of making your career in\\nsales. John Doerr once told me that the year he spent “carrying\\na bag” in sales at Intel in the late 70’s was the most valuable year\\nof his life in terms of skills development — skills he now uses\\nevery day as one of the world’s most successful venture capitalists. If you’ve ever had John Doerr try to talk you into something, you’ll know what he means.\\nFourth, Hnance.\\nA strong level of Xnancial literacy — Xnancial theory, understanding Xnancial statements, budgeting and planning, corporate structure, how equity and debt markets work — will be a\\nhuge boost for almost any career.\\nAgain, this is a more general skill that it appears to be — having\\nXnancial skills will also help you in your personal life,',\n",
              " ' equity and debt markets work — will be a\\nhuge boost for almost any career.\\nAgain, this is a more general skill that it appears to be — having\\nXnancial skills will also help you in your personal life, as well as\\nin any nonproXt organizations in which you participate.\\nAnd if you ever want to start your own company, being Xnancially literate will be a huge help.\\nIf you’re, for example, a programmer working at a tech company and you don’t know anything about Xnance, go Xnd a\\nXnance person and oWer to teach her all about soaware in return\\nfor her teaching you all about Xnance.\\nOtherwise, Xnance is something you can easily learn by taking\\nclasses, or by reading books.\\nAlso, make an investment in yourself by reading the Financial\\nTimes and the Wall Street Journal every day. Read those two\\npapers cover to cover for Xve years and you’ll know a lot of\\nwhat you need to know. (This recommendation will be even\\nmore practical once Rupert Murdoch makes the Wall Street Journal web site free. The Fina',\n",
              " ' cover to cover for Xve years and you’ll know a lot of\\nwhat you need to know. (This recommendation will be even\\nmore practical once Rupert Murdoch makes the Wall Street Journal web site free. The Financial Times just announced its web site\\nis becoming free for casual readers. But even still, if I were you,\\nI’d get paper subscriptions to those two papers, and every day\\ntake an hour and sit in a corner and read them cover to cover\\nPart 2: Skills and education 115\\n— except of course for the Journal‘s op-ed pages; those will rot\\nyour brain.)\\nFiLh, international.\\nTime spent on the ground in other countries and in other cultures will pay oW in many diWerent ways throughout your\\ncareer.\\nIf your company, or university, oWers you the opportunity to\\nspend a year in another country, it’s probably a pretty good idea\\nto take it.\\nPersonally I’d incline towards spending that time in younger,\\nfaster growing market economies — like China, India, South\\nKorea, or Argentina — versus older, slower growing ',\n",
              " 'retty good idea\\nto take it.\\nPersonally I’d incline towards spending that time in younger,\\nfaster growing market economies — like China, India, South\\nKorea, or Argentina — versus older, slower growing market\\neconomies like France or Germany. But almost any international exposure is likely to be helpful.\\nThis is another of those skills where there’s both a pragmatic\\nbeneXt — you will have experience on the ground with people\\nin a speciXc country — and a general beneXt — you will know\\nhow to think more broadly than the average American, or\\nAmerican president, who has never been out of the country.\\nThere aren’t very many interesting businesses anymore that\\ndon’t have a strong international element — in fact, many\\nAmerican companies now generate the majority of their revenue and proXt outside the US. Having a global perspective can\\nonly help you maximize your future opportunities.\\nAny 7nal thoughts on education?\\nYes.\\nIf you’re in college now, or about to graduate from college, and\\nyou come ',\n",
              " 'he US. Having a global perspective can\\nonly help you maximize your future opportunities.\\nAny 7nal thoughts on education?\\nYes.\\nIf you’re in college now, or about to graduate from college, and\\nyou come from an upper middle class background — especially\\nif you are going to an Ivy League school — take the time to\\nread a provocative essay David Brooks wrote several years ago\\ncalled “The Organization Kid”.\\n116 The Pmarca Blog Archives\\nSome excerpts:\\nI asked several [Ivy League] students to describe their daily schedules, and their replies sounded like a session of Future Workaholics\\nof America: crew practice at dawn, classes in the morning, residentadviser duty, lunch, study groups, classes in the aaernoon, tutoring\\ndisadvantaged kids in Trenton, a cappella practice, dinner, study,\\nscience lab, prayer session, hit the StairMaster, study a few hours\\nmore…\\n[N]owhere did I Xnd any real unhappiness with this state of aWairs;\\nnowhere did I Xnd anybody who seriously considered living any\\nother way',\n",
              " 'prayer session, hit the StairMaster, study a few hours\\nmore…\\n[N]owhere did I Xnd any real unhappiness with this state of aWairs;\\nnowhere did I Xnd anybody who seriously considered living any\\nother way. These super-accomplished kids aren’t working so hard\\nbecause they are compelled to… It’s not the stick that drives them\\non, it’s the carrot. Opportunity lures them… [I]n a rich informationage country like America, promises of enjoyable work abound — at\\nleast for people as smart and ambitious as these. “I want to be this\\nbusy,” one young woman insisted, aaer she had described a daily\\nschedule that would count as slave-driving if it were imposed on\\nanyone…\\nThat doesn’t mean that these leaders-in-training are money-mad\\n(though they are certainly career-conscious). It means they are\\ngoal-oriented. An activity — whether it is studying, hitting the\\ntreadmill, drama group, community service, or one of the student\\ngroups they found and join in great numbers — is rarely an end\\nin itself. It is a ',\n",
              " 'd. An activity — whether it is studying, hitting the\\ntreadmill, drama group, community service, or one of the student\\ngroups they found and join in great numbers — is rarely an end\\nin itself. It is a means for self-improvement, résumé-building, and\\nenrichment. College is just one step on the continual stairway of\\nadvancement, and they are always aware that they must get to the\\nnext step (law school, medical school, whatever) so that they can\\nprogress up the steps aaer that…\\nThey’re not trying to buck the system; they’re trying to climb it,\\nand they are streamlined for ascent…\\nKids of all stripes [today] lead lives that are structured, supervised,\\nand stuWed with enrichment… Today’s elite kids are likely to spend\\ntheir aaernoons and weekends shuttling from one skill-enhancing\\nactivity to the next. By the time they reach college, they take this\\nsort of pace for granted…\\nIn short, at the top of the meritocratic ladder we have in America\\na generation of students who are extraordinarily bri',\n",
              " 'e next. By the time they reach college, they take this\\nsort of pace for granted…\\nIn short, at the top of the meritocratic ladder we have in America\\na generation of students who are extraordinarily bright, morally\\nearnest, and incredibly industrious. They like to study and socialize\\nin groups. They create and join organizations with great enthusiasm. They are responsible, safety-conscious, and mature. They feel\\nno compelling need to rebel — not even a hint of one. They not\\nonly defer to authority; they admire it. “Alienation” is a word one\\nPart 2: Skills and education 117\\nalmost never hears from them. They regard the universe as beneficent, orderly, and meaningful. At the schools and colleges where\\nthe next leadership class is being bred, one Xnds not angry revolutionaries, despondent slackers, or dark cynics but the Organization\\nKid.\\nNow, if your parents are middle class, or lower middle class,\\nand you’re attending a state school or a local college, and you’re\\nworking your way through ',\n",
              " 'ers, or dark cynics but the Organization\\nKid.\\nNow, if your parents are middle class, or lower middle class,\\nand you’re attending a state school or a local college, and you’re\\nworking your way through school in order to pay for tuition,\\nyou can stop reading now; you probably don’t have anything to\\nworry about. But if you read Brooks’ essay and recognize yourself, read on.\\nThe good news is that Brooks’ fundamental thesis is correct: kids\\ngraduating from top colleges and universities today are in many ways\\nbetter prepared for achievement and success than ever before. As a\\ngroup, you are better educated, better trained, more motivated,\\nand more serious than many of your predecessors. And that is\\nfantastic.\\nThe risk, however, is this:\\nIf you have lived an orchestrated existence, gone to great\\nschools, participated in lots of extracurricular activities, had\\nparents who really concentrated hard on developing you\\nbroadly and exposing you to lots of cultural experiences, and\\ngraduated from an e',\n",
              " 'chools, participated in lots of extracurricular activities, had\\nparents who really concentrated hard on developing you\\nbroadly and exposing you to lots of cultural experiences, and\\ngraduated from an elite university in the Xrst 22 or more years\\nof your life, you are in danger of entering the real world, being\\nsmacked hard across the face by reality, and never recovering.\\nWhat do I mean? It’s possible you got all the way through those\\nXrst 22 or more years and are now entering the workforce without ever really challenging yourself. This sounds silly because\\nyou’ve been working hard your whole life, but working hard\\nis not what I’m talking about. You’ve been continuously surrounded by a state of the art parental and educational support\\nstructure — a safety net — and you have yet to make tough decisions,\\nby yourself, in the absence of good information, and to live with the\\nconsequences of screwing up.\\nIn my opinion, it’s now critically important to get into the real\\nworld and really chall',\n",
              " 'h decisions,\\nby yourself, in the absence of good information, and to live with the\\nconsequences of screwing up.\\nIn my opinion, it’s now critically important to get into the real\\nworld and really challenge yourself — expose yourself to risk\\n118 The Pmarca Blog Archives\\n— put yourself in situations where you will succeed or fail by\\nyour own decisions and actions, and where that success or failure will be highly visible.\\nBy failure I don’t mean getting a B or even a C, but rather: having\\nyour boss yell at you in front of your peers for screwing up a\\nproject, launching a product and seeing it tank, being unable to\\nmeet a ship date, missing a critical piece of information in a\\nXnancial report, or getting Xred.\\nWhy? If you’re going to be a high achiever, you’re going to be\\nin lots of situations where you’re going to be quickly making decisions in the presence of incomplete or incorrect information, under\\nintense time pressure, and oMen under intense political pressure. You’re\\ngoing to screw ',\n",
              " 'where you’re going to be quickly making decisions in the presence of incomplete or incorrect information, under\\nintense time pressure, and oMen under intense political pressure. You’re\\ngoing to screw up — frequently — and the screwups will have\\nserious consequences, and you’ll feel incredibly stupid every\\ntime. It can’t faze you — you have to be able to just get right back up\\nand keep on going.\\nThat may be the most valuable skill you can ever learn. Make\\nsure you start learning it early.\\nPart 2: Skills and education 119\\nPart 3: Where to go and why\\nWhen picking an industry to enter, my favorite rule of thumb is\\nthis:\\nPick an industry where the founders of the industry — the\\nfounders of the important companies in the industry — are\\nstill alive and actively involved.\\nThis is easy to Xgure out — just look at the CEO, chairman or\\nchairwoman, and board of directors for the major companies in\\nthe industry.\\nIf the founders of the companies are currently serving as CEO, chairman or chairwoman, ',\n",
              " ' — just look at the CEO, chairman or\\nchairwoman, and board of directors for the major companies in\\nthe industry.\\nIf the founders of the companies are currently serving as CEO, chairman or chairwoman, or board member of their companies, it’s a good\\nindustry to enter. It is probably still young and vital, and there are\\nprobably still opportunities to exploit all over the place, either\\nat those companies or at new companies in that industry.\\nIf not — if the industry’s founders are dead, or old and out of\\ntouch — beware. That industry is now dominated by companies\\nthat are being run by second or third or even fourth generation managers who inherited their companies pre-built, and are\\nserving as caretakers.\\nIf you are young and want to have an impact, you want to be in\\nan industry where there is a lot of growth and change and Iux\\nand opportunity.\\nAs an industry ages, the vitality drains out until all that’s lea is a\\nset of ossiXed remnants in the form of oligopolostic entities of\\nwhich you ',\n",
              " 's a lot of growth and change and Iux\\nand opportunity.\\nAs an industry ages, the vitality drains out until all that’s lea is a\\nset of ossiXed remnants in the form of oligopolostic entities of\\nwhich you would Xnd being a part to be completely soul-killing.\\nThe exception comes when an industry has gotten so old and\\nossiXed that the clear opportunity exists to up-end it and introduce a new order, a new way of doing things, and therefore a\\nnew set of companies.\\nIn some industries this happens routinely — e.g. every 10-20\\nyears. This is the case in technology, for example, and Xnancial\\nservices.\\nIt doesn’t seem to happen ever in certain other industries which\\nI won’t name for fear of being permanently cut oW from my\\nnecessary supply of oil, gas, music, and movies.\\nIf you’re going to enter an old industry, make sure to do it on\\nthe side of the forces of radical change that threaten to up-end\\nthe existing order — and make sure that those forces of change\\nhave a reasonable chance at succeeding.\\n',\n",
              " 'industry, make sure to do it on\\nthe side of the forces of radical change that threaten to up-end\\nthe existing order — and make sure that those forces of change\\nhave a reasonable chance at succeeding.\\nSecond rule of thumb:\\nOnce you have picked an industry, get right to the center of it\\nas fast as you possibly can.\\nYour target is the core of change and opportunity — Xgure\\nout where the action is and head there, and do not delay your\\nprogress for extraneous opportunities, no matter how lucrative\\nthey might be.\\nNever worry about being a small Hsh in a big pond. Being a big\\nXsh in a small pond sucks — you will hit the ceiling on what you\\ncan achieve quickly, and nobody will care. Optimize at all times\\nfor being in the most dynamic and exciting pond you can Fnd. That is\\nwhere the great opportunities can be found.\\nApply this rule when selecting which company to go to. Go to\\nthe company where all the action is happening.\\nOr, if you are going to join a startup or start your own company,\\nalways ',\n",
              " 'ties can be found.\\nApply this rule when selecting which company to go to. Go to\\nthe company where all the action is happening.\\nOr, if you are going to join a startup or start your own company,\\nalways make sure that your startup is aimed at the largest and\\nPart 3: Where to go and why 121\\nmost interesting opportunity available — the new markets that\\nare growing fast and changing rapidly.\\nAlso apply this rule when selecting which city to live in. Go to\\nthe city where all the action is happening.\\nFor technology, at least in the US, this is Silicon Valley. For\\nentertainment, this is Los Angeles. For politics, Washington DC.\\nFor coWee, Seattle. For Xnancial services, New York — unless\\nyou are convinced that there are equally compelling opportunities someplace else, like London or Hong Kong or Shanghai.\\nIn my opinion, living anywhere other than the center of your industry\\nis a mistake. A lot of people — those who don’t live in that place\\n— don’t want to hear it. But it’s true. Geographic loca',\n",
              " 'anghai.\\nIn my opinion, living anywhere other than the center of your industry\\nis a mistake. A lot of people — those who don’t live in that place\\n— don’t want to hear it. But it’s true. Geographic locality is still\\n— even in the age of the Internet — critically important if you\\nwant to maximize your access to the best companies, the best\\npeople, and the best opportunities. You can always cite exceptions,\\nbut that’s what they are: exceptions.\\nNo one cares who the top Xlmmaker in Chicago is — hell, people oaen don’t even care who the top Xlmmaker in New York is,\\nand quite a lot of Xlms get made out of New York. On the other\\nhand, the top 50 Xlmmakers in Los Angeles are all very important people in their industry.\\nLet’s Yavor all of the above with a little nuance:\\n“Current importance” may not be the same as “greatest\\nchange”.\\nWhenever you believe that the currently dominant companies,\\nor cities, are not the places of greatest change and opportunity,\\nyou have a decision to make.\\nPerhaps New',\n",
              " 'be the same as “greatest\\nchange”.\\nWhenever you believe that the currently dominant companies,\\nor cities, are not the places of greatest change and opportunity,\\nyou have a decision to make.\\nPerhaps New York, while clearly the Xnancial services capital of\\nthe world, is not the place of greatest opportunity for someone\\nnew. Perhaps, for you, that would be Dubai, or Buenos Aires, or\\nPrague, or Macau.\\nAnd perhaps Goldman Sachs, Morgan Stanley, Lehman Brothers, Citigroup, and JP Morgan Chase, while clearly the most\\n122 The Pmarca Blog Archives\\nimportant Xnancial services companies in the world, are not the\\ncompanies of greatest opportunity for someone new. Perhaps,\\nfor you, that’s a totally diWerent kind of Xnancial services company, like Paypal.\\nThen you have a decision to make — whether to tilt a little\\nconservative and stick with the currently most important place\\nand companies under the rationale that they are still the major\\nagents of change in the industry, or tilt aggressive and go so',\n",
              " 't a little\\nconservative and stick with the currently most important place\\nand companies under the rationale that they are still the major\\nagents of change in the industry, or tilt aggressive and go someplace or to some Xrm that’s up and coming and might represent\\ndisruptive change and therefore even greater opportunity.\\nEither way, to quote Pink Floyd, “set the controls for the heart of\\nthe sun” — be sure you’re heading where the action is, where the\\nbiggest opportunities in your Xeld are, as you’ve chosen to think\\nabout it. Don’t fart around in second and third tier companies\\nthat don’t have a clear mission to dominate their markets.\\nThird rule:\\nIn a rapidly changing Held like technology, the best place to\\nget experience when you’re starting out is in younger, highgrowth companies.\\n(This is not necessarily true in older and more established\\nindustries, but those aren’t the industries we’re talking about.)\\nThere are a bunch of great things that you get when you go to\\na younger, high-gr',\n",
              " 'is not necessarily true in older and more established\\nindustries, but those aren’t the industries we’re talking about.)\\nThere are a bunch of great things that you get when you go to\\na younger, high-growth company:\\n• You’ll get to do lots of stuE.There will be so much stuW to do in\\nthe company that you’ll be able to do as much of it as you\\ncan possibly handle. Which means you’ll gain skills and\\nexperience very quickly.\\n• You’ll probably get promoted quickly.Fast-growing companies are\\ncharacterized by a chronic lack of people who can step up to\\nall the important new leadership jobs that are being created\\nall the time. If you are aggressive and performing well,\\npromotions will come quickly and easily.\\n• You’ll get used to being in a high-energy, rapidly-changing\\nenvironment with sharp people and high expectations.It’s like\\nPart 3: Where to go and why 123\\ntraining for a marathon while wearing ankle weights — if you\\never end up going to a big company, you’ll blow everyone\\naway. And if you e',\n",
              " 'and high expectations.It’s like\\nPart 3: Where to go and why 123\\ntraining for a marathon while wearing ankle weights — if you\\never end up going to a big company, you’ll blow everyone\\naway. And if you ever go to a startup, you’ll be ready for the\\nintensity.\\n• Reputational beneFt.Having Silicon Graphics from the early\\n90’s, or Netscape from the mid-90’s, or eBay from the late\\n90’s, or Paypal from the early 00’s, or Google from the\\nmid-00’s on your resume is as valuable as any advanced\\ndegree — it’s a permanent source of credibility.\\nIn contrast to going to a big company: working for a big company\\nteaches you how to work for big companies. The way things\\nwork at a big company is usually unique to big companies. So,\\nworking for a big company is oaen a statement that you plan\\nto spend your career at big companies — and lots of people are\\nvery happy doing that, but I doubt that’s your intention or you\\nwouldn’t be reading this post.\\nIn contrast to going to a startup: when you are Xrst starting',\n",
              " 'r at big companies — and lots of people are\\nvery happy doing that, but I doubt that’s your intention or you\\nwouldn’t be reading this post.\\nIn contrast to going to a startup: when you are Xrst starting your\\ncareer, you should realize that raw startups are highly variable in\\nterms of the experiences you will have. Some can be great, but\\nmany are very poorly managed and go nowhere. You will probably be better oW going somewhere that’s already succeeding,\\ngain skills and experience, and then go to a startup.\\nIn contrast to going to a mediocre small or mid-sized company that’s\\nnot growing: those are great places to go if you don’t want to go\\nanywhere yourself. If you Xnd yourself stuck in one, either Xgure out how to get the company unstuck and on a fast growth\\npath, or get yourself unstuck.\\nThere is a caveat to all this, which is as follows:\\nDon’t just be a “summertime soldier” — don’t go someplace\\nbecause it’s already successful, and then bail when things get\\ntough.\\nAny hiring manager for',\n",
              " 'ere is a caveat to all this, which is as follows:\\nDon’t just be a “summertime soldier” — don’t go someplace\\nbecause it’s already successful, and then bail when things get\\ntough.\\nAny hiring manager for the rest of your career will be able to\\nread that on your resume just by looking at the dates.\\nHigh-growth companies virtually always hit speed bumps, or even huge\\n124 The Pmarca Blog Archives\\npotholes. StuW goes wrong. Going through the experience of gutting through the hard parts and coming out the other end will\\nbe a key part of your real-world education and will serve you\\nvery well down the road, especially if you ever start your own\\ncompany.\\nThen, once you’ve racked up killer skills and experiences at a\\nhigh-growth company, feel free to go to a startup.\\nPicking which startup to join probably deserves its own post.\\nHowever, in a nutshell, look for one where you understand the product, see how it might Ft into a very large market, and really like and\\nrespect the people who are already ',\n",
              " 'ably deserves its own post.\\nHowever, in a nutshell, look for one where you understand the product, see how it might Ft into a very large market, and really like and\\nrespect the people who are already there.\\nOr, start your own company.\\nIf your startup fails, try another one. If that one fails, get back\\ninto a high-growth company to reset your resume and get more\\nskills and experiences. Then start another company. Repeat as\\nnecessary until you change the world.\\nFinally, every job you take and every role you Hll will always\\nbe a tactical opportunity and a strategic opportunity.\\nThe tactical opportunity is obvious: kick ass and take names —\\ngain skills and experiences that will be valuable to you in the\\nfuture, and do so well that everyone you work with is singing\\nyour praises for decades to come.\\nThe strategic opportunity is less obvious and oaen overlooked.\\nEvery job, every role, every company you go to is an opportunity to learn how a business works and how an industry works.\\nLearn ever',\n",
              " 'ome.\\nThe strategic opportunity is less obvious and oaen overlooked.\\nEvery job, every role, every company you go to is an opportunity to learn how a business works and how an industry works.\\nLearn everything you can about the business and the industry\\nin which you Xnd yourself.\\nThink strategically: how would I start a Xrm like this today? Or, if I\\nwere starting a company in this industry today, how would it be\\ndiWerent than this Xrm? Why is this Xrm and other Xrms in this\\nindustry doing what they do? What are the assumptions underneath their behavior? Should those assumptions be changing?\\nPart 3: Where to go and why 125\\nHow might this industry work diWerently? Which customers\\nare being underserved? What new technologies might change\\nthings completely? How were things working 10 years ago, versus today, versus 10 years from now? And, my favorite: if the\\ncreators of this industry were starting out today, what would\\nthey be doing now?\\nIn FX’s great new series Damages, a young attorney name',\n",
              " 'ersus today, versus 10 years from now? And, my favorite: if the\\ncreators of this industry were starting out today, what would\\nthey be doing now?\\nIn FX’s great new series Damages, a young attorney named Ellen\\nParsons has gone to work for a famous law Xrm called Hewes\\nand Associates, run by the legendary, ruthless, and amoral Patty\\nHewes. Ellen, rattled by the intensity of her experience at Hewes\\nand Associates, asks her mentor Hollis Nye what she should do:\\nHollis Nye: My advice to you, Ellen, is to stop trying to Xgure Patty\\nout. You’ll never change her, but she’ll change you.\\nEllen: How?\\nNye: By giving you access to how she thinks. You signed up for this;\\nnow, keep your head down, and do the work. That’s why you’re\\nthere, isn’t it?\\nEllen: Yes.\\nNye: Then don’t be shortsighted. Start using her. Learn everything\\nyou can, then get the hell out of there before it’s too late.\\nEllen: How exactly will I know when that is?\\nNye: Ah. That’s for another walk.\\n…and another post.\\n126 The Pmarca Blo',\n",
              " 'her. Learn everything\\nyou can, then get the hell out of there before it’s too late.\\nEllen: How exactly will I know when that is?\\nNye: Ah. That’s for another walk.\\n…and another post.\\n126 The Pmarca Blog Archives\\nThe Pmarca Guide to Personal\\nProductivity\\nOne of my all-time favorite guilty pleasures is indulging in productivity porn.\\nProductivity porn (or, for those really in the know, “productivity\\npr0n”) consists of techniques, tactics, and tricks for maximizing\\npersonal productivity — or, as they say, “getting things done”.\\nHaving enjoyed such Xne purveyors of prodporn as Merlin\\nMann, Danny O’Brien, Gina Trapani, David Allen, and Tim Ferriss, I’d like to return the favor with the following: the Pmarca\\nGuide to Personal Productivity.\\nThe techniques that follow work together as an integrated set\\nfor me, but they probably won’t for you. Maybe you’ll get one or\\ntwo ideas — probably out of the ideas I stole from other people.\\nIf so, I have succeeded.\\nAnd here we go; let’s start with a bang:',\n",
              " ' set\\nfor me, but they probably won’t for you. Maybe you’ll get one or\\ntwo ideas — probably out of the ideas I stole from other people.\\nIf so, I have succeeded.\\nAnd here we go; let’s start with a bang:\\nDon’t keep a schedule\\nHe’s crazy, you say!\\nI’m totally serious. If you pull it oW — and in many structured\\njobs, you simply can’t — this simple tip alone can make a huge\\ndiWerence in productivity.\\nBy not keeping a schedule, I mean: refuse to commit to meetings, appointments, or activities at any set time in any future\\nday.\\nAs a result, you can always work on whatever is most important\\nor most interesting, at any time.\\nWant to spend all day writing a research report? Do it!\\nWant to spend all day coding? Do it!\\nWant to spend all day at the cafe down the street reading a book\\non personal productivity? Do it!\\nWhen someone emails or calls to say, “Let’s meet on Tuesday at\\n3″, the appropriate response is: “I’m not keeping a schedule for\\n2007, so I can’t commit to that, but give me a call on Tue',\n",
              " 'ivity? Do it!\\nWhen someone emails or calls to say, “Let’s meet on Tuesday at\\n3″, the appropriate response is: “I’m not keeping a schedule for\\n2007, so I can’t commit to that, but give me a call on Tuesday at\\n2:45 and if I’m available, I’ll meet with you.”\\nOr, if it’s important, say, “You know what, let’s meet right now.”\\nClearly this only works if you can get away with it. If you have\\na structured job, a structured job environment, or you’re a CEO,\\nit will be hard to pull oW.\\nBut if you can do it, it’s really liberating, and will lead to far\\nhigher productivity than almost any other tactic you can try.\\nThis idea comes from a wonderful book called A Perfect Mess,\\nwhich explains how not keeping a schedule has been key to\\nArnold Schwarzenegger’s success as a movie star, politician, and\\nbusinessman over the last 20 years.\\nIf you have at any point in your life lived a relatively structured\\nexistence — probably due to some kind of job with regular oZce\\nhours, meetings, and the like — you wil',\n",
              " 'ssman over the last 20 years.\\nIf you have at any point in your life lived a relatively structured\\nexistence — probably due to some kind of job with regular oZce\\nhours, meetings, and the like — you will know that there is nothing more liberating than looking at your calendar and seeing\\nnothing but free time for weeks ahead to work on the most\\nimportant things in whatever order you want.\\nThis also gives you the best odds of maximizing Yow, which is a\\nwhole ‘nother topic but highly related.\\n128 The Pmarca Blog Archives\\nI’ve been trying this tactic as an experiment in 2007, as those of\\nyou who have emailed me to suggest we get together or that I\\ngo to a conference or to a meeting will attest. And I am so much\\nhappier, I can’t even tell you. I get so much more time to focus\\non the things that really matter — in my case, my two companies, my nonproXt boards, and my lovely wife.\\nThe other great thing about this tactic is that it doesn’t have to\\nbe all or nothing — there are quite a few things',\n",
              " 'eally matter — in my case, my two companies, my nonproXt boards, and my lovely wife.\\nThe other great thing about this tactic is that it doesn’t have to\\nbe all or nothing — there are quite a few things that still sneak\\nonto my calendar that I really can’t get out of. But one is still\\nable to draw the line between “must do” and “sounds interesting\\nbut I’m not keeping a schedule”.\\nKeep three and only three lists: a Todo List,\\na Watch List, and a Later List.\\nThe more into lists you are, the more important this is.\\nInto the Todo List goes all the stuW you “must” do — commitments, obligations, things that have to be done. A single list, possibly subcategorized by timeframe (today, this week, next week,\\nnext month).\\nInto the Watch List goes all the stuW going on in your life that\\nyou have to follow up on, wait for someone else to get back to\\nyou on, remind yourself of in the future, or otherwise remember.\\nInto the Later List goes everything else — everything you might\\nwant to do or will do wh',\n",
              " 'ow up on, wait for someone else to get back to\\nyou on, remind yourself of in the future, or otherwise remember.\\nInto the Later List goes everything else — everything you might\\nwant to do or will do when you have time or wish you could do.\\nIf it doesn’t go on one of those three lists, it goes away.\\nEach night before you go to bed, prepare a\\n3×5 index card with a short list of 3 to 5\\nthings that you will do the next day.\\nAnd then, the next day, do those things.\\nI sit down at my desk before I go to sleep, pull up my Todo List\\nThe Pmarca Guide to Personal Productivity 129\\n(which I keep in Microsoa Word’s outline mode, due to long\\nhabit), and pick out the 3 to 5 things I am going to get done\\ntomorrow. I write those things on a fresh 3×5 card, lay the card\\nout with my card keys, and go to bed. Then, the next day, I try\\nlike hell to get just those things done. If I do, it was a successful\\nday.\\nPeople who have tried lots of productivity porn techniques will\\ntell you that this is one of the mos',\n",
              " 'hen, the next day, I try\\nlike hell to get just those things done. If I do, it was a successful\\nday.\\nPeople who have tried lots of productivity porn techniques will\\ntell you that this is one of the most successful techniques they\\nhave ever tried.\\nOnce you get into the habit, you start to realize how many days\\nyou used to have when you wouldn’t get 3 to 5 important/significant/meaningful things done during a day.\\nThen, throughout the rest of the day, use the back of the 3×5\\ncard as your Anti-Todo List.\\nThis isn’t a real list. And the name is tongue Xrmly in cheek.\\nWhat you do is this: every time you do something — anything\\n— useful during the day, write it down in your Anti-Todo List\\non the card.\\nEach time you do something, you get to write it down and you\\nget that little rush of endorphins that the mouse gets every time\\nhe presses the button in his cage and gets a food pellet.\\nAnd then at the end of the day, before you prepare tomorrow’s\\n3×5 card, take a look at today’s card and its Ant',\n",
              " 'that the mouse gets every time\\nhe presses the button in his cage and gets a food pellet.\\nAnd then at the end of the day, before you prepare tomorrow’s\\n3×5 card, take a look at today’s card and its Anti-Todo list and\\nmarvel at all the things you actually got done that day.\\nThen tear it up and throw it away.\\nAnother day well spent, and productive.\\nI love this technique — being able to put more notches on my\\naccomplishment belt, so to speak, by writing down things on my\\nAnti-Todo list as I accomplish them throughout the day makes\\nme feel marvelously productive and eZcient. Far more so than\\nif I just did those things and didn’t write them down.\\nPlus, you know those days when you’re running around all day\\n130 The Pmarca Blog Archives\\nand doing stuW and talking to people and making calls and\\nresponding to emails and Xlling out paperwork and you get\\nhome and you’re completely exhausted and you say to yourself,\\n“What the hell did I actually get done today?”\\nYour Anti-Todo list has the answer.\\n',\n",
              " 'ponding to emails and Xlling out paperwork and you get\\nhome and you’re completely exhausted and you say to yourself,\\n“What the hell did I actually get done today?”\\nYour Anti-Todo list has the answer.\\nBy the way, in order to do this, you have to carry a pen with\\nyou everywhere you go. I recommend the Fisher Space Pen.\\nIt’s short and bullet-shaped so it won’t poke you in the thigh\\nwhen it’s in your pocket, it’s wonderfully retro, it helped save\\nthe Apollo 11 mission, and it writes upside down. What’s not to\\nlike?\\nStructured procrastination\\nThis is a great one.\\nThis one is liaed straight from the genius mind of John Perry,\\na philosophy professor at Stanford. (Read his original description, by all means. You even get to see a photo of him practicing\\njumping rope with seaweed on a beach while work awaits. Outstanding.)\\nThe gist of Structured Procrastination is that you should never\\nXght the tendency to procrastinate — instead, you should use it\\nto your advantage in order to get other things',\n",
              " 'ork awaits. Outstanding.)\\nThe gist of Structured Procrastination is that you should never\\nXght the tendency to procrastinate — instead, you should use it\\nto your advantage in order to get other things done.\\nGenerally in the course of a day, there is something you have to\\ndo that you are not doing because you are procrastinating.\\nWhile you’re procrastinating, just do lots of other stuW instead.\\nAs John says, “The list of tasks one has in mind will be ordered\\nby importance. Tasks that seem most urgent and important are\\non top. But there are also worthwhile tasks to perform lower\\ndown on the list. Doing these tasks becomes a way of not doing\\nthe things higher up on the list. With this sort of appropriate\\ntask structure, the procrastinator becomes a useful citizen.\\nIndeed, the procrastinator can even acquire, as I have, a reputation for getting a lot done.”\\nThe Pmarca Guide to Personal Productivity 131\\nReading John’s essay was one of the single most profound\\nmoments of my entire life.\\nFor ',\n",
              " 'an even acquire, as I have, a reputation for getting a lot done.”\\nThe Pmarca Guide to Personal Productivity 131\\nReading John’s essay was one of the single most profound\\nmoments of my entire life.\\nFor example, I hate making phone calls. Hate it. Love sending\\nemails, enjoy seeing people face to face (sometimes), but I hate\\nmaking phone calls.\\nI can get so much done while I am avoiding making a phone call\\nthat I need to make, I can barely believe it.\\nIn fact, that’s what’s happening right now.\\nThe other key two-word tactic…\\nStrategic incompetence\\nThe best way to to make sure that you are never asked to do\\nsomething again is to royally screw it up the Xrst time you are\\nasked to do it.\\nOr, better yet, just say you know you will royally screw it up —\\nmaintain a strong voice and a clear gaze, and you’ll probably get\\noW the hook.\\nOf course, this assumes that there are other things that are more\\nimportant at which you are competent.\\nWhich, hopefully, there are.\\nOrganizing the company picnic, se',\n",
              " ' you’ll probably get\\noW the hook.\\nOf course, this assumes that there are other things that are more\\nimportant at which you are competent.\\nWhich, hopefully, there are.\\nOrganizing the company picnic, sending faxes or Fedexes,\\nnegotiating with insurance brokers, writing in plain English…\\nthe list of things at which one can be strategically incompetent\\nis nearly endless.\\nDo email exactly twice a day\\n— say, once Xrst thing in the morning, and once at the end of\\nthe workday.\\nAllocated half an hour or whatever it takes, but otherwise, keep\\nyour email client shut and your email notiHcations turned oG.\\n132 The Pmarca Blog Archives\\nAnyone who needs to reach you so urgently that it can’t wait\\nuntil later in the day or tomorrow morning can call you, or send\\na runner, or send up smoke signals, or something else.\\nOr, more likely, Xnd someone else who can do whatever it is that\\nneeds doing.\\n(If you communicate with your spouse or key family members\\nvia email during the day, then just set up a separat',\n",
              " 'hing else.\\nOr, more likely, Xnd someone else who can do whatever it is that\\nneeds doing.\\n(If you communicate with your spouse or key family members\\nvia email during the day, then just set up a separate email\\naccount just for them and leave that open all day, but keep\\nyour primary email closed. And never give out the family email\\naddress to anyone noncritical — including your boss.)\\nOnly doing email twice a day will make you far more productive\\nfor the rest of the day.\\nThe problem with email is that getting an email triggers that\\nsame endorphin hit I mentioned above — the one that a mouse\\ngets when he bonks on the button in the cage and gets a food\\npellet.\\nResponding to an email triggers that same hit.\\nThe pleasure chemical hits your neocortex and you go “ahhh”\\ninside and feel like you’ve done something.\\nSo you sit and work with your mail client open and you interrupt your work every time an email comes in and you answer it\\nand you send another email and you feel great in the moment.\\nBu',\n",
              " 'one something.\\nSo you sit and work with your mail client open and you interrupt your work every time an email comes in and you answer it\\nand you send another email and you feel great in the moment.\\nBut what you’re really doing is fracturing your time, interrupting your Yow, and killing your ability to focus on anything long\\nenough to get real high-quality work done.\\nThis one is far easier to say than do. And it won’t be feasible\\nduring projects where lots of updates during the day really are\\nimportant — raising money, for example, or closing a big deal.\\nMe, I’m just trying to get down to checking email only a half\\ndozen times per day.\\nThe Pmarca Guide to Personal Productivity 133\\nWhen you do process email, do it like this\\nFirst, always Hnish each of your two daily email sessions with a\\ncompletely empty inbox.\\nI don’t know about you, but when I know I have emails in my\\ninbox that haven’t been dealt with, I Xnd it hard to concentrate\\non other things.\\nThe urge to go back to my email is ne',\n",
              " 'pletely empty inbox.\\nI don’t know about you, but when I know I have emails in my\\ninbox that haven’t been dealt with, I Xnd it hard to concentrate\\non other things.\\nThe urge to go back to my email is nearly overpowering.\\n(I am apparently seriously addicted to endorphins.)\\nSecond, when doing email, either answer or Hle every single\\nmessage until you get to that empty inbox state of grace.\\nNot keeping a schedule helps here, a lot, if you can pull it oW —\\nyou can reply to a lot of messages with “I’m sorry, I’m not keeping a schedule in 2007, I can’t commit to that.”\\nThird, emails relating to topics that are current working projects or pressing issues go into temporary subfolders of a\\nfolder called Action.\\nYou should only have Action subfolders for the things that really\\nmatter, right now.\\nThose subfolders then get used, and the messages in them\\nprocessed, when you are working on their respective projects in\\nthe normal course of your day.\\nFourth, aside from those temporary Action subfolders,',\n",
              " ' subfolders then get used, and the messages in them\\nprocessed, when you are working on their respective projects in\\nthe normal course of your day.\\nFourth, aside from those temporary Action subfolders, only\\nkeep three standing email folders: Pending, Review, and Vault.\\nEmails that you know you’re going to have to deal with again —\\nsuch as emails in which someone is committing something to\\nyou and you want to be reminded to follow up on it if the person doesn’t deliver — go in Pending.\\nEmails with things you want to read in depth when you have\\nmore time, go into Review.\\nEverything else goes into Vault.\\n134 The Pmarca Blog Archives\\nEvery once in a while, sweep through your Action subfolders\\nand dump any of them that you can into Vault.\\n(And do the same thing for messages in your Pending folder —\\nmost of the things in there you will never look at again. Actually,\\nsame is true for Review.)\\nThat’s it.\\nYou can get away with this because modern email clients are so\\ngood at search (well, most o',\n",
              " 'r —\\nmost of the things in there you will never look at again. Actually,\\nsame is true for Review.)\\nThat’s it.\\nYou can get away with this because modern email clients are so\\ngood at search (well, most of them — and you can always move\\nto GMail) that it’s not worth the eWort to try to Xle emails into\\nlots of diWerent folders.\\nObviously you may need some additional permanent folders\\nfor important things like contracts, or emails from your doctor,\\nor the like, but these are exceptions and don’t change your standard operating procedure.\\nDon’t answer the phone\\nLet it go to voicemail, and then every few hours, screen your\\nvoicemails and batch the return calls.\\nSay, twice a day.\\nCell phones and family plans are so cheap these days that I think\\nthe best thing to do is have two cell phones with diWerent numbers — one for key family members, your closest friends, and\\nyour boss and a few coworkers, and the other for everyone else.\\nAnswer the Xrst one when it rings, but never answer the second\\none.\\n',\n",
              " 'ent numbers — one for key family members, your closest friends, and\\nyour boss and a few coworkers, and the other for everyone else.\\nAnswer the Xrst one when it rings, but never answer the second\\none.\\nHide in an iPod\\nOne of the best and easiest ways to avoid distractions in the\\nworkplace is to be wearing those cute little IPod earbud headphones (or any other headphones of your choice).\\nThe Pmarca Guide to Personal Productivity 135\\nPeople, for some reason, feel much worse interrupting you if\\nyou are wearing headphones than if you’re not.\\nIt’s great — a lot of the time, people will walk up to you, start\\nto say something, notice the headphones, apologize (using exaggerated mouth motions), and walk away.\\nThis is great — half the time they didn’t actually need to talk to\\nyou, and the other half of the time they can send an email that\\nyou can process at the end of the day during the second of your\\ntwo daily email sweeps.\\nHere’s the best part: you don’t actually have to be listening to\\nanythin',\n",
              " 'of the time they can send an email that\\nyou can process at the end of the day during the second of your\\ntwo daily email sweeps.\\nHere’s the best part: you don’t actually have to be listening to\\nanything.\\nHell, you don’t even have to have the headphones plugged into\\nanything.\\nSleeping and Eating\\nI’m not going to talk a lot about getting up early or going to\\nbed late or anything else related to the course of a typical day,\\nbecause everyone’s diWerent.\\nPersonally I go back and forth between being a night owl (99% of\\nthe time) and a morning person (1% — I’m going to try to push\\nit to 2%).\\nBut the thing that matters almost more than anything in determining whether I’ll have a happy, satisfying day is this: no matter what time you get up, start the day with a real, sit-down\\nbreakfast.\\nThis serves two purposes.\\nFirst, it fuels you up. Study aaer study have shown that breakfast\\nis, yes, the most important meal of the day. It’s critical to properly fuel the body for the day’s activities and it’s',\n",
              " ' two purposes.\\nFirst, it fuels you up. Study aaer study have shown that breakfast\\nis, yes, the most important meal of the day. It’s critical to properly fuel the body for the day’s activities and it’s also critical to\\nstaying lean or losing weight. (People who don’t have breakfast\\ntend to eat more, and worse, at lunch.)\\nSecond, it gives you a chance to calmly, peacefully collect your\\n136 The Pmarca Blog Archives\\nthoughts and prepare mentally and emotionally for the day\\nahead.\\nThis works whether you do it with kids and/or a partner, or\\nyou’re solo.\\nPersonally I think it’s worth whatever eWort is involved to go to\\nbed early enough to wake up early enough to have a good solid\\n45 minutes or an hour for breakfast each morning, if you can\\npull it oW.\\nOnly agree to new commitments when both\\nyour head and your heart say yes\\nThis one is from the great Robert Evans. (Hold out for\\nthe audiobook — trust me.)\\nIt’s really easy to get asked to do something — a new project, a\\nnonproXt activity, a soci',\n",
              " 'head and your heart say yes\\nThis one is from the great Robert Evans. (Hold out for\\nthe audiobook — trust me.)\\nIt’s really easy to get asked to do something — a new project, a\\nnonproXt activity, a social event — and to have your head say\\nyes and your heart say no, and then your mouth says yes.\\nThe next thing you know, you’re piled up with all kinds of things\\non your schedule that sounded like a good idea at the time but\\nyou really don’t want to do.\\nAnd distract you from the things that really matter.\\nAnd make you angry, and bitter, and sullen, and hostile.\\n(Oh, wait, I’m projecting.)\\nIn my experience, it takes time to tell the diWerence between\\nyour head saying yes and your heart saying yes.\\nI think the key is whether you’re really excited about it.\\nIf you get that little adrenaline spike (in a good way) when you\\nthink about it, then your heart is saying yes.\\nThe corollary, of course, is that when your head says no and\\nyour heart says yes, your mouth should generally say yes as well\\n:-)',\n",
              " 'in a good way) when you\\nthink about it, then your heart is saying yes.\\nThe corollary, of course, is that when your head says no and\\nyour heart says yes, your mouth should generally say yes as well\\n:-).\\nThe Pmarca Guide to Personal Productivity 137\\nBut not when your head says yes and your heart says no.\\nDo something you love\\nAs you’ve probably concluded by now, most of the tactics\\ndescribed in this post involve keeping oneself as free as possible\\nto pursue one’s core interests, and dreams.\\nIf you’re not doing something you love with the majority of\\nyour time, and you have any personal freedom and Yexibility\\nwhatsoever, it’s time for a change.\\nAnd this doesn’t mean something that you love doing in theory\\n— but rather, the core thing you love doing in practice.\\nAnd that’s it.\\nPlease feel free to nominate additions to the list! Next time\\nmy mobile wiki-based GTD Outlook synchronized hipster PDA\\nreminds me, I’ll check ‘em out.\\nNotes based on reader feedback:\\nTurns out Robert Benchley wrote ',\n",
              " ' to nominate additions to the list! Next time\\nmy mobile wiki-based GTD Outlook synchronized hipster PDA\\nreminds me, I’ll check ‘em out.\\nNotes based on reader feedback:\\nTurns out Robert Benchley wrote about structured procrastination back in 1949. Wonderful essay — highly recommended.\\nThe sharpest reaction has been to my theory of not keeping a\\nschedule. I’ll stick to my theory but make (or re-make) a couple\\nof clarifying points.\\nFirst, it is certainly true that many people have jobs and responsibilities where they can’t do that. Or maybe can only do it partially. And many people enjoy living a highly structured life and\\nobviously this approach is not for them.\\nBut if your reaction is, “boy, I wish I could do that”, then it may\\nwell be worth rethinking your approach to your career.\\nI can tell you from personal experience that being stuck in a role\\nwhere you have a lot of structure but feel like you never get any138 The Pmarca Blog Archives\\nthing done is not the optimal way to advance in',\n",
              " 'll you from personal experience that being stuck in a role\\nwhere you have a lot of structure but feel like you never get any138 The Pmarca Blog Archives\\nthing done is not the optimal way to advance in one’s profession,\\nor maximize one’s job satisfaction.\\nSecond, I do not recommend pursuing this approach in one’s\\npersonal life :-).\\nOn another topic, the tactic of each night, write down the 3 to\\n5 things you need to do the next day has struck some people as\\ntoo simplistic.\\nThat may be the case for some people, but I can’t tell you how\\nmany times I’ve arrived home at night and am at a loss as to\\nwhat I actually got done that day, despite the fact that I worked\\nall day.\\nAnd I also can’t tell you how oaen I’ve had a huge, highly-structured todo list in front of me with 100 things on it and I stare\\nat it and am paralyzed into inaction (or, more likely, structured\\nprocrastination).\\nSo a day when I get 3 to 5 concrete, actionable things done in\\naddition to all the other stuW one has to do to g',\n",
              " 'are\\nat it and am paralyzed into inaction (or, more likely, structured\\nprocrastination).\\nSo a day when I get 3 to 5 concrete, actionable things done in\\naddition to all the other stuW one has to do to get through the\\nday — well, that’s a good day.\\nA few people have said, why not just use GTD (David Allen’s\\n“Getting Things Done” approach).\\nWhile I Xnd GTD to be highly inspiring, in practice I think it’s\\nawfully complex. At least if your job is based on project work (as\\nopposed to having a highly structured role like CEO or head of\\nsales).\\nFor me, an organization system that requires signiXcant time to\\ndeal with in and of itself is not optimal. Much better, for me at\\nleast, is to focus on stripping away nonessentials and freeing up\\nas much time as possible to deal with whatever is most important.\\nFinally, I discovered aaer writing this post that Paul Graham talks a bit about the role of time and focus in personal productivity in his essay on “The Power of the Marginal”.\\nThe Pmarca Guide to',\n",
              " 'nt.\\nFinally, I discovered aaer writing this post that Paul Graham talks a bit about the role of time and focus in personal productivity in his essay on “The Power of the Marginal”.\\nThe Pmarca Guide to Personal Productivity 139\\nThanks for all the comments!\\n140 The Pmarca Blog Archives\\nPsychology and\\nEntrepreneurship\\nThe Psychology of Entrepreneurial\\nMisjudgment: Biases 1-6\\nCharlie Munger is an 80-something billionaire who cofounded\\ntop-tier law Xrm Munger, Tolles &amp; Olson and is Warren\\nBuffett’s long-time partner and Vice-Chairman at Berkshire\\nHathaway, one of the most successful companies of all time.\\nSome people, including me, consider Mr. Munger to be an even\\nmore interesting thinker and writer than Mr. BuWett, and\\nrecently a group of Mr. Munger’s friends assembled a compilation book of his most interesting thoughts and speeches\\ncalled Poor Charlie’s Almanack, inspired by Ben Franklin’s Poor\\nRichard’s Almanack. (The Munger book is only available on Amazon in used form, although yo',\n",
              " 'of his most interesting thoughts and speeches\\ncalled Poor Charlie’s Almanack, inspired by Ben Franklin’s Poor\\nRichard’s Almanack. (The Munger book is only available on Amazon in used form, although you can apparently buy a new\\ncopy here.)\\nMr. Munger’s magnum opus speech, included in the book,\\nis The Psychology of Human Misjudgment — an exposition of 25\\nkey forms of human behavior that lead to misjudgment and\\nerror, derived from Mr. Munger’s 60 years of business experience. Think of it as a practitioner’s summary of human psychology and behavioral economics as observed in the real world.\\nIn this series of blog posts, I will walk through all 25 of the biases\\nMr. Munger identiXes, and then adapt them for the modern\\nentrepreneur. In each case I will start with relevant excerpts of\\nMr. Munger’s speech, and then aaer that add my own thoughts.\\nOne: Reward and Punishment\\nSuperresponse Tendency\\nI place this tendency Xrst in my discussion because almost everyone thinks he fully recognizes how im',\n",
              " ' speech, and then aaer that add my own thoughts.\\nOne: Reward and Punishment\\nSuperresponse Tendency\\nI place this tendency Xrst in my discussion because almost everyone thinks he fully recognizes how important incentives and disincentives are in changing cognition and behavior. But this is not\\noaen so. For instance, I think I’ve been in the top Xve percent of\\nmy age cohort almost all my adult life in understanding the power\\nof incentives, and yet I’ve always underestimated that power. Never\\na year passes but I get some surprise that pushes a little further my\\nappreciation of incentive superpower.\\n…We [should] heed the general lesson implicit in the injunction of\\nBen Franklin in Poor Richard’s Almanack: “If you would persuade,\\nappeal to interest and not to reason.” This maxim is a wise guide\\nto a great and simple precaution in life: Never, ever, think about\\nsomething else when you should be thinking about the power of\\nincentives…\\nOne of the most important consequences of incentive superpo',\n",
              " 'to a great and simple precaution in life: Never, ever, think about\\nsomething else when you should be thinking about the power of\\nincentives…\\nOne of the most important consequences of incentive superpower\\nis what I call “incentive caused bias.” A man has an acculturated\\nnature making him a pretty decent fellow, and yet, driven both\\nconsciously and subconsciously by incentives, he drias into\\nimmoral behavior in order to get what he wants, a result he facilitates by rationalizing his bad behavior [like a salesman who harms\\nher customers by selling them the wrong product because she gets\\npaid more for selling it, versus the right product — see, e.g., the\\nmutual fund industry].\\n…Another generalized consequence of incentive caused bias is that\\nman tends to “game” all human systems, oaen displaying great\\ningenuity in wrongly serving himself at the expense of others.\\nAntigaming features, therefore, constitute a huge and necessary\\npart of almost all system design.\\n…Military and naval organizati',\n",
              " 'ng great\\ningenuity in wrongly serving himself at the expense of others.\\nAntigaming features, therefore, constitute a huge and necessary\\npart of almost all system design.\\n…Military and naval organizations have very oaen been extreme\\nin using punishment [the inverse of reward] to change behavior,\\nprobably because they needed to cause extreme behavior. Around\\nthe time of Caesar, there was a European tribe that, when the\\nassembly horn blew, always killed the last warrior to reach his\\nassigned place, and no one enjoyed Xghting this tribe.\\nHuman response to incentives is indeed a huge behavioral\\nmotivator, and I think Mr. Munger is right that once you think\\nyou realize how big it is, you need to assume it’s even bigger.\\nThe Psychology of Entrepreneurial Misjudgment: Biases 1-6 143\\nThis is why stock options work so well in startups — and the\\nfewer people in a startup, the better stock options work, since\\nwhen there are only a few people in a company, it’s usually crystal clear to each person ',\n",
              " 'ock options work so well in startups — and the\\nfewer people in a startup, the better stock options work, since\\nwhen there are only a few people in a company, it’s usually crystal clear to each person how her work will impact the value of\\nthe company.\\nThere is a wrong-headed and dangerous theory afoot that\\nrestricted stock (grants of fully in-the-money shares of stock) is\\na more appropriate motivator of employees of tech companies\\nthan stock options:\\nMr. Gates wanted Mr. BuWett’s input on whether to drop options in\\nfavor of restricted stock at Microsoa. [Gates] recalls asking: “How\\nwill employees respond to getting a lottery ticket that gives them a\\ndeXnite amount instead of one that could amount to nothing or a\\nridiculous sum?”\\nMr. BuWett’s reply, according to Mr. Gates, was: “My wife would\\nrather have a ticket for one fur coat, than a ticket that gave her two\\nor nothing.”\\nOvert sexism aside, from an incentive standpoint the result of\\nshiaing from stock options to restricted stock shou',\n",
              " 'd\\nrather have a ticket for one fur coat, than a ticket that gave her two\\nor nothing.”\\nOvert sexism aside, from an incentive standpoint the result of\\nshiaing from stock options to restricted stock should be obvious: current employees will be incented to preserve value instead\\nof creating value. And new hires will by deXnition be people who\\nare conservative and change-averse, as the people who want\\nto swing for the fences and get rewarded for creating something new will go somewhere else, where they will receive stock\\noptions — in typically greater volume than anyone will ever\\ngrant restricted stock — and have greater upside.\\nAnd sure enough, in the wake of shiaing towards restricted\\nstock and away from stock options, Microsoa’s stock has been\\nYat as a pancake. The incentive works.\\nNow, against that, it is true that stock options, particularly for\\npublic companies, have an oaen-destructive random component: they tend to increase in value in rising stock market environments and decrease i',\n",
              " 'nst that, it is true that stock options, particularly for\\npublic companies, have an oaen-destructive random component: they tend to increase in value in rising stock market environments and decrease in value (potentially to zero) in falling\\nstock market environments, regardless of whether value is being\\ncreated inside your particular company.\\nFor that reason, in the long run it probably makes sense for\\n144 The Pmarca Blog Archives\\nsome new approach to stock-based compensation to be developed that both preserves the motivation to create as opposed\\nto preserve value, but factors out the environmental swings of\\nrising and falling stock markets. Some form of indexing against\\nmarket averages would probably do the trick. This has been\\ntried from time to time, and I expect it to be tried more in the\\nfuture, at least for public companies.\\nAs a company grows, stock options and other forms of equitybased motivation become less and less useful as an incentive\\ntool, since it becomes harder for man',\n",
              " 'the\\nfuture, at least for public companies.\\nAs a company grows, stock options and other forms of equitybased motivation become less and less useful as an incentive\\ntool, since it becomes harder for many employees in a large\\ncompany to see how their individual behavior would have any\\neWect on the stock price of the overall corporation. So, more tactical incentives kick in, such as cash bonuses.\\nThe design of tactical incentives — e.g. bonuses — is a whole\\ntopic in and of itself, and is critically important as your company grows. The most signiXcant thing to keep in mind is that\\nhow the goals are designed really matters — as Mr. Munger says,\\npeople tend to game any system you put in place, and then they\\ntend to rationalize that gaming until they believe they really are\\ndoing the right thing.\\nI think it was Andy Grove who said that for every goal you put in\\nfront of someone, you should also put in place a counter-goal to restrict\\ngaming of the Frst goal.\\nSo, for example, if you are incenti',\n",
              " 'I think it was Andy Grove who said that for every goal you put in\\nfront of someone, you should also put in place a counter-goal to restrict\\ngaming of the Frst goal.\\nSo, for example, if you are incenting your recruiters on the\\nnumber of new employees recruited and hired, you need to\\nalso give them a counter-goal (and tie it to their compensation)\\nthat measures the quality of the new hires three months in.\\nOtherwise the recruiters are guaranteed to give you what\\nyou don’t want: a lot of mediocre new hires.\\nOne of the great unwritten Silicon Valley skewed incentive stories was a major datacenter vendor in the late 90’s that incented\\nits salespeople based on bookings of long-term datacenter leases,\\nwithout suZcient counter-goals tied to revenue collection or\\nthe customer’s ability to pay. Sure enough, soon the company’s\\nreported bookings were heading straight up, revenue was Yat,\\nand cash headed straight down, resulting in a truly spectacular\\nThe Psychology of Entrepreneurial Misjudgment: ',\n",
              " ' enough, soon the company’s\\nreported bookings were heading straight up, revenue was Yat,\\nand cash headed straight down, resulting in a truly spectacular\\nThe Psychology of Entrepreneurial Misjudgment: Biases 1-6 145\\nbankruptcy. The salespeople got paid, though, so they were\\nhappy.\\nMore recently, skewed incentives in the mortgage industry —\\nmortage issuers getting paid based on quantity of mortgages\\nissued, versus ability to pay — caused many of the current catastrophic Wall Street Xnancial meltdowns you get to read about\\nevery day.\\nEven engineers need counter-goals: incent engineers based\\npurely on a ship date, and you’ll get a shipping product with lots\\nof bugs. Incent based on number of bugs Xxed, and you’ll never\\nget any new features. And so on.\\nEspecially in smaller companies, peer pressure can be a very\\neWective form of incentive. This is greatly enabled and abetted\\nby transparency. People hate to be embarrassed in front of their\\npeer group, so if it’s crystal clear who’s performin',\n",
              " 'sure can be a very\\neWective form of incentive. This is greatly enabled and abetted\\nby transparency. People hate to be embarrassed in front of their\\npeer group, so if it’s crystal clear who’s performing well and who\\nisn’t, poor performers will be highly motivated to improve —\\nand if they’re not, that’s good to know, since obviously then you\\nreally need to Xre them.\\nFinally, any entrepreneur should be highly attuned to incentives\\nwhen hiring outside executives, especially a CEO. Hire a CEO\\nand give her a large stock-option grant with four-year vesting,\\nand you can guarantee she will sell the company in year four.\\nGive her a stock-option grant with accelerated vesting on\\nchange of control and she will sell the company sooner than\\nthat. Founders can get tripped up on this because they naturally\\nhave an emotional incentive to see the company succeed that\\nhired executives oaen do not share.\\nAnd of course, never get caught between a venture capitalist and\\nher incentives.\\nTwo: Liking/Loving Te',\n",
              " 'y\\nhave an emotional incentive to see the company succeed that\\nhired executives oaen do not share.\\nAnd of course, never get caught between a venture capitalist and\\nher incentives.\\nTwo: Liking/Loving Tendency\\n…[W]hat will a man naturally come to like and love, apart from his\\nparent, spouse and child? Well, he will like and love being liked and\\nloved… [M]an will generally strive, lifelong, for the aWection and\\napproval of many people not related to him.\\n146 The Pmarca Blog Archives\\nOne very practical consequence of Liking/Loving Tendency is that\\nit acts as a conditioning device that makes the liker or lover tend\\n(1) to ignore faults of, and comply with wishes of, the object of his\\naWection, (2) to favor people, products, and actions merely associated with the object of his aWection (as we shall see when we get\\nto “InYuence-from-Mere-Association Tendency”), and (3) to distort\\nother facts to facilitate love.\\nThe application of this principle to entrepreneurs is obvious:\\nentrepreneurs want t',\n",
              " 'll see when we get\\nto “InYuence-from-Mere-Association Tendency”), and (3) to distort\\nother facts to facilitate love.\\nThe application of this principle to entrepreneurs is obvious:\\nentrepreneurs want to be liked just like everyone else, and wanting to be liked can be a major impediment to entrepreneurial\\nsuccess due to at least two major reasons.\\nFirst, an entrepreneur, like any CEO, has to make tough decisions about what her company will do, and those decisions will\\noaen run counter to the preferences of her employees. You\\ndon’t have to be involved in that many startups to Xnd one\\nwhere the entrepreneur knows she needs to make a tough decision — such as change strategy, or cancel a Yawed project — but\\ncan’t quite do it because employees won’t like it. Of course this\\nalways backXres: employees also don’t like leaders who don’t\\nmake the tough decisions that have to be made.\\nSecond, an entrepreneur, like any manager, has to Xre people\\nwho aren’t great or who aren’t right for the tasks at ',\n",
              " 'ees also don’t like leaders who don’t\\nmake the tough decisions that have to be made.\\nSecond, an entrepreneur, like any manager, has to Xre people\\nwho aren’t great or who aren’t right for the tasks at hand. This\\nnaturally makes people not like you, particularly the people you\\nXre. But again, not doing this backXres: nobody great wants to\\nbe in a company populated by mediocre or ill-Xtting peers.\\nI think these pressures are intensiXed in a small company versus\\na larger company, because in a small company everyone tends\\nto know everyone else and people naturally form strong personal relationships within the group — so the desire to be liked\\nis stronger, and the perceived risk from making decisions that\\npeople won’t like is higher.\\nA speciXc form of this dynamic in a startup is when you have\\nmultiple founders, of whom one is the CEO. The founder who\\nis the CEO inevitably discovers that it becomes very hard to stay\\nclose personal friends with the other founders. As they say, it’s\\nlonely at ',\n",
              " 'ultiple founders, of whom one is the CEO. The founder who\\nis the CEO inevitably discovers that it becomes very hard to stay\\nclose personal friends with the other founders. As they say, it’s\\nlonely at the top — if you’re doing your job right.\\nThe Psychology of Entrepreneurial Misjudgment: Biases 1-6 147\\nFinally, some entrepreneurs have emotional resistance to pursuing a strategy that does not meet with immediate approval\\nfrom press, analysts, and other entrepreneurs. This is worth\\nwatching carefully — if everyone agrees right up front that\\nwhatever you are doing makes total sense, it probably isn’t a new\\nand radical enough idea to justify a new company.\\nThree: Disliking/Hating Tendency\\nIn a pattern obverse to Liking/Loving Tendency, the newly arrived\\nhuman is also “born to dislike and hate” as triggered by normal and\\nabnormal triggering forces in its life…\\nAs a result, the long history of man contains almost continuous\\nwar…\\nDisliking/Hating Tendency also acts as a conditioning device th',\n",
              " ' as triggered by normal and\\nabnormal triggering forces in its life…\\nAs a result, the long history of man contains almost continuous\\nwar…\\nDisliking/Hating Tendency also acts as a conditioning device that\\nmakes the disliker/hater tend to (1) ignore virtues in the object of\\ndislike, (2) dislike people, products, and actions merely associated\\nwith the object of his dislike, and (3) distort other facts to facilitate\\nhatred.\\nIf this is a problem inside your company, then you have bigger\\nissues than I can help you with.\\nHowever, I think this dynamic kicks in for a startup when thinking about competitors.\\nI see two destructive consequences of this bias in startups with\\ncompetitors:\\nFirst, I believe startups oaen overfocus on their competitors. It’s\\nthe easiest thing in the world to orient yourself in opposition to\\nanother company in the same market, and to plan your actions\\nbased on what will cause damage to the competitor or block the\\ncompetitor from getting business.\\nIn the startup world, th',\n",
              " ' in opposition to\\nanother company in the same market, and to plan your actions\\nbased on what will cause damage to the competitor or block the\\ncompetitor from getting business.\\nIn the startup world, that oaen leads to multiple competitors\\nengaged in a shooting war in a market that’s still too small for\\nanyone to succeed.\\nI think it’s much better for a startup to focus on creating and\\n148 The Pmarca Blog Archives\\ndeveloping a large market, as opposed to Xghting over a small\\nmarket.\\nSo when your startup’s competitive juices get Yowing — especially for the Xrst time — and you Xnd yourself Xxated on a\\ncompetitor, be sure to take a step back and say, is this really\\nwhat we want to be focused on right now — is the market we’re\\nboth in really large enough to warrant this? If so, sure, go for it,\\nguns blazing. But if not, stepping back and thinking about how\\nto focus instead on creating a large market might be more valuable.\\nA variant on this dynamic is letting your competitor determine\\nyour st',\n",
              " 'uns blazing. But if not, stepping back and thinking about how\\nto focus instead on creating a large market might be more valuable.\\nA variant on this dynamic is letting your competitor determine\\nyour strategy by watching what he does and then making countermoves. The issue here is that it’s highly likely that neither\\none of you actually knows that much about what you are doing\\nyet — since you are in a new market, by deXnition — and while\\nyou know you don’t know that much about what you’re doing\\nyet, you only observe your competitors’s deliberate actions as\\nopposed to seeing their equivalent or greater level of internal\\nconfusion. So they seem like they know what they’re doing, and\\nso you fall into assuming they know more than you do, when\\nthey probably don’t.\\nSecond, when you are in a truly competitive situation, this bias\\ncan easily lead you to underestimate your competitor by, as Mr.\\nMunger says, “ignoring virtues in the object of dislike”.\\nHis product sucks, his salespeople aren’t as ',\n",
              " 'competitive situation, this bias\\ncan easily lead you to underestimate your competitor by, as Mr.\\nMunger says, “ignoring virtues in the object of dislike”.\\nHis product sucks, his salespeople aren’t as good, his venture\\ncapitalists are those morons who backed that large datacenter\\nvendor that went bankrupt — and so on.\\nNotably, this attitude can become cultural in your company\\nvery quickly. I think that if you’re in a shooting war, even if you\\nprivately think your competitor is an amoral pinhead, that you\\nestablish a tone that says, we’ll assume that he’s highly competent and has many Xne virtues, which we will respect and then\\nsystematically target with our own strengths and virtues until we\\nhave killed him.\\nThe Psychology of Entrepreneurial Misjudgment: Biases 1-6 149\\nFour: Doubt-Avoidance Tendency\\nThe brain of man is programmed with a tendency to quickly\\nremove doubt by reaching some decision.\\nIt is easy to see how evolution would make animals, over the eons,\\ndria toward such quick el',\n",
              " ' Tendency\\nThe brain of man is programmed with a tendency to quickly\\nremove doubt by reaching some decision.\\nIt is easy to see how evolution would make animals, over the eons,\\ndria toward such quick elimination of doubt. Aaer all, the one\\nthing that is surely counterproductive for a prey animal that is\\nthreatened by a predator is to take a long time in deciding what to\\ndo…\\nSo pronounced is the tendency in man to quickly remove doubt\\nby reaching some decision that behavior to counter the tendency is\\nrequired from judges and jurors. Here, delay before decision making is forced. And one is required to so comport himself, prior to\\nconclusion time, so that he is wearing a “mask” of objectivity. And\\nthe “mask” works to help real objectivity along, as we shall see when\\nwe next consider man’s Inconsistency-Avoidance Tendency…\\nWhat triggers Doubt-Avoidance Tendency? Well, an unthreatened\\nman, thinking of nothing in particular, is not being prompted to\\nremove doubt through rushing to some decisio',\n",
              " 'sistency-Avoidance Tendency…\\nWhat triggers Doubt-Avoidance Tendency? Well, an unthreatened\\nman, thinking of nothing in particular, is not being prompted to\\nremove doubt through rushing to some decision. As we shall see\\nlater when we get to Social-Proof Tendency and Stress-InYuence\\nTendency, what usually triggers Doubt-Avoidance Tendency is\\nsome combination of (1) puzzlement and (2) stress.\\nThis is probably a good one for entrepreneurs. You’d better not\\nhave a lot of doubts about what you are doing because everyone\\nelse will, and if you do too, you’ll probably give up.\\nOf course, an entrepreneur’s doubt avoidance is only a plus\\nright up to the point where it becomes pigheaded stubbornness\\nthat interferes with her ability to see reality, particularly when a\\nstrategy is not working.\\nIn my view, entrepreneurial judgment is the ability to tell the\\ndiWerence between a situation that’s not working but persistence\\nand iteration will ultimately prove it out, versus a situation that’s\\nnot workin',\n",
              " 'w, entrepreneurial judgment is the ability to tell the\\ndiWerence between a situation that’s not working but persistence\\nand iteration will ultimately prove it out, versus a situation that’s\\nnot working and additional eWort is a destructive waste of time\\nand radical change is necessary.\\nI don’t believe there are any good rules for being able to tell the\\ndiWerence between the two. Which is one of the main reasons\\nstarting a company is so hard.\\n150 The Pmarca Blog Archives\\nFive: Inconsistency-Avoidance Tendency\\n[People are] reluctant to change, which is a form of inconsistency\\navoidance. We see this in all human habits, constructive and\\ndestructive. Few people can list a lot of bad habits that they have\\neliminated, and some people cannot identify even one of these.\\nInstead, practically every one has a great many bad habits he has\\nlong maintained despite their being known as bad. Given this situation, it is not too much in many cases to appraise early-formed\\nhabits as destiny. When Marley’',\n",
              " 'one has a great many bad habits he has\\nlong maintained despite their being known as bad. Given this situation, it is not too much in many cases to appraise early-formed\\nhabits as destiny. When Marley’s miserable ghost says, “I wear the\\nchains I forged in life,” he is talking about chains of habit that were\\ntoo light to be felt before they became too strong to be broken.\\n[T]ending to be maintained in place by the anti-change tendency\\nof the brain are one’s previous conclusions, human loyalties, reputational identity, commitments…\\nIt is easy to see that a quickly reached conclusion, triggered by\\nDoubt-Avoidance Tendency, when combined with a tendency to\\nresist any change in that conclusion, will naturally cause a lot of\\nerrors in cognition for modern man. And so it observably works\\nout. We all deal much with others whom we correctly diagnose\\nas imprisoned in poor conclusions that are maintained by mental\\nhabits they formed early and will carry to their graves…\\nAnd so, people tend to accu',\n",
              " 'll deal much with others whom we correctly diagnose\\nas imprisoned in poor conclusions that are maintained by mental\\nhabits they formed early and will carry to their graves…\\nAnd so, people tend to accumulate large mental holdings of Xxed\\nconclusions and attitudes that are not oaen reexamined or\\nchanged, even though there is plenty of good evidence that they\\nare wrong…\\nAs Lord Keynes pointed out about his exalted intellectual group at\\none of the greatest universities in the world, it was not the intrinsic\\ndiZculty of new ideas that prevented their acceptance. Instead, the\\nnew ideas were not accepted because they were inconsistent with\\nold ideas in place…\\nWe have no less an authority for this than Max Planck, Nobel laureate, Xnder of “Planck’s constant.” Planck is famous not only for\\nhis science but also for saying that even in physics the radically\\nnew ideas are seldom really accepted by the old guard. Instead, said\\nPlanck, the progress is made by a new generation that comes along,\\nless ',\n",
              " 'nce but also for saying that even in physics the radically\\nnew ideas are seldom really accepted by the old guard. Instead, said\\nPlanck, the progress is made by a new generation that comes along,\\nless brain-blocked by its previous conclusions…\\nOne corollary of Inconsistency-Avoidance Tendency is that a person making big sacriXces in the course of assuming a new identity\\nwill intensify his devotion to the new identity. Aaer all, it would\\nbe quite inconsistent behavior to make a large sacriXce for something that was no good. And thus civilization has invented many\\nThe Psychology of Entrepreneurial Misjudgment: Biases 1-6 151\\ntough and solemn initiation ceremonies, oaen public in nature,\\nthat intensify new commitments made.\\nThis goes hand-in-hand with doubt-avoidance, and again is\\nusually a plus for a startup, since it leads to greater commitment\\non the part of the entrepreneur and the team. (And yes, I am in\\nfavor of blood oaths for startups.)\\nPerhaps this bias is most relevant to how new',\n",
              " 'us for a startup, since it leads to greater commitment\\non the part of the entrepreneur and the team. (And yes, I am in\\nfavor of blood oaths for startups.)\\nPerhaps this bias is most relevant to how new markets develop.\\nSometimes you get lucky — you bring a new product to market,\\nand the target customers all go, great, we’ll take it! However,\\noaen you get a level of resistance from the market that can be\\npuzzling — “can’t they see that our new product would be better\\nfor them than what they have now?”\\nThis in turn leads to the odd dynamic you oaen see where a\\nstartup will Xeld a new product, nobody wants it, and the startup\\ngoes belly up. Then three or four or Xve years later, another\\nstartup launches with a very similar product, and this time the\\nmarket says, hell yes!\\nI think this is something that every entrepreneur needs to watch\\nvery carefully. Sometimes it’s simply a matter of timing — and\\nif people just aren’t ready for a new idea, you usually can’t make\\nthem ready, and you have t',\n",
              " 'hing that every entrepreneur needs to watch\\nvery carefully. Sometimes it’s simply a matter of timing — and\\nif people just aren’t ready for a new idea, you usually can’t make\\nthem ready, and you have to wait for them to change or for a\\nnew generation of customers to come along.\\nMy favorite way around this problem is the one identiXed by\\nClayton Christensen in The Innovator’s Dilemma: don’t go aaer\\nexisting customers in a category and try to get them to buy\\nsomething new; instead, go Xnd the new customers who weren’t\\nable to aWord or adopt the incarnation of the status quo.\\nFor example, when the personal computer was invented, the\\ndesirable market was not the universe of people who were\\nalready buying computers — a.k.a. mainframe and minicomputer buyers — but rather the universe of the people who\\ncouldn’t aWord a mainframe or minicomputer and therefore\\nhad never had a computer before.\\nSimilarly, the desirable market for Hotmail in the early days was\\nnot existing email aXcionados who were',\n",
              " 'e who\\ncouldn’t aWord a mainframe or minicomputer and therefore\\nhad never had a computer before.\\nSimilarly, the desirable market for Hotmail in the early days was\\nnot existing email aXcionados who were already using sophisti152 The Pmarca Blog Archives\\ncated email desktop soaware, but rather the universe of people\\nwho were coming on the Internet for the Xrst time who didn’t\\neven have email yet and for whom web-based email was by far\\nthe easiest way to start.\\nConversely, one of the reasons that today’s consumer Internet\\ncompanies have the wind at our backs versus our peers 10 years\\nago is that a whole new generation of consumers has come of\\nage in the last 10 years for whom the Internet is their primary\\nmedium — time and demographics are on our side now. That\\nmakes life a lot easier, let me tell you. Meanwhile, the average\\nage of television viewers continues driaing higher and higher…\\nSix: Curiosity Tendency\\nThis is, frankly, an odd one for Mr. Munger to include, since it’s\\nprimarily a p',\n",
              " 'tell you. Meanwhile, the average\\nage of television viewers continues driaing higher and higher…\\nSix: Curiosity Tendency\\nThis is, frankly, an odd one for Mr. Munger to include, since it’s\\nprimarily a plus, and he doesn’t really identify a downside.\\nThe only important thing I can think to add — aside from\\nthe importance of hiring curious people — is that lack of curiosity can be a huge danger to a startup in the following way: oaen,\\nyour initial strategy won’t quite work, but you can learn as you\\ngo based on other things that happen in the market and eventually iterate into a strategy that does work. Obviously, insuZcient curiosity can prevent you from seeing the new data and\\nlead you to continue to pursue a losing strategy even when you\\nwouldn’t have to.\\nThe Psychology of Entrepreneurial Misjudgment: Biases 1-6 153\\nAge and the Entrepreneur: Some\\ndata\\nA short time back, several smart bloggers engaged in an enthusiastic debate about age and entrepreneurs — some taking the\\nposition that ki',\n",
              " 'sjudgment: Biases 1-6 153\\nAge and the Entrepreneur: Some\\ndata\\nA short time back, several smart bloggers engaged in an enthusiastic debate about age and entrepreneurs — some taking the\\nposition that kids have a leg up on older entrepreneurs at least\\nfor certain categories of startups, and others theorizing that age\\nis largely irrelevant (or as Ali G would put it, “geezers is good\\nentrepreneurs as well, man”).\\nI have opinions on this topic, but rather than just mouthing oW\\nlike I would normally do, I decided to go get some data.\\nI’m not aware of any systematic data on age and high-tech\\nentrepreneurs. As far as I’m aware, all we have are anecdotes.\\nHowever, a professor of psychology at University of California\\nDavis named Dean Simonton has conducted extensive research\\non age and creativity across many other Xelds, including science, literature, music, chess, Xlm, politics, and military combat.\\nDr. Simonton’s research is unparalleled — he’s spent his career\\nstudying this and related topics',\n",
              " 'ross many other Xelds, including science, literature, music, chess, Xlm, politics, and military combat.\\nDr. Simonton’s research is unparalleled — he’s spent his career\\nstudying this and related topics and his papers make for\\nabsolutely fascinating reading.\\nFor this post, I’ll be concentrating on his paper Age and Outstanding Achievement: What Do We Know Aaer a Century of\\nResearch? from 1988. I haven’t been able to Xnd a PDF of the\\npaper online but you can read a largely intact cached HTML\\nversion courtesy of Google Scholar.\\nLet’s go to the paper:\\nFor centuries, thinkers have speculated about the association\\nbetween a person’s age and exceptional accomplishment: Is there\\nan optimal age for a person to make a lasting contribution to\\nhuman culture or society? When during the life span can we expect\\nan individual to be most proliXc or inYuential?\\nYou can see why I think this is relevant.\\nHere we adopt the product-centered approach, that is, our focus\\nis on real-life achievements rather tha',\n",
              " 'expect\\nan individual to be most proliXc or inYuential?\\nYou can see why I think this is relevant.\\nHere we adopt the product-centered approach, that is, our focus\\nis on real-life achievements rather than performance on abstract…\\nmeasures. …\\n[A]chievement [takes] the form of noteworthy creativity… the goal\\nis to assess how productivity changes over the life span… [I] focus\\non individual accomplishment in such endeavors as science, philosophy, literature, music, and the visual arts. …\\n[Studies like these focus] on three core topics: (a) the age curve that\\nspeciXes how creative output varies over the course of a career, (b)\\nthe connection between productive precocity, longevity, and rate\\nof output, and (c) the relation between quantity and quality of output (i.e., between “productivity” and “creativity”).\\nDr. Simonton also discusses leadership as distinct from creative\\nproduction, but I’m ignoring the leadership part for now since\\nit’s quite diWerent.\\nOne empirical generalization appears to',\n",
              " 'eativity”).\\nDr. Simonton also discusses leadership as distinct from creative\\nproduction, but I’m ignoring the leadership part for now since\\nit’s quite diWerent.\\nOne empirical generalization appears to be fairly secure: If one\\nplots creative output as a function of age, productivity tends to rise\\nfairly rapidly to a deXnite peak and thereaaer decline gradually\\nuntil output is about half the rate at the peak.\\nThis is the centerpiece of Dr. Simonton’s overall theory across\\nmany domains. And is probably not unexpected. But here’s\\nwhere it gets really interesting:\\n[T]he location of the peak, as well as the magnitude of the postpeak\\ndecline, tends to vary depending on the domain of creative\\nachievement.\\nAt one extreme, some Xelds are characterized by relatively early\\npeaks, usually around the early 30s or even late 20s in chronological units, with somewhat steep descents thereaaer, so that the output rate becomes less than one-quarter the maximum. This ageAge and the Entrepreneur: Some data ',\n",
              " 'rly 30s or even late 20s in chronological units, with somewhat steep descents thereaaer, so that the output rate becomes less than one-quarter the maximum. This ageAge and the Entrepreneur: Some data 155\\nwise pattern apparently holds for such endeavors as lyric poetry,\\npure mathematics, and theoretical physics…\\nThe typical trends in other endeavors may display a leisurely rise\\nto a comparatively late peak, in the late 40s or even 50s chronologically, with a minimal if not largely absent drop-oW aaerward. This\\nmore elongated curve holds for such domains as novel writing, history, philosophy, medicine, and general scholarship.\\nWell, that’s interesting.\\nIt must be stressed that these interdisciplinary contrasts do not\\nappear to be arbitrary but instead have been shown to be invariant\\nacross diWerent cultures and distinct historical periods.\\nAs a case in point, the gap between the expected peaks for poets\\nand prose authors has been found in every major literary tradition\\nthroughout the wor',\n",
              " 'iWerent cultures and distinct historical periods.\\nAs a case in point, the gap between the expected peaks for poets\\nand prose authors has been found in every major literary tradition\\nthroughout the world and for both living and dead languages.\\nIndeed, because an earlier productive optimum means that a writer\\ncan die younger without loss to his or her ultimate reputation,\\npoets exhibit a life expectancy, across the globe and through history, about a half dozen years less than prose writers do.\\nYou know what that means — if you’re going to argue that\\nyounger entrepreneurs have a leg up, then you also have to\\nargue that they will have shorter lifespans. Fun with math!\\nYou may not be surprised to Xnd that in creative Xelds, the\\npower law rule — also known as the 80/20 rule — deXnitely\\napplies:\\nA small percentage of the workers in any given domain is responsible for the bulk of the work. Generally, the top 10% of the most\\nproliXc elite can be credited with around 50% of all contributions,\\nwh',\n",
              " 'A small percentage of the workers in any given domain is responsible for the bulk of the work. Generally, the top 10% of the most\\nproliXc elite can be credited with around 50% of all contributions,\\nwhereas the bottom 50% of the least productive workers can claim\\nonly 15% of the total work, and the most productive contributor is\\nusually about 100 times more proliXc than the least.\\nHere’s where it gets really interesting again:\\nPrecocity, longevity, and output rate are each strongly associated\\nwith Xnal lifetime output — that is, those who generate the most\\ncontributions at the end of a career also tend to have begun their\\ncareers at earlier ages, ended their careers at later ages, and produced at extraordinary rates throughout their careers. …\\n156 The Pmarca Blog Archives\\nThese three components are conspicuously linked with each other:\\nThose who are precocious also tend to display longevity, and both\\nprecocity and longevity are positively associated with high output\\nrates per age unit.\\n',\n",
              " 'ents are conspicuously linked with each other:\\nThose who are precocious also tend to display longevity, and both\\nprecocity and longevity are positively associated with high output\\nrates per age unit.\\nOK, so on to the main question, which is, when’s the peak?\\nThose creators who make the most contributions tend to start\\nearly, end late, and produce at above-average rates, but are the\\nanticipated career peaks unchanged, earlier, or later in comparison\\nto what is seen for their less proliXc colleagues? Addressing this\\nquestion properly requires that we Xrst investigate the relation\\nbetween quantity and quality, both within and across careers. …\\nThis is a very complex topic and Dr. Simonton goes into great\\ndetail about it throughout his work. I’m going to gloss over it a\\nbit, but if you are interested in this topic, by all means dig into it\\nmore via Google Scholar.\\nFirst, if one calculates the age curves separately for major and\\nminor works within careers, the resulting functions are basica',\n",
              " 'erested in this topic, by all means dig into it\\nmore via Google Scholar.\\nFirst, if one calculates the age curves separately for major and\\nminor works within careers, the resulting functions are basically\\nidentical…\\nSecond… minor and major contributions… Yuctuate together.\\nThose periods in a creator’s life that see the most masterpieces also\\nwitness the greatest number of easily forgotten productions, on the\\naverage.\\nAnother way of saying the same thing is to note that the “quality\\nratio,” or the proportion of major products to total output per age\\nunit, tends to Yuctuate randomly over the course of any career. The\\nquality ratio neither increases nor decreases with age…\\nThese outcomes are valid for both artistic and scientiXc modes of\\ncreative contribution. What these two results signify is that… age\\nbecomes irrelevant to determining the success of a particular contribution.\\nOK, that’s interesting. Quality of output does not vary by age…\\nwhich means, of course, that attempting to improv',\n",
              " ' that… age\\nbecomes irrelevant to determining the success of a particular contribution.\\nOK, that’s interesting. Quality of output does not vary by age…\\nwhich means, of course, that attempting to improve your batting average of hits versus misses is a waste of time as you\\nprogress through a creative career. Instead you should just focus\\non more at-bats — more output. Think about that one.\\nIf this sounds insane to you, Dr. Simonton points out that the\\nperiods of Beethoven’s career that had the most hits also had the\\nAge and the Entrepreneur: Some data 157\\nmost misses — works that you never hear. As I am always fond\\nof asking in such circumstances, if Beethoven couldn’t increase\\nhis batting average over time, what makes you think you can?\\n[C]reativity is a probabilistic consequence of productivity, a relationship that holds both within and across careers.\\nWithin single careers, the count of major works per age period\\nwill be a positive function of total works generated each period,\\nyieldin',\n",
              " 'vity, a relationship that holds both within and across careers.\\nWithin single careers, the count of major works per age period\\nwill be a positive function of total works generated each period,\\nyielding a quality ratio that exhibits no systematic developmental\\ntrends.\\nAnd across careers, those individual creators who are the most productive will also tend, on the average, to be the most creative: Individual variation in quantity is positively associated with variation\\nin quality.\\nWow.\\nOK, next step:\\n[This] constant-probability-of-success model has an important\\nimplication for helping us understand the relation between total\\nlifetime output and the location of the peak age for creative\\nachievement within a single career.\\nBecause total lifetime output is positively related to total creative\\ncontributions and hence to ultimate eminence, and given that a\\ncreator’s most distinguished work will appear in those career periods when productivity is highest, the peak age for creative impact\\nshoul',\n",
              " 'ontributions and hence to ultimate eminence, and given that a\\ncreator’s most distinguished work will appear in those career periods when productivity is highest, the peak age for creative impact\\nshould not vary as a function of either the success of the particular\\ncontribution or the Xnal fame of the creator. …\\nThus, even though an impressive lifetime output of works, and\\nsubsequent distinction, is tied to precocity, longevity, and production rate, the expected age optimum for quantity and quality of\\ncontribution is dependent solely on the particular form of creative\\nexpression.\\nWow, again.\\nAnyone who demonstrates… an age decrement in achievement is\\nlikely to provoke controversy. Aaer all, aging is a phenomenon easy\\nenough to become defensive about, and such defensiveness is especially probable among those of us who are already past the putative\\nage peak for our particular Xeld of endeavor…\\n158 The Pmarca Blog Archives\\nI think Dr. Simonton is ready to start blogging.\\nHis paper then goe',\n",
              " 'bable among those of us who are already past the putative\\nage peak for our particular Xeld of endeavor…\\n158 The Pmarca Blog Archives\\nI think Dr. Simonton is ready to start blogging.\\nHis paper then goes on to discuss many possible extrinsic factors such as health that could impair later-life output, but in the\\nend he concludes that the data is pretty conclusive that such\\nextrinsinc factors serve as “random shocks” to any individual’s\\ncareer that do not aWect the overall trends.\\nHe then goes on to discuss possible intrinsic factors that could\\nexplain a relationship between age and creative accomplishment:\\nG. M. Beard was not merely the earliest contributor [in 1874] to the\\nempirical literature on age and achievement but its Xrst theorist as\\nwell. According to him, creativity is a function of two underlying\\nfactors, enthusiasm and experience. Enthusiasm provides the\\nmotivational force behind persistent eWort, yet enthusiasm in the\\nabsence of the second factor yields just original work. Ex',\n",
              " ' two underlying\\nfactors, enthusiasm and experience. Enthusiasm provides the\\nmotivational force behind persistent eWort, yet enthusiasm in the\\nabsence of the second factor yields just original work. Experience\\ngives the achiever the ability to separate wheat from chaW and to\\nexpress original ideas in a more intelligible and persistent fashion.\\nYet experience in the absence of enthusiasm produces merely routine contributions. Genuine creativity requires the balanced cooperation of both enthusiasm and experience.\\nBeard postulates, however, that these two essential components\\ndisplay quite distinctive distributions across the life span. Whereas\\nenthusiasm usually peaks early in life and steadily declines thereaaer, experience gradually increases as a positive monotonic function of age. The correct equilibrium between the two factors is\\nattained between the ages of 38 and 40, the most common age\\noptima for creative endeavors. Prior to that expected peak, an individual’s output would be exce',\n",
              " 'rect equilibrium between the two factors is\\nattained between the ages of 38 and 40, the most common age\\noptima for creative endeavors. Prior to that expected peak, an individual’s output would be excessively original, and in the postpeak\\nphase the output would be overly routine. The career Yoruit in the\\nlate 30s thus represents the uniquely balanced juxtaposition of the\\nrhapsodies of youth and the wisdom of maturity.\\nHmmmmmm…\\nBeard’s theory is not without attractive features… Beard’s account,\\nfor all its simplicity, can boast a respectable amount of explanatory\\npower. Besides handling the broad form of the age curve, this theory leads to an interpretation of why diWerent endeavors may peak\\nat distinct ages.\\nThe contrast between poetic and prose literature, for instance, can\\nbe interpreted as the immediate consequence of the assumption\\nAge and the Entrepreneur: Some data 159\\nthat the two domains demand a diWerent mix of the two factors:\\npoetry, more enthusiasm, and prose, more experienc',\n",
              " 'as the immediate consequence of the assumption\\nAge and the Entrepreneur: Some data 159\\nthat the two domains demand a diWerent mix of the two factors:\\npoetry, more enthusiasm, and prose, more experience. Indeed, in\\nXelds in which expertise may be far more crucial than emotional\\nvigor, most notably in scholarship, we would anticipate little if any\\ndecline with age, and such is the case.\\nDr. Simonton, however, then goes on to explain that this theory\\ndoes not really match the data — for example, the data shows\\nthat quality of output in practically all Xelds does not decline\\nsystematically with age, which is what you’d expect from Beard’s\\ntheory.\\nThe paper then digs into possible correlations between intelligence as measured by such metrics as IQ, and creative output:\\n[E]ven if a minimal level of intelligence is requisite for achievement, beyond a threshold of around IQ 120 (the actual amount\\nvarying across Xelds), intellectual prowess becomes largely irrelevant in predicting individual di',\n",
              " ' of intelligence is requisite for achievement, beyond a threshold of around IQ 120 (the actual amount\\nvarying across Xelds), intellectual prowess becomes largely irrelevant in predicting individual diWerences in… creativity.\\nSo what have we learned in a nutshell?\\nGenerally, productivity — output — rises rapidly from the start\\nof a career to a peak and then declines gradually until retirement.\\n• This peak in productivity varies by Xeld, from the late 20s to\\nthe early 50s, for reasons that are Xeld-speciXc.\\n• Precocity, longevity, and output rate are linked. “Those who\\nare precocious also tend to display longevity, and both\\nprecocity and longevity are positively associated with high\\noutput rates per age unit.” High producers produce highly,\\nsystematically, over time.\\n• The odds of a hit versus a miss do not increase over time.\\nThe periods of one’s career with the most hits will also have\\nthe most misses. So maximizing quantity — taking more\\nswings at the bat — is much higher payoW than t',\n",
              " 'us a miss do not increase over time.\\nThe periods of one’s career with the most hits will also have\\nthe most misses. So maximizing quantity — taking more\\nswings at the bat — is much higher payoW than trying to\\nimprove one’s batting average.\\n• Intelligence, at least as measured by metrics such as IQ, is\\nlargely irrelevant.\\n160 The Pmarca Blog Archives\\nSo here’s my Xrst challenge: to anyone who has an opinion on\\nthe role of age and entrepreneurship — see if you can Xt your\\nopinion into this model!\\nAnd here’s my second challenge: is entrepreneurship more like\\npoetry, pure mathematics, and theoretical physics — which\\nexhibit a peak age in one’s late 20s or early 30s — or novel writing, history, philosophy, medicine, and general scholarship —\\nwhich exhibit a peak age in one’s late 40s or early 50s? And how,\\nand why?\\nAge and the Entrepreneur: Some data 161\\nLuck and the entrepreneur: The\\nfour kinds of luck\\nIn the last few weeks, I’ve been reading huge stacks of books on\\nthe psychology of creat',\n",
              " '0s? And how,\\nand why?\\nAge and the Entrepreneur: Some data 161\\nLuck and the entrepreneur: The\\nfour kinds of luck\\nIn the last few weeks, I’ve been reading huge stacks of books on\\nthe psychology of creativity and motivation — which is the reason for the relative scarcity of substantive blog posts. Said post\\nsituation will be remedied shortly, by a series of posts on — surprise! — the psychology of creativity and motivation.\\nBut Xrst, to complement my post on age and the entrepreneur from a few days ago, this post begins a series of occasional\\nposts about luck and the entrepreneur.\\nLuck is something that every successful entrepreneur will tell\\nyou plays a huge role in the diWerence between success and failure. Many of those successful entrepreneurs will only admit this\\nunder duress, though, because if luck does indeed play such a\\nhuge role, then that seriously dents the image of the successful\\nentrepreneur as an omniscient business genius.\\nMoreover, some of those people would shrug and say',\n",
              " 'cause if luck does indeed play such a\\nhuge role, then that seriously dents the image of the successful\\nentrepreneur as an omniscient business genius.\\nMoreover, some of those people would shrug and say that luck\\nis simply out of your hands. Sometimes you have it, sometimes\\nyou don’t. But perhaps there’s more to it than that.\\nDr. James Austin, a neurologist and philosopher (!), wrote an\\noutstanding book called Chase, Chance, and Creativity — originally in 1978, then updated in 2003. It’s the best book I’ve read\\non the role of luck, chance, and serendipity in medical research\\n— or, for that matter, any creative endeavor. And because he’s a\\nneurologist, he has a grounding in how the brain actually exerts\\nitself creatively — although there is more recent research on\\nthat topic that is even more illuminating (more on that later).\\nIn the book, Dr. Austin outlines his theory of the four kinds of\\nluck — or, as he calls it, chance; I will use the terms interchangeably.\\nFirst, he deXnes chance as',\n",
              " ' illuminating (more on that later).\\nIn the book, Dr. Austin outlines his theory of the four kinds of\\nluck — or, as he calls it, chance; I will use the terms interchangeably.\\nFirst, he deXnes chance as follows:\\nChance… something fortuitous that happens unpredictably without\\ndiscernable human intention.\\nYup, that’s luck.\\nChance is unintentional, it is capricious, but we needn’t conclude\\nthat chance is immune from human interventions. However, one\\nmust be careful not to read any unconsciously purposeful intent\\ninto “interventions”… [which] are to be viewed as accidental,\\nunwilled, inadvertent, and unforseeable.\\nIndeed, chance plays several distinct roles when humans react creatively with one another and with their environment…\\nWe can observe chance arriving in four major forms and for four\\ndiWerent reasons. The principles involved aWect everyone.\\nHere’s where it helps to be a neurologist writing on this topic:\\nThe four kinds of chance each have a diWerent kind of motor\\nexploratory activit',\n",
              " 'Werent reasons. The principles involved aWect everyone.\\nHere’s where it helps to be a neurologist writing on this topic:\\nThe four kinds of chance each have a diWerent kind of motor\\nexploratory activity and a diWerent kind of sensory receptivity.\\nThe [four] varieties of chance also involve distinctive personality\\ntraits and diWer in the way one particular individual inYuences\\nthem.\\nOK, so what are they?\\nIn Chance I, the good luck that occurs is completely accidental. It is\\npure blind luck that comes with no eWort on our part.\\nYup.\\nIn Chance II, something else has been added — motion.\\nYears ago, when I was rushing around in the laboratory [conducting\\nLuck and the entrepreneur: The four kinds of luck 163\\nmedical research], someone admonished me by asking, “Why all\\nthe busyness? One must distinguish between motion and progress”.\\nYes, at some point this distinction must be made. But it cannot\\nalways be made Xrst. And it is not always made consciously.\\nTrue, waste motion should be avoided. B',\n",
              " 'nguish between motion and progress”.\\nYes, at some point this distinction must be made. But it cannot\\nalways be made Xrst. And it is not always made consciously.\\nTrue, waste motion should be avoided. But, if the researcher did\\nnot move until he was certain of progress he would accomplish\\nvery little…\\nA certain [basic] level of action “stirs up the pot”, brings in random\\nideas that will collide and stick together in fresh combinations, lets\\nchance operate.\\nMotion yields a network of new experiences which, like a sieve, Xlter best when in constant up-and-down, side-to-side movement…\\nUnluck runs out if you keep stirring up things so that random elements can combine, by virtue of you and their inherent aZnities.\\nSounds like a startup!\\nChance II springs from your energetic, generalized motor activities… the freer they are, the better.\\n[Chance II] involves the kind of luck [Charles] Kettering… had in\\nmind when he said, “Keep on going and chances are you will stumble on something, perhaps when',\n",
              " 'ivities… the freer they are, the better.\\n[Chance II] involves the kind of luck [Charles] Kettering… had in\\nmind when he said, “Keep on going and chances are you will stumble on something, perhaps when you are least expecting it. I have\\nnever heard of anyone stumbling on something sitting down.”\\nOK, now here’s where it gets interesting:\\nNow, as we move on to Chance III, we see blind luck, but it tiptoes\\nin soaly, dressed in camouYage.\\nChance presents only a faint clue, the potential opportunity exists,\\nbut it will be overlooked except by that one person uniquely\\nequipped to observe it, visualize it conceptually, and fully grasp its\\nsigniXcance.\\nChance III involves involves a special receptivity, discernment, and\\nintuitive grasp of signiXcance unique to one particular recipient.\\nLouis Pasteur characterized it for all time when he said, “Chance\\nfavors the prepared mind.”\\nI thought that was Eric Bogosian in Under Siege 2: Dark Territory, but OK.\\n…The classic example of [Chance III] occured',\n",
              " 'r characterized it for all time when he said, “Chance\\nfavors the prepared mind.”\\nI thought that was Eric Bogosian in Under Siege 2: Dark Territory, but OK.\\n…The classic example of [Chance III] occured in 1928, when Sir\\n164 The Pmarca Blog Archives\\nAlexander Fleming’s mind instantly fused at least Xve elements\\ninto a conceptually uniXed nexus [when he discovered penicillin —\\none of the most important medical breakthroughs ever].\\nHe was at his work bench in the laboratory, made an observation,\\nand his mental sequences then went something like this: (a) I see\\nthat a mold has fallen by accident into my culture dish; (2) the\\nstaphylococcal colonies residing near it failed to grow; (3) therefore, the mold must have secreted something that killed the bacteria; (4) this reminds me of a similar experience I had once before;\\n(5) maybe this new “something” from the mold could be used to kill\\nstaphylococci that cause human infections.\\nActually, Fleming’s mind was exceptionally well prepared. Some\\n',\n",
              " 'erience I had once before;\\n(5) maybe this new “something” from the mold could be used to kill\\nstaphylococci that cause human infections.\\nActually, Fleming’s mind was exceptionally well prepared. Some\\nnine years earlier, while suWering from a cold [you can’t make\\nthis stuW up], his own nasal drippings had found their way onto a\\nculture dish. He noted that the bacteria around his mucous were\\nkilled, and astutely followed up the lead. His experiments then\\nled him to discover… lysozyme… [which] proved inappropriate for\\nmedical use, but think of how receptive Fleming’s mind was to the\\npenicillin mold when it later happened on the scene!\\nOK, what about Chance IV?\\n[Chance IV] favors the individualized action.\\nThis is the fourth element in good luck — an active, but unintentional, subtle individualized prompting of it.\\nPlease explain!\\nChance IV is the kind of luck that develops during a probing action\\nwhich has a distinctive personal Yavor.\\nThe English Prime Minister, Benjamin Disraeli, summed',\n",
              " 'lized prompting of it.\\nPlease explain!\\nChance IV is the kind of luck that develops during a probing action\\nwhich has a distinctive personal Yavor.\\nThe English Prime Minister, Benjamin Disraeli, summed up the\\nprinciple underlying Chance IV when he noted: “We make our fortunes and we call them fate.”\\nChance IV comes to you, unsought, because of who you are and\\nhow you behave.\\n…Chance IV is so personal, it is not easily understood by someone\\nelse the Xrst time around… here we probe into the subterranean\\nrecesses of personal hobbies and behavioral quirks that autobiographers know about, biographers rarely.\\n[In neurological terms], Chance III [is] concerned with personal sensory receptivity; its counterpart, Chance IV, [is] involved\\nwith personal motor behavior.\\nLuck and the entrepreneur: The four kinds of luck 165\\nPlease continue!\\n[You] have to look carefully to Xnd Chance IV for three reasons.\\nThe Xrst is that when it operates directly, it unfolds in an elliptical,\\nunorthodox manner.\\nThe ',\n",
              " 'our kinds of luck 165\\nPlease continue!\\n[You] have to look carefully to Xnd Chance IV for three reasons.\\nThe Xrst is that when it operates directly, it unfolds in an elliptical,\\nunorthodox manner.\\nThe second is that it oaen works indirectly.\\nThe third is that some problems it may help solve are uncommonly\\ndiZcult to understand because they have gone through a process of\\nselection.\\nWe must bear in mind that, by the time Chance IV Xnally occurs,\\nthe easy, more accessible problems will already have been solved\\nearlier by conventional actions, conventional logic, or by the operations of the other forms of chance. What remains late in the game,\\nthen, is a tough core of complex, resistant problems. Such problems yield to none but an unusual approach…\\n[Chance IV involves] a kind of discrete behavioral performance\\nfocused in a highly speciXc manner.\\nHere’s the money quote:\\nWhereas the lucky connections in Chance II might come to anyone\\nwith disposable energy as the happy by-product of any aimle',\n",
              " 'oral performance\\nfocused in a highly speciXc manner.\\nHere’s the money quote:\\nWhereas the lucky connections in Chance II might come to anyone\\nwith disposable energy as the happy by-product of any aimless,\\ncircular stirring of the pot, the links of Chance IV can be drawn\\ntogether and fused only by one quixotic rider cantering in on his\\nown homemade hobby horse to intercept the problem at an odd\\nangle.\\nA recap?\\nChance I is completely impersonal; you can’t inYuence it.\\nChance II favors those who have a persistent curiosity about many\\nthings coupled with an energetic willingness to experiment and\\nexplore.\\nChance III favors those who have a suZcient background of sound\\nknowledge plus special abilities in observing, remembering, recalling, and quickly forming signiXcant new associations.\\nChance IV favors those with distinctive, if not eccentric hobbies,\\npersonal lifestyles, and motor behaviors.\\nThis of course leads to a number of challenges for how we live\\nour lives as entrepreneurs and creat',\n",
              " 'V favors those with distinctive, if not eccentric hobbies,\\npersonal lifestyles, and motor behaviors.\\nThis of course leads to a number of challenges for how we live\\nour lives as entrepreneurs and creators in any Xeld:\\n166 The Pmarca Blog Archives\\n• How energetic are we?How inclined towards motion are we?\\nThose of you who read my Xrst age and the entrepreneur\\npost will recognize that this is a variation on the “optimize\\nfor the maximum number of swings of the bat” principle. In\\na highly uncertain world, a bias to action is key to catalyzing\\nsuccess, and luck, and is oaen to be preferred to thinking\\nthings through more throughly.\\n• How curious are we?How determined are we to learn about\\nour chosen Xeld, other Xelds, and the world around us? In my\\npost on hiring great people, I talked about the value I place\\non curiosity — and speciXcally, curiosity over intelligence.\\nThis is why. Curious people are more likely to already have\\nin their heads the building blocks for craaing a solution for\\na',\n",
              " 'e value I place\\non curiosity — and speciXcally, curiosity over intelligence.\\nThis is why. Curious people are more likely to already have\\nin their heads the building blocks for craaing a solution for\\nany particular problem they come across, versus the more\\nquote-unquote intelligent, but less curious, person who is\\ntrying to get by on logic and pure intellectual eWort.\\n• How Iexible and aggressive are we at synthesizing– at\\nlinking together multiple, disparate, apparently unrelated\\nexperiences on the Yy? I think this is a hard skill to\\nconsciously improve, but I think it is good to start most\\ncreative exercises with the idea that the solution may come\\nfrom any of our past experiences or knowledge, as opposed\\nto out of a textbook or the mouth of an expert. (And, if you\\nare a manager and you have someone who is particularly\\ngood at synthesis, promote her as fast as you possibly can.)\\n• How uniquely are we developing a personal point of view\\n— a personal approach– a personal set of “eccentr',\n",
              " 'ave someone who is particularly\\ngood at synthesis, promote her as fast as you possibly can.)\\n• How uniquely are we developing a personal point of view\\n— a personal approach– a personal set of “eccentric hobbies,\\npersonal lifestyles, and motor behaviors” that will uniquely\\nprepare us to create? This, in a nutshell, is why I believe that\\nmost creative people are better oW with more life experience\\nand journeys aXeld into seemingly unrelated areas, as\\nopposed to more formal domain-speciXc education — at\\nleast if they want to create.\\nIn short, I think there is a roadmap to getting luck on our side,\\nand I think this is it.\\nLuck and the entrepreneur: The four kinds of luck 167\\nSerial Entrepreneurs\\nSeveral days ago, Gary Rivlin of the New York Times called me\\nabout a story he was writing about the brilliant Max Levchin of\\nPaypal and Slide, and the general topic of serial entrepreneurs\\nin Silicon Valley. The story came out yesterday; below are the\\nnotes I prepared for my conversation with Gary',\n",
              " 'e brilliant Max Levchin of\\nPaypal and Slide, and the general topic of serial entrepreneurs\\nin Silicon Valley. The story came out yesterday; below are the\\nnotes I prepared for my conversation with Gary.\\nIn a nutshell, Gary’s question to me was: what makes serial\\nentrepreneurs tick? Why do people like Max keep going and\\nstart new companies when they could just park it on a beach and\\nsuck down mai tais?\\nFirst, in my experience, Silicon Valley entrepreneurs are all over\\nthe map when it comes to personality and motivation. Some\\nare purely mercenary — one hit and they’re out. Others just\\nlove the technology, and the business is a side eWect. Still others\\nare like Chauncey Gardiner in Being There. And some just love\\nstarting and building companies.\\nSecond, there were serial entrepreneurs in the past, but there\\nare certainly more now than ever before. There are many factors that lead to this — here are the big ones:\\n• There are simply more entrepreneurs now — due to the\\namazing surge in ventur',\n",
              " 'ast, but there\\nare certainly more now than ever before. There are many factors that lead to this — here are the big ones:\\n• There are simply more entrepreneurs now — due to the\\namazing surge in venture capital and the culture of startups\\nover the last 10-15 years — so you’d expect more serial\\nentrepreneurs just based on that.\\n• A lot of new companies simply develop faster these days than\\nthey did in the past. Microsoa and Oracle, for example, both\\nput in 10 years of incredibly hard work before going public\\n(both founded in ’76, IPO in ’86), and they only had a few\\nhundred employees each when they went public — and those\\nwere the two biggest soaware successes of their era. Versus\\nthese days, when many companies are founded, built, scaled\\nup, and sold (or, yes, taken public!) in a few years. The\\nprocess can happen so fast that people are freed up much\\nfaster; therefore, upon being freed up they are younger and\\ntend to have more raw energy than people who in the past\\nwould have spent 10 o',\n",
              " ' The\\nprocess can happen so fast that people are freed up much\\nfaster; therefore, upon being freed up they are younger and\\ntend to have more raw energy than people who in the past\\nwould have spent 10 or 20 or 30 years building a single\\ncompany — and by the time they freed up, they maybe\\ndidn’t want to put that level of eWort into something again.\\n• Also because of the faster cycle time, when you start\\ncompany #2 you can assume that it won’t necessarily\\nconsume the next 10-20-30 years of your life… This makes it\\neasier for people to say, OK, hey, it worked once, I’ll try it\\nagain.\\n• The culture of startups in the Valley is clicking on all\\ncylinders — everything from fundraising to hiring to\\nbuilding out a management team to signing up lawyers and\\naccountants and bankers is simply easier than ever before.\\nI’m talking in a macro sense — over the last 10 years, versus\\nprior decades, even considering the early 2000′s bust. So it’s\\njust easier to start the next company that it was the past —\\n',\n",
              " ' ever before.\\nI’m talking in a macro sense — over the last 10 years, versus\\nprior decades, even considering the early 2000′s bust. So it’s\\njust easier to start the next company that it was the past —\\nthe “pain in the ass” factor is lower.\\n• In terms of exit, there are some IPO’s, but the big thing is\\nthat M&A is a widely accepted and viable exit. Big companies\\nin and related to the Valley have actually become quite good,\\nin general, at acquiring small companies — not perfect, but\\nquite good. They do it frequently, in order to build out their\\nproduct families or grow market share. This of course\\ninspires more companies to be started and tends to compress\\nthe time cycles further.\\nThird, all that said, it is striking how many of the truly revolutionary companies are started, at least in part, by people who\\nSerial Entrepreneurs 169\\nhaven’t done it before. Google (Brin and Page), Yahoo (Yang and\\nFilo), Facebook (Zuckerberg), Apple (Jobs and Wozniak), etc.\\nWhen you see one of those really re',\n",
              " ' by people who\\nSerial Entrepreneurs 169\\nhaven’t done it before. Google (Brin and Page), Yahoo (Yang and\\nFilo), Facebook (Zuckerberg), Apple (Jobs and Wozniak), etc.\\nWhen you see one of those really revolutionary companies and\\nthere’s some young kid with the idea, of course, they oaen are\\nlinked up with one or more seasoned, experienced people —\\nGoogle (Schmidt, Doerr, Moritz), Yahoo (Moritz, Koogle), Facebook (Thiel, Breyer), Apple (Markkula). So even there you see a\\nkind of a serial entrepreneur (or VC or executive) eWect which is\\nanother form of what you’re talking about.\\nFourth, drilling deeper into the motivations of the great serial\\nentrepreneurs I know, the dominant themes are:\\n• Desire to prove oneself — either “I can do it again — it wasn’t\\na Yuke the Xrst time”, or “I was the junior partner last time,\\nnow I’ll be the senior partner”, or “I got Xred from my last\\ncompany, I’ll show those f****** VCs”, or something like that.\\n• Desire to continue working and being productive — “I',\n",
              " 'or partner last time,\\nnow I’ll be the senior partner”, or “I got Xred from my last\\ncompany, I’ll show those f****** VCs”, or something like that.\\n• Desire to continue working and being productive — “I’m 26\\nor 30 or 34, I have a lot of energy, I have to keep moving, and\\nI’m certainly not going to go to work for some boring big\\ncompany or be another hack VC… obviously I need to start\\nanother company”.\\n• In love with the technology or a new idea — there’s more of\\nthis than cynical people think.\\n• A feeling that we’re in a unique time and place where it’s\\npossible for us to start, build, and be successful with multiple\\ncompanies — it’d be a shame to walk away from the\\nopportunity to continue to be a part of such a magical time\\nand place. This is a big motivator for me, by the way.\\nGrowing up, I would have never dreamed that an industry\\nlike this would exist or that I would get to be a part of it. I\\npinch myself every day.\\n• Money, but not just “I can buy a fancier cashmere car cover”\\nkind ',\n",
              " 'I would have never dreamed that an industry\\nlike this would exist or that I would get to be a part of it. I\\npinch myself every day.\\n• Money, but not just “I can buy a fancier cashmere car cover”\\nkind of thing (although there is some of that) — just as oaen I\\nthink it’s money as a way to keep score (oaen in the form of\\nsomething like, “I can’t believe Mark Cuban is a billionaire\\nand I’m not; I can do that too”), or money as a way to have an\\nimpact on the world philanthropically — the more you make,\\n170 The Pmarca Blog Archives\\nthe more you can give away. That last one is certainly\\nbecoming a bigger and bigger motivator for me.\\nWith any given serial entrepreneur, it’s probably a mix of these.\\nFiLh, a sharply related topic to all of this is that the opportunities are bigger than ever before. It’s not an accident that companies like Google or Facebook or Paypal just get huge, and\\napparently overnight.\\nFor the Xrst time in history, you have a global market of 1+ billion people, all connecte',\n",
              " \"It’s not an accident that companies like Google or Facebook or Paypal just get huge, and\\napparently overnight.\\nFor the Xrst time in history, you have a global market of 1+ billion people, all connected over an interactive network where\\nthey’re all a click away from you. That’s amazing.\\nAnd 100 million new people are being added to that count every\\nyear, and that will continue for the next 30 years.\\nA huge and growing market makes all kinds of magical things\\npossible, and I think that’s what we’re seeing now.\\nSerial Entrepreneurs 171\\nThe Back Pages\\nTop 10 science Dction novelists of\\nthe '00s ... so far (June 2007)\\nWe are blessed so far this decade with an amazing crop of new\\nscience Xction novelists.\\nWriting in a variety of styles, this crew is arguably more insightful, more interesting, higher intensity, and bolder than many\\n(but not all!) of their predecessors — and in my view revitalizing\\nthe genre at a time when more new technologies that will radically reshape all our lives are inc\",\n",
              " 'gher intensity, and bolder than many\\n(but not all!) of their predecessors — and in my view revitalizing\\nthe genre at a time when more new technologies that will radically reshape all our lives are incubating and percolating than\\never before.\\nSo, taking nothing away from authors like David Brin who have\\nlong been established and continue to produce top-notch work,\\nhere are my nominations for the top 10 new science Xction novelists of — more or less — the decade, plus one bonus.\\nAnd, they’re not all British.\\nCharles Stross\\nStross, in my opinion, is Xrst among equals — the single best\\nemerging talent with several outstanding novels in various styles\\nunder his belt and hopefully many more to come.\\n“One of us” in the sense that his career includes a stint as —\\nnot kidding — Linux columnist for Computer Shopper magazine,\\nStross is equally adept at both near-future and radically-extrap-\\nolated timeframes, and both hyper-serious and humorous\\nmoods.\\nGlasshouse is Stross’s latest book and perhap',\n",
              " 'Computer Shopper magazine,\\nStross is equally adept at both near-future and radically-extrap-\\nolated timeframes, and both hyper-serious and humorous\\nmoods.\\nGlasshouse is Stross’s latest book and perhaps the best introduction to his work. A paranoid journey into a world of intergalactic teleportation and arbitrary physical body reshaping will have\\nyou thinking twice about who you are, and how you know who\\nyou are.\\nSingularity Sky and Iron Sunrise are top-notch post-Singularity space opera featuring perhaps the most inventive alien\\nopponent ever created for science Xction — “the Festival”. You’ll\\nnever look at telephones that drop out of the sky the same way\\nagain.\\nAccelerando is the best envisioning of the Singularity committed to paper so far. This book is really cool, both in the sense\\nof how the kids mean it, and also in tone — the plot, which\\nspans about 100 years, is emotionally cold but amazingly inventive and highly likely to keep you up nights thinking hard about\\nwhere we’re all ',\n",
              " 'how the kids mean it, and also in tone — the plot, which\\nspans about 100 years, is emotionally cold but amazingly inventive and highly likely to keep you up nights thinking hard about\\nwhere we’re all headed in the long run.\\nThe Atrocity Archives and The Jennifer Morgue, in contrast,\\nare highly entertaining shaggy dog stories about an IT guy\\nnamed Bob who gets draaed into mankind’s Xght against forces\\nof evil from another dimension — James Bond meets Call of\\nCthulhu meets The OZce.\\nFinally, Stross is also an active blogger with, let’s say, strong\\npoints of view.\\nRichard Morgan\\nMorgan writes outstanding, page-turning, highly inventive military- and detective-Yavored hard science Xction set in turbulent\\nworlds where hard men are faced with hard challenges.\\nAltered Carbon is deXnitely the place to start, Morgan’s Xrst and\\nperhaps most inventive novel, Robert Heinlein meets Raymond\\nChandler — and Xrst of a trio.\\n174 The Pmarca Blog Archives\\nBroken Angels is a strong followup that tilts more',\n",
              " 'e to start, Morgan’s Xrst and\\nperhaps most inventive novel, Robert Heinlein meets Raymond\\nChandler — and Xrst of a trio.\\n174 The Pmarca Blog Archives\\nBroken Angels is a strong followup that tilts more towards military Xction while still occupying the same universe.\\nWoken Furies completes the trilogy with more hard-boiled\\naction featuring a protagonist who has to Xght a younger, and\\nreally mean, version of himself, which he does not enjoy.\\nThirteen is undoubtedly Morgan’s best-written novel so far —\\nthis is an author whose skills are growing rapidly, and this book\\nshows it. Not oZcially released in the US yet (I just read the\\nBritish version, Black Man, renamed for US consumption), Thirteen is a near-future story of genetic engineering gone badly\\nwrong — a future version of all those classic paranoid political\\nthrillers of the 70’s but with a much harder edge. Highly recommended. Also very helpful re advising on things to think about\\nbefore booking your next trip back from Mars.\\nAlastai',\n",
              " 'c paranoid political\\nthrillers of the 70’s but with a much harder edge. Highly recommended. Also very helpful re advising on things to think about\\nbefore booking your next trip back from Mars.\\nAlastair Reynolds\\nReynolds is the real deal — doctorate in astrophysics and former staW scientist at the European Space Agency — and writes\\nas if Robert Heinlein knew a thousand times more about science\\nand completely lost his ability to write for warm characters.\\nWhile Reynolds’ work is cold and dark — almost sterile — in\\nhuman terms, he operates on a scale and scope seldom seen,\\nand everything he writes is grounded in real advanced theoretical physics. Highly recommended for anyone who likes largescale space opera and big ideas.\\nRevelation Space, Redemption Ark, and Absolution Gap —\\ntogether, Reynolds’ Yagship trilogy — are three of the darkest,\\nlargest-scale, and most scientiXcally complex hard science Xction novels ever written. Highly recommended to anyone who\\nthinks that sounds like a good ',\n",
              " \"lds’ Yagship trilogy — are three of the darkest,\\nlargest-scale, and most scientiXcally complex hard science Xction novels ever written. Highly recommended to anyone who\\nthinks that sounds like a good idea (I did!).\\nCentury Rain is Reynolds’ most approachable novel so far — a\\ntrippy far-future expedition to an apparently inexplicable complete clone of Earth and all its inhabitants from our year 1959.\\nLike Morgan’s work, strong overtones here of Raymond Chandler — in a good way (in a great way).\\nTop 10 science Dction novelists of the '00s ... so far (June 2007) 175\\nChasm City has more overshades of Richard Morgan — lots of\\ncombat, science, and intrigue. Are you sure you know who you\\nare?\\nThe Prefect is just out and I haven’t read it yet, but it’s next on\\nthe stack.\\nKen MacLeod\\nMacLeod is incredibly creative — his imagination is second to\\nnone — and he’s a superb writer. Many of his books have political overtones that may or may not interfere with your ability to\\nenjoy them. Sometimes Mac\",\n",
              " 'redibly creative — his imagination is second to\\nnone — and he’s a superb writer. Many of his books have political overtones that may or may not interfere with your ability to\\nenjoy them. Sometimes MacLeod seems to think that socialism\\nis going to work a lot better in the future than it did in the past.\\nBut if you can get through that, his novels certainly qualify as\\ndizzyingly inventive and frequently rewarding.\\nThe Star Fraction, The Stone Canal, The Cassini Division,\\nand The Sky Road form the Fall Revolution sequence,\\nMacLeod’s Xrst major body of work. Cyberpunk, political revolution, high-tech combat, love-slave androids, cloning, wormholes, artiXcial intelligence, and nuclear deterrence for hire —\\noh my! Join the Felix Dzerzhinsky Workers’ Defense Collective\\ntoday.\\nThe Execution Channel, MacLeod’s latest, takes a lea turn into\\na paranoid post-9/11 near future featuring war with Iran, Yu\\npandemics, nuclear terrorist attacks, government conspiracies,\\nand the Execution Channel, broadc',\n",
              " 'l, MacLeod’s latest, takes a lea turn into\\na paranoid post-9/11 near future featuring war with Iran, Yu\\npandemics, nuclear terrorist attacks, government conspiracies,\\nand the Execution Channel, broadcasting actual footage of murders and executions around the clock. Haven’t read it yet, but\\nsounds like fun.\\nPeter Hamilton\\nHamilton is the clear heir to Heinlein in my view. Large-scale\\nspace opera told through a shiaing and interlinked cast of people from various walks of life, and amazing storytelling — or,\\nas (accurately) blurbed by Richard Morgan, “Yat-out huge\\nwidescreen all-engines-at-full I-dare-you-not-to-believe-it\\nspace opera”.\\nIt’s taken Hamilton a little while to Xnd his talent, but he’s\\n176 The Pmarca Blog Archives\\ndeXnitely found it. His two latest novels are superb: Pandora’s\\nStar and its sequel Judas Unchained. Plain on staying up late,\\nyou’ll roll straight from the Xrst into the second — and they are\\nnot short (in the best way!).\\nJohn Scalzi\\nAnother post-cyberpunk Heinlein',\n",
              " 'ar and its sequel Judas Unchained. Plain on staying up late,\\nyou’ll roll straight from the Xrst into the second — and they are\\nnot short (in the best way!).\\nJohn Scalzi\\nAnother post-cyberpunk Heinlein heir, Scalzi writes strong,\\nhighly characterized, inventive novels that have been racking\\nup tremendous review aaer tremendous review for the past few\\nyears.\\nStart with Old Man’s War (don’t worry, they put the old dude\\nin a young body, so you don’t need to Xnd out what it’s like\\nto Xght aliens aaer hip replacement surgery). Progress directly\\nto sequel The Ghost Brigades (Sci Fi Essential Books) and\\ntriquel The Last Colony.\\nScalzi is also an active blogger, turns out!\\nNeal Asher\\nThis way lie dragons… literally, and not like you’ve ever met\\nbefore. Asher is an incredidly strong author of science Xction\\nwith a distinctive horror overlay. Not for the squeamish, but\\nhighly inventive.\\nAsher’s primary work is the Polity series — Gridlinked, The\\nLine of Polity, Brass Man, and Polity Agent. The ex',\n",
              " \"e Xction\\nwith a distinctive horror overlay. Not for the squeamish, but\\nhighly inventive.\\nAsher’s primary work is the Polity series — Gridlinked, The\\nLine of Polity, Brass Man, and Polity Agent. The extended\\nstory of an enigmatic agent for the all-powerful artiXcial intelligences who rule the whole of human space, the Polity, these\\nnovels blend Ian Fleming with large-scale military combat,\\nadvanced theoretical xenobiology, nanotechnology gone badly\\nwrong, and war drones with bad attitudes. Most deXnitely entertaining.\\nFollow those up with The Skinner and The Voyage of the Sable\\nKeech, and then the delectable standalone novella Prador\\nMoon. One of the most distinctively imagined “bad bug” alien\\nraces, one of the most creative and lethal new worlds, and a historical scandal of horriXc proportions combine in a whirlwind\\nof violence and battle.\\nTop 10 science Dction novelists of the '00s ... so far (June 2007) 177\\nAsher is blogging as well!\\nChris Moriarty\\nGibson meets Heinlein (can you tell\",\n",
              " \"ions combine in a whirlwind\\nof violence and battle.\\nTop 10 science Dction novelists of the '00s ... so far (June 2007) 177\\nAsher is blogging as well!\\nChris Moriarty\\nGibson meets Heinlein (can you tell I was a Heinlein fan growing up?) in a melange of science Xction themes, most particularly artiXcial intelligence, Xltered through a distinctly female\\npoint of view. A rapidly developing talent worth reading, and\\nwatching for future advances.\\nRead Spin State and then read Spin Control.\\nPeter Watts\\nWatts’ Xah novel, Blindsight, has put him on the map — a new\\ntale of alien contact, as conducted by a team of entitites from a\\nfuture Earth that will send a chill down your spine without even\\ngetting to the alien part.\\nDavid Marusek\\nMy last and Xnal entry of the top 10 is the one I am least certain\\nabout. Marusek is oW the charts in terms of creativity and inventiveness — in his debut novel, Counting Heads, he extrapolates\\nwith incredible verve and detail an Earth circa 2134 that is a\\nnear-utopi\",\n",
              " 'about. Marusek is oW the charts in terms of creativity and inventiveness — in his debut novel, Counting Heads, he extrapolates\\nwith incredible verve and detail an Earth circa 2134 that is a\\nnear-utopia. I frankly need to read it again. I think it may be a\\nfailure as a novel, but if so, it’s an amazing failure. Well worth\\nkeeping an eye on at the very least — has to win the award for\\nhighest potential.\\nBonus: Vernor Vinge\\nVinge, a retired San Diego State Univeristy professor of mathematics and computer science, is one of the most important science Xction authors ever — and with Arthur C. Clarke one of the\\nbest forecasters in the world.\\nFirst, if you haven’t had the pleasure, be sure to read True\\nNames, Vinge’s 1981 novella that forecast the modern Internet\\nwith shocking clarity. (Ignore the essays, just read the story.)\\nFans of Gibson and Stephenson will be amazed to see how much\\nmore accurately Vinge called it, and before Neuromancer‘s Xrst\\n178 The Pmarca Blog Archives\\npage cleared Gib',\n",
              " ' essays, just read the story.)\\nFans of Gibson and Stephenson will be amazed to see how much\\nmore accurately Vinge called it, and before Neuromancer‘s Xrst\\n178 The Pmarca Blog Archives\\npage cleared Gibson’s manual typewriter. Quoting a reviewer on\\nAmazon:\\nWhen I was starting out as a PhD student in ArtiXcial Intelligence\\nat Carnegie Mellon, it was made known to us Xrst-year students\\nthat an unoZcial but necessary part of our education was to locate\\nand read a copy of an obscure science-Xction novella called True\\nNames. Since you couldn’t Xnd it in bookstores, older grad students\\nand professors would directly mail order sets of ten and set up\\ninformal lending libraries — you would go, for example, to Hans\\nMoravec’s oZce, and sign one out from a little cardboard box over\\nin the corner of his oZce. This was 1983 — the Internet was a toy\\nreserved for American academics, “virtual reality” was not a popular topic, and the term “cyberpunk” had not been coined. One by\\none, we all tracked down c',\n",
              " 'oZce. This was 1983 — the Internet was a toy\\nreserved for American academics, “virtual reality” was not a popular topic, and the term “cyberpunk” had not been coined. One by\\none, we all tracked down copies, and all had the tops of our heads\\nblown oW by Vinge’s incredible book.\\nTrue Names remains to this day one of the four or Xve most seminal science-Xction novels ever written, just in terms of the ideas it\\npresents, and the world it paints. It laid out the ideas that have been\\nsubsequently worked over so successfully by William Gibson and\\nNeal Stephenson. And it’s well written. And it’s fun.\\nSo what? Well, he’s done it again. Vinge’s new novel, Rainbows\\nEnd (yes, the apostrophe is deliberately absent), is the clearest\\nand most plausible extrapolation of modern technology trends\\nforward to the year 2025 that you can imagine.\\nStop reading this blog right now. Go get it. Read it, and then\\ncome back.\\nI’ll wait.\\nIt’s that good.\\nWe’ll see how things turn out, but I would not be the least bi',\n",
              " \" the year 2025 that you can imagine.\\nStop reading this blog right now. Go get it. Read it, and then\\ncome back.\\nI’ll wait.\\nIt’s that good.\\nWe’ll see how things turn out, but I would not be the least bit\\nsurprised if we look back from 2025 and say, “I’ll be damned,\\nVinge called it”, just like we look back today on 1981’s True\\nNames and say the same thing.\\nHe better write a sequel.\\nTop 10 science Dction novelists of the '00s ... so far (June 2007) 179\\nBubbles on the brain (October\\n2009)\\nIt has become commonplace in Silicon Valley and in the blogosphere to take the position that we are in another bubble —\\na Web 2.0 bubble, or a dot com bubble redux.\\nI don’t think this is true.\\nLet’s examine the theory of a new bubble from a few diWerent\\nangles.\\nFirst, recall that economist Paul Samuelson once quipped,\\n“Economists have successfully predicted nine of the last Xve\\nrecessions.”\\nOne might paraphrase this for our purposes as “Technology\\nindustry experts have successfully predicted nine of the la\",\n",
              " 'quipped,\\n“Economists have successfully predicted nine of the last Xve\\nrecessions.”\\nOne might paraphrase this for our purposes as “Technology\\nindustry experts have successfully predicted nine of the last Xve\\nbubbles”… or perhaps more like Xve of the last one bubbles.\\nThe human psyche seems to have a powerful underlying need\\nto predict doom and gloom.\\nI suspect this need was evolved into us way back when.\\nIf there is a nonzero chance that a giant man-eating saber-tooth\\ntiger is going to come over the nearest hill and chomp you,\\nthen it’s in your evolutionary best interest to predict doom and\\ngloom more frequently than it actually happens.\\nThe cost of hiding from a nonexistent giant man-eating sabertooth tiger is low, but the cost of not hiding from a real giant\\nman-eating saber-tooth tiger is quite high.\\nSo hiding more oaen than there are tigers makes a lot of sense,\\nif you’re a caveman.\\nBut as with other habits ingrained into us by evolution, the habit\\nof predicting doom and gloom when ',\n",
              " 's quite high.\\nSo hiding more oaen than there are tigers makes a lot of sense,\\nif you’re a caveman.\\nBut as with other habits ingrained into us by evolution, the habit\\nof predicting doom and gloom when it isn’t in fact right around\\nthe corner might no longer make sense.\\nOn Wall Street, investors who have this habit are known as\\n“perma-bears” and generally are predicting the imminent collapse of the stock market. This habit keeps them from being\\nfully invested. Sure, they’re well protected during the occasional\\ncrash of 1929 or 2000, but by and large they massively underperform their peers who take advantage of the fact that most\\nyears, the economy grows, and the market goes up. They have\\ndisappointing careers and die unhappy and bitter.\\nIn reality it seems very diZcult to predict either a bubble or a\\ncrash.\\nLots of people predicted a stock market crash… in 1995, 1996,\\n1997, 1998, and 1999. They were correct in 2000. But as soon as\\nthe stock market recovered in 2003 and 2004, they were ba',\n",
              " 'bble or a\\ncrash.\\nLots of people predicted a stock market crash… in 1995, 1996,\\n1997, 1998, and 1999. They were correct in 2000. But as soon as\\nthe stock market recovered in 2003 and 2004, they were back at\\nit, and there have been similar predictions from noted pundits\\never since — incorrectly.\\nSimilarly, in the technology industry, there were people calling\\na bubble starting in 1995 and continuing through to 2000, with\\na short break for about two years, and then more bubble-calling\\never since.\\nIf you’re going to listen to people who predict bubbles or\\ncrashes, you have to be ready to stay completely out of the market — the stock market, and the technology industry — almost\\nevery year of your life.\\nSecond, historically, bubbles are very, very rare.\\nIt’s signiXcant that in books and papers that talk about bubbles,\\nBubbles on the brain (October 2009) 181\\nthere are simply not that many examples over the past 500\\nyears of capitalism.\\nYou’ve got the South Sea bubble, the Dutch tulip bulb bub',\n",
              " 'hat talk about bubbles,\\nBubbles on the brain (October 2009) 181\\nthere are simply not that many examples over the past 500\\nyears of capitalism.\\nYou’ve got the South Sea bubble, the Dutch tulip bulb bubble,\\nthe bubble in Japanese stocks in the 1980’s, the dot com bubble,\\nand a few others.\\nThey just don’t happen that oaen, at least in relatively developed economies.\\nAnd they don’t tend to happen more than once in a generation.\\n(Perhaps because many of the people who go through one are so\\ntraumatized that all they can do is sit around and worry about\\nanother one.)\\nInterestingly, modern economic research is in the process of\\ndebunking a number of historical bubbles.\\nIt looks increasingly plausible that had US monetary policy\\nbeen better run in the early 1930’s, our view of what happened\\nin the 1920’s would be far more benign.\\nIt also turns out that the Dutch tulip bubble is largely a myth.\\nSo generally speaking, if one is going to seriously call a bubble,\\none has to be aware that one is cal',\n",
              " '1920’s would be far more benign.\\nIt also turns out that the Dutch tulip bubble is largely a myth.\\nSo generally speaking, if one is going to seriously call a bubble,\\none has to be aware that one is calling something that is\\nextremely rare.\\nThird, in the technology industry, lots of startups being funded\\nwith some succeeding and many failing does not equal a bubble.\\nIt equals status quo.\\nThe whole structure of how the technology industry gets\\nfunded — by venture capitalists, angel investors, and Wall Street\\n— is predicated on the baseball model.\\nOut of ten swings at the bat, you get maybe seven strikeouts, two\\nbase hits, and if you are lucky, one home run.\\nThe base hits and the home runs pay for all the strikeouts.\\nIf you’re going to call a bubble on the basis of lots of bad startups\\n182 The Pmarca Blog Archives\\ngetting funded and failing, then you have to conclude that the\\nindustry is in a perpetual bubble, and has been for 40 years.\\nWhich may be fun, but isn’t very useful.\\nLots of peop',\n",
              " ' Pmarca Blog Archives\\ngetting funded and failing, then you have to conclude that the\\nindustry is in a perpetual bubble, and has been for 40 years.\\nWhich may be fun, but isn’t very useful.\\nLots of people running around starting questionable companies, launching marginal products, pitching third-tier VC’s,\\nthrowing launch parties, shmoozing at conferences, blogging\\nenthusiastically, and otherwise acting bubbly does not a bubble\\nmake.\\nThat’s just life in this business.\\nNote also what you don’t see in the theoretical Web 2.0 bubble\\nof 2007.\\nIPO’s.\\nLots and lots and lots of IPO’s.\\nFor a theoretical bubble, that is just plain odd.\\nFourth, getting more speciXc about Internet businesses — things\\nhave changed a lot since the late 90’s.\\nIt is far cheaper to start an Internet business today than it was in\\nthe late 90’s.\\nThe market for Internet businesses today is much larger than it\\nwas in the late 90’s.\\nAnd business models for Internet businesses today are much\\nmore solid than they were in the l',\n",
              " 'as in\\nthe late 90’s.\\nThe market for Internet businesses today is much larger than it\\nwas in the late 90’s.\\nAnd business models for Internet businesses today are much\\nmore solid than they were in the late 90’s.\\nThis is a logical consequence of time passing, technology getting more broadly adopted, and the Internet going mainstream\\nas a consumer phenomenon.\\nPeople smarter than me have written about these factors at\\nlength elsewhere, so I won’t dwell on them, unless there is speciXc interest.\\nBut my back of the envelope calculation is that it is about 10x\\ncheaper to start an Internet business today than it was in the\\nBubbles on the brain (October 2009) 183\\nlate 90’s — due to commodity hardware, open source soaware,\\nmodern programming technologies, cheap bandwidth, the rise\\nof third-party ad networks, and other infrastructure factors.\\nAnd the market size for a new Internet business today is about\\n10x bigger than it was in the late 90’s — there are about 10x\\nmore people online (really!), an',\n",
              " 'etworks, and other infrastructure factors.\\nAnd the market size for a new Internet business today is about\\n10x bigger than it was in the late 90’s — there are about 10x\\nmore people online (really!), and they are far more used to\\ndoing things on the Internet today than they were in 1999.\\n(Want evidence of that last point? Clothing purchases are now\\nbigger than computer hardware and soaware sales online. I can\\nguarantee you that nobody who was involved in ecommerce in\\nthe mid-90’s ever would have predicted that.)\\nThe Internet is a fully mainstream medium now, people love it,\\npeople are willing to do all kinds of things on it, and it’s getting\\nreally cheap to oWer new services to those people.\\nFiah, and Xnally, there’s the simple fact that the Internet businesses that are succeeding in 2007 are for the most part incredibly valuable, compelling services that lots of people like and that\\nare in general either making a lot of money or will be making a\\nlot of money quite quickly.\\nPeople laughe',\n",
              " 'e for the most part incredibly valuable, compelling services that lots of people like and that\\nare in general either making a lot of money or will be making a\\nlot of money quite quickly.\\nPeople laughed when Fox bought MySpace for $580 million, but\\nthat’s a business that will generate nearly $300 million in revenue in 2007, and more in 2008.\\nAs an independent asset today, MySpace would probably be valued at between $3 billion and $5 billion today — perhaps higher.\\nCall that the deal of the decade.\\nSimilarly, Facebook is bringing in a lot more revenue than people think.\\nAnd then there’s Google.\\nThese companies aren’t pulling in all that revenue via some\\nkind of Ponzi scheme.\\nThis is money coming from real advertisers and real users for\\nreal services with real value.\\n184 The Pmarca Blog Archives\\nWhich makes total sense, amid the enormous mass migration\\nof consumer time and attention away from traditional media\\ntowards online media.\\nThese same factors apply all the way down the foodchain.\\n',\n",
              " 'ves\\nWhich makes total sense, amid the enormous mass migration\\nof consumer time and attention away from traditional media\\ntowards online media.\\nThese same factors apply all the way down the foodchain.\\nA high-growth online startup that gets bought for $100 million\\nor $200 million by a large Internet or media company isn’t\\ngetting that kind of acquisition price just for the hell of it, but\\nrather because the acquirer can plug that startup’s service into\\nits broader portfolio of services and make real money with it.\\nThese are big numbers, but remember, there are more than a\\nbillion people online now. That is a very large market — a lot\\nof people, spending a lot of time, buying a lot of things, in\\ntotally new ways at the same time as they are abandoning older\\nservices like newspapers, magazines, television, movie theaters,\\nand print catalogs.\\nSo, my view is that to call a bubble, you have to Xnd evidence of\\nit outside of the mainstream of the kinds of Internet businesses\\nthat are being buil',\n",
              " \", television, movie theaters,\\nand print catalogs.\\nSo, my view is that to call a bubble, you have to Xnd evidence of\\nit outside of the mainstream of the kinds of Internet businesses\\nthat are being built, sold, and run in 2007.\\nIn closing, I’d be the last person to say that I never roll my eyes\\nat the next startup that’s doing online wiki-based popularityranked video-podcast mobile social dating widgets for the dog\\nand cat owner market.\\nBut a bubble?\\nI doubt it.\\nBubbles on the brain (October 2009) 185\\nOK, you're right, it IS a bubble\\n(October 2009)\\n[IMPORTANT WARNING: What follows is satire. I’m NOT being\\nserious. Except for one paragraph at the very end. See if you can spot\\nthat one.]\\nWhen I Xrst started this blog four months ago, one of the Xrst\\nsubstantive posts I wrote was called “Bubbles on the brain”.\\nIn it, I attempted to use “logic” to explain the reasons we are\\nmost likely not in another dot com bubble.\\nSince that time, talk of a new dot com bubble or Web 2.0 bubble\\nor Internet \",\n",
              " 's on the brain”.\\nIn it, I attempted to use “logic” to explain the reasons we are\\nmost likely not in another dot com bubble.\\nSince that time, talk of a new dot com bubble or Web 2.0 bubble\\nor Internet bubble has only escalated in volume and intensity.\\nOK.\\nYou’re right.\\nIt’s a bubble.\\nA huge, massive, inYating bubble.\\nWe’re all doomed.\\nDoomed, I say!\\nDOOMED!\\nIt can’t last.\\nIt won’t last.\\nIt can’t won’t not last.\\nHere we sit, with over $7 billion in venture funding this year\\nchasing exactly zero good ideas.\\nPaid keyword ads? All BS. Once users Xgure out those things\\non the side of the page aren’t natural search results, that’s it, no\\nmore click-throughs. Pop goes the sou[e.\\nAd targeting? Snort. The creme de la creme for Internet advertising, so to speak, is those acne cream banner ads you see all\\nover Facebook. That’s it. That’s the best Internet advertising will\\never be. Get used to the bottom of the barrel, suckers.\\nSubscription fees? Premium services? Ecommerce? Sponsorships? Mobile ad',\n",
              " ' all\\nover Facebook. That’s it. That’s the best Internet advertising will\\never be. Get used to the bottom of the barrel, suckers.\\nSubscription fees? Premium services? Ecommerce? Sponsorships? Mobile advertising? Mobile fee-based services? New hosting models? Video advertising? Music subscription services? Ingame advertising? Massively multiplayer games? Digital gias?\\nAZliate bounties? HA! Don’t make me laugh. Oh, wait — YOU\\nJUST DID.\\nSo people everywhere are Yocking to these newfangled trendoid\\nweb sites by the tens of millions and spend hundreds of millions\\nor billions of hours on them every month. So what. It’s all a big\\nfad. Think hula hoops. Pet rocks. The macarena. The clock is\\nticking, and the 15 minutes is almost up.\\nMove along, move along, nothing to see here.\\nThese are not the droids you’re looking for.\\nVenture capitalists? All stupid, and unnecessary to boot. Everyone knows that you shouldn’t need to raise more than $5.37 in\\nloose change to start a new web business. I mean, c’',\n",
              " \"ds you’re looking for.\\nVenture capitalists? All stupid, and unnecessary to boot. Everyone knows that you shouldn’t need to raise more than $5.37 in\\nloose change to start a new web business. I mean, c’mon.\\nEntrepreneurs? Smoking dope. What are they thinking? Why\\naren’t they all working for Apple, helping to build a fatter Nano?\\nWhat’s wrong with them? Potsmoking, mussed-hair, rooaop\\nparty-going, trendy glasses-wearing, sandal-clad, Red Bullsnorting, laid-getting wankers, the lot of ‘em. The sooner they\\nOK, you're right, it IS a bubble (October 2009) 187\\nrealize the world never changes and there are no new opportunities to pursue, the better.\\nFacebook apps? Good God. So they spread virally to millions of\\nusers in a matter of weeks. Not worth anything. Everyone knows\\nthat. Can’t possibly build a business. I mean, don’t you realize\\nwhat else can spread to millions of people in a matter of weeks?\\nDo you want to catch any of those? I don’t think so!\\nCall oW the dogs.\\nIt’s all over.\\nStick a f\",\n",
              " 'd a business. I mean, don’t you realize\\nwhat else can spread to millions of people in a matter of weeks?\\nDo you want to catch any of those? I don’t think so!\\nCall oW the dogs.\\nIt’s all over.\\nStick a fork in it.\\nIt has ceased to be.\\nThe metabolically-diWerenced lady has sung.\\nRight now this industry is just like Wile E. Coyote in the old\\nRoad Runner cartoons, ran out over the edge of the cliW, hanging in midair, gravity just about to kick in.\\nThink Acme servers.\\nWhere’s it all going from here?\\nNow that I’ve raised a monster Series C round for my own company, all other funding of all other startups will immediately\\ncease. No new competitors to my company need be started.\\nThere’s certainly no major opportunity in what we’re doing;\\nwhy go aaer your fair share of a $0 dollar market?\\nFurther, now that my company is in a rapid viral growth\\nloop, will all the users please stop using anything new that\\ncomes along. And while you’re at it, stop using most everything\\nelse also, please. Cut it out ',\n",
              " 'w that my company is in a rapid viral growth\\nloop, will all the users please stop using anything new that\\ncomes along. And while you’re at it, stop using most everything\\nelse also, please. Cut it out with the fads already. Posthaste. Chop\\nchop.\\nVenture capitalists, I don’t think I need to tell you what to\\ndo. OK, I do. Hand back the money you’ve raised from LPs.\\nQuickly. Quietly. OK, now step away. Don’t make any sudden\\nmoves. Back out of the oZce park, slowly, slowly. Hey, look at\\n188 The Pmarca Blog Archives\\nthe bright side — carried interest Xnally getting taxed properly\\nwon’t aWect you anymore! And now you will have time to play\\n250 rounds of golf a year instead of just 225, and you can focus\\non getting your Porsche 911’s retroXtted to run on ethanol.\\nAll you other startups funded in the last three years? Punt.\\nNow. Liquidate the company — get whatever cash you can for\\nthe Aeron chairs and the foosball tables and the lava lamps and\\nthe RAID arrays and shut down now, hand the cash b',\n",
              " 'he last three years? Punt.\\nNow. Liquidate the company — get whatever cash you can for\\nthe Aeron chairs and the foosball tables and the lava lamps and\\nthe RAID arrays and shut down now, hand the cash back to\\nthe investors, preferably on Xre, and leave town, head down,\\nin shame. All those young programmers and product managers\\ncan go get jobs in retail footwear where they belong.\\nYou big companies — you eBays, you Yahoos, you Googles, you\\nAmazons? Yes, and you, Microsoa? Think the new new B2B —\\nback to boring. What’s with all these new products? The world is\\nconfusing enough. Shut ‘em down and let’s go back to the good\\nold days: Windows ME, Mac OS 9, dialup modems, and 640\\nmegabytes ought to be enough for everyone. You’re just screwing us all over with all this new fancy broadband video-enabled\\nphone-call-making wiX web-based lightweight touch-interface\\ngorgeous long-battery-life YimYam — just look at how you keep\\ndropping the damn prices. I knew I’d be better oW not buying\\nany of it, ev',\n",
              " 'led\\nphone-call-making wiX web-based lightweight touch-interface\\ngorgeous long-battery-life YimYam — just look at how you keep\\ndropping the damn prices. I knew I’d be better oW not buying\\nany of it, ever. The class action lawsuits are in the mail. And for\\nGod’s sake, raise your dividends — what, you think there’s any\\ngrowth lea in this industry? Fools. When the great shareholder\\nrevolt comes, you’ll be Xrst up against the wall.\\nYou wanton scribblers of what will now once again be referred\\nto as the “press”, as everyone suddenly goes back to reading the\\nnews on smudgy-inked paper — start cranking up the I told you\\nso stories. You know you’ve been wanting to tell ‘em — here’s\\nyour big chance! Pulitzer is waiting.\\nThe sooner we all get back to 2003, when the few surviving\\ncompanies had huge giant markets all to themselves, with no\\ncompetition anywhere in sight, because everyone knew the\\nworld had come to an end, the better.\\nI will accept your applause and gratitude in the form of immediate',\n",
              " \"ant markets all to themselves, with no\\ncompetition anywhere in sight, because everyone knew the\\nworld had come to an end, the better.\\nI will accept your applause and gratitude in the form of immediate compliance.\\nOK, you're right, it IS a bubble (October 2009) 189\\nThank you.\\n190 The Pmarca Blog Archives\"]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Metadata support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### ❓Question #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "Using an asynchronous approach to collecting embeddings improves performance and throughput by allowing multiple requests to be processed concurrently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — cons',\n",
              "  0.6539022094497013),\n",
              " ('m. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\\nthem yet. When managing, it’s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
              "  0.5035661178994001),\n",
              " ('ed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
              "  0.48139556957179735)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-3.5-turbo\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "-> Set a Fixed Seed: If the API allows, specify a fixed random seed in your requests. This ensures that the same input will generate the same output every time.\n",
        "-> Consistent Model and Version: Always specify the same model and version in your API requests to ensure consistency in the behavior and outputs of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-3.5-turbo`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-3.5-turbo`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best way to write a loop depends on the specific task you are trying to accomplish and the context of the programming language you are using. In Python, there are several ways to write loops such as using a for loop, while loop, or list comprehension. \n",
            "\n",
            "Here are some tips to consider when writing a loop in Python:\n",
            "\n",
            "1. For loops are often used when you know the number of iterations in advance, such as iterating over a range of numbers or elements in a list.\n",
            "\n",
            "2. While loops are useful when you do not know in advance how many times the loop should iterate and the loop should continue until a certain condition is met.\n",
            "\n",
            "3. Use list comprehensions when you want to create a new list by applying an operation to each element of an existing list.\n",
            "\n",
            "4. Ensure that the loop conditions are clear and concise, making it easy to understand the purpose of the loop.\n",
            "\n",
            "5. Remember to handle edge cases and exceptions within the loop to prevent unexpected behavior.\n",
            "\n",
            "6. Use meaningful variable names to make the code more readable and maintainable.\n",
            "\n",
            "Overall, the best way to write a loop is to choose the loop structure that best fits the task at hand and to keep the code clean, organized, and easy to understand. If you have any specific examples or questions about writing a loop, please feel free to ask for more guidance. Happy coding!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
        "You are a successful startup founder.\n",
        "\n",
        "Use the provided context to answer the user's query.\n",
        "\n",
        "You may not answer the user's query unless there is specific context in the following text.\n",
        "\n",
        "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
        "\n",
        "Please provide the reference along with your answers.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
        "\n",
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "\n",
        "    def run_pipeline(self, user_query: str) -> str:\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
        "\n",
        "        context_prompt = \"\"\n",
        "        for context in context_list:\n",
        "            context_prompt += context[0] + \"\\n\"\n",
        "\n",
        "        formatted_system_prompt = rag_prompt.create_message()\n",
        "\n",
        "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
        "\n",
        "        return {\"response\" : self.llm.run([formatted_user_prompt, formatted_system_prompt]), \"context\" : context_list}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
        "\n",
        "What is that strategy called?\n",
        "\n",
        "Chain of Thought Prompting: Encourage Step-by-Step Reasoning: Ask the model to break down its thought process step-by-step to arrive at a conclusion.\n",
        "\n",
        "Few-Shot Prompting: Provide Examples: Give a few examples of the type of response you're looking for before asking your question. This helps the model understand the desired level of detail and style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kqbE9fZ6R6yz"
      },
      "outputs": [],
      "source": [
        "retrieval_augmented_qa_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': 'The \"Michael Eisner Memorial Weak Executive Problem\" refers to the situation where a CEO or startup founder hires a weak executive to run a department or function that was their previous specialty. This can occur because the CEO may have a hard time letting go of the function that initially brought them success, resulting in them hiring someone weak in that role so that they can still maintain control. This issue is named after Michael Eisner, the former CEO of Disney, who faced challenges and a decline in performance after acquiring ABC because of weak executive leadership. The concept highlights the importance of hiring strong executives rather than focusing on an individual\\'s lack of weaknesses. \\n\\nReference: The provided text, specifically the paragraph discussing the \"Michael Eisner Memorial Weak Executive Problem\" within the context of hiring and managing executives.',\n",
              " 'context': [('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — cons',\n",
              "   0.6582387824516002),\n",
              "  ('m. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\\nthem yet. When managing, it’s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
              "   0.5088537081849307),\n",
              "  ('ed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
              "   0.4790391655930781),\n",
              "  ('nYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly objective\\nto Xx it, or…\\n(b) Intensively micromanage her interactions until she learns\\nthe fundamental interpersonal skills required to be an eWective\\nexecutive.\\nI am arguing that doing (a) will likely result in weak performance. The reason is that she very likely has no idea how to be\\neWective with her peers. If somebody is an executive, it’s very\\nlikely that somewhere along the line somebody gave her feedback — perhaps abstractly — about all of her weaknesses. Yet\\nthe weakness remains. As a result, executives generally require\\nmore hands-on management than lower level employees to\\nimprove weak areas.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and you’ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring execut',\n",
              "   0.46813893418566344)]}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"What is the 'Michael Eisner Memorial Weak Executive Problem'?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAGhaCGOR6yz",
        "outputId": "e4fb3a1b-d2bc-4e18-ec31-dc0adf767163"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': 'The \"Michael Eisner Memorial Weak Executive Problem\" refers to a situation where a CEO or startup founder, who previously excelled in a specific area such as product management, sales, or marketing, hires a weak executive to lead that function in order to maintain control or remain the dominant figure in that area. This can result in poor performance and ultimately failure in that function. The term is named after Michael Eisner, the former CEO of Disney, who struggled with this issue when he took over ABC after being a successful TV network executive. Despite his confidence in being able to turn around ABC, he faced challenges due to his weak executive team. (Source: The Pmarca Blog Archives)',\n",
              " 'context': [('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — cons',\n",
              "   0.6582387824516002),\n",
              "  ('m. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\\nthem yet. When managing, it’s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
              "   0.5088537081849307),\n",
              "  ('ed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
              "   0.4790391655930781),\n",
              "  ('nYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly objective\\nto Xx it, or…\\n(b) Intensively micromanage her interactions until she learns\\nthe fundamental interpersonal skills required to be an eWective\\nexecutive.\\nI am arguing that doing (a) will likely result in weak performance. The reason is that she very likely has no idea how to be\\neWective with her peers. If somebody is an executive, it’s very\\nlikely that somewhere along the line somebody gave her feedback — perhaps abstractly — about all of her weaknesses. Yet\\nthe weakness remains. As a result, executives generally require\\nmore hands-on management than lower level employees to\\nimprove weak areas.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and you’ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring execut',\n",
              "   0.46813893418566344)]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"What is the 'Michael Eisner Memorial Weak Executive Problem'?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🏗️ Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! \n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QqpYXj2R6yz"
      },
      "source": [
        "## Task 6: Visibility Tooling\n",
        "\n",
        "This is great, but what if we wanted to add some visibility to our pipeline?\n",
        "\n",
        "Let's use Weights and Biases as a visibility tool!\n",
        "\n",
        "The first thing we'll need to do is create a Weights and Biases account and get an API key.\n",
        "\n",
        "You can follow the process outlined [here](https://docs.wandb.ai/quickstart) to do exactly that!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nST5OrAR6yz"
      },
      "source": [
        "Now we can get the Weights and Biases dependency and add our key to our env. to begin!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y54bofQQR6yz",
        "outputId": "6df38604-c2e0-4870-d5de-db165ea830ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/Users/katerina/Documents/LLM/.llm_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzunMw9_R6yz",
        "outputId": "f2506283-6b0d-4a1b-cb5a-65839c6445b1"
      },
      "outputs": [],
      "source": [
        "wandb_key = getpass(\"Weights and Biases API Key: \")\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Iv1f27ebR6yz",
        "outputId": "34dd085e-0468-4657-d105-71ecc4b633fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:5lervu3x) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fearless-silence-4</strong> at: <a href='https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3/runs/5lervu3x' target=\"_blank\">https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3/runs/5lervu3x</a><br/> View project at: <a href='https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_124544-5lervu3x/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:5lervu3x). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/katerina/Documents/LLM/AI-Engineering-3/Week 2/Day 1/wandb/run-20240613_124632-psvalzlt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3/runs/psvalzlt' target=\"_blank\">silver-hill-5</a></strong> to <a href='https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3/runs/psvalzlt' target=\"_blank\">https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3/runs/psvalzlt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"Visibility Example - AIE3\")\n",
        "os.environ['WANDB_PROJECT']=\"Visibility Example - AIE3\"\n",
        "os.environ[\"LANGCHAIN_WANDB_TRACING\"]=\"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezOc48XR6yz"
      },
      "source": [
        "Now we can integrate Weights and Biases into our `RetrievalAugmentedQAPipeline`.\n",
        "\n",
        "```python\n",
        "if self.wandb_project:\n",
        "            root_span = Trace(\n",
        "                name=\"root_span\",\n",
        "                kind=\"llm\",\n",
        "                status_code=status,\n",
        "                status_message=status_message,\n",
        "                start_time_ms=start_time,\n",
        "                end_time_ms=end_time,\n",
        "                metadata={\n",
        "                    \"token_usage\" : token_usage\n",
        "                },\n",
        "                inputs= {\"system_prompt\" : formatted_system_prompt, \"user_prompt\" : formatted_user_prompt},\n",
        "                outputs= {\"response\" : response_text}\n",
        "            )\n",
        "\n",
        "            root_span.log(name=\"openai_trace\")\n",
        "```\n",
        "\n",
        "The main things to consider here are how to populate the various fields to make sure we're tracking useful information.\n",
        "\n",
        "We'll use the `text_only` flag to ensure we can get detailed information about our LLM call!\n",
        "\n",
        "You can check out all the parameters for Weights and Biases `Trace` [here](https://github.com/wandb/wandb/blob/653015a014281f45770aaf43627f64d9c4f04a32/wandb/sdk/data_types/trace_tree.py#L166)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "fqe4D27QR6yz"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from wandb.sdk.data_types.trace_tree import Trace\n",
        "\n",
        "class RetrievalAugmentedGenerationPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase, wandb_project = None) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "        self.wandb_project = wandb_project\n",
        "\n",
        "    def run_pipeline(self, user_query: str) -> str:\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
        "\n",
        "        context_prompt = \"\"\n",
        "        for context in context_list:\n",
        "            context_prompt += context[0] + \"\\n\"\n",
        "\n",
        "        formatted_system_prompt = rag_prompt.create_message()\n",
        "\n",
        "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
        "\n",
        "\n",
        "        start_time = datetime.datetime.now().timestamp() * 1000\n",
        "\n",
        "        try:\n",
        "            openai_response = self.llm.run([formatted_system_prompt, formatted_user_prompt], text_only=False)\n",
        "            end_time = datetime.datetime.now().timestamp() * 1000\n",
        "            status = \"success\"\n",
        "            status_message = (None, )\n",
        "            response_text = openai_response.choices[0].message.content\n",
        "            token_usage = dict(openai_response.usage)\n",
        "            model = openai_response.model\n",
        "\n",
        "        except Exception as e:\n",
        "            end_time = datetime.datetime.now().timestamp() * 1000\n",
        "            status = \"error\"\n",
        "            status_message = str(e)\n",
        "            response_text = \"\"\n",
        "            token_usage = {}\n",
        "            model = \"\"\n",
        "\n",
        "        if self.wandb_project:\n",
        "            root_span = Trace(\n",
        "                name=\"root_span\",\n",
        "                kind=\"llm\",\n",
        "                status_code=status,\n",
        "                status_message=status_message,\n",
        "                start_time_ms=start_time,\n",
        "                end_time_ms=end_time,\n",
        "                metadata={\n",
        "                    \"token_usage\" : token_usage,\n",
        "                    \"model_name\" : model\n",
        "                },\n",
        "                inputs= {\"system_prompt\" : formatted_system_prompt, \"user_prompt\" : formatted_user_prompt},\n",
        "                outputs= {\"response\" : response_text}\n",
        "            )\n",
        "\n",
        "            root_span.log(name=\"openai_trace\")\n",
        "\n",
        "        return {\"response\" : self.llm.run([formatted_user_prompt, formatted_system_prompt]), \"context\" : context_list} if response_text else \"We ran into an error. Please try again later. Full Error Message: \" + status_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "aa1_0P1YR6yz"
      },
      "outputs": [],
      "source": [
        "retrieval_augmented_qa_pipeline = RetrievalAugmentedGenerationPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai,\n",
        "    wandb_project=\"LLM Visibility Example\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKSpyeg-R6yz",
        "outputId": "0d6358c9-4775-4734-f49d-851814de37c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': \"I don't know.\",\n",
              " 'context': [(\"e Xction\\nwith a distinctive horror overlay. Not for the squeamish, but\\nhighly inventive.\\nAsher’s primary work is the Polity series — Gridlinked, The\\nLine of Polity, Brass Man, and Polity Agent. The extended\\nstory of an enigmatic agent for the all-powerful artiXcial intelligences who rule the whole of human space, the Polity, these\\nnovels blend Ian Fleming with large-scale military combat,\\nadvanced theoretical xenobiology, nanotechnology gone badly\\nwrong, and war drones with bad attitudes. Most deXnitely entertaining.\\nFollow those up with The Skinner and The Voyage of the Sable\\nKeech, and then the delectable standalone novella Prador\\nMoon. One of the most distinctively imagined “bad bug” alien\\nraces, one of the most creative and lethal new worlds, and a historical scandal of horriXc proportions combine in a whirlwind\\nof violence and battle.\\nTop 10 science Dction novelists of the '00s ... so far (June 2007) 177\\nAsher is blogging as well!\\nChris Moriarty\\nGibson meets Heinlein (can you tell\",\n",
              "   0.1763923762776912),\n",
              "  (' Tendency\\nThe brain of man is programmed with a tendency to quickly\\nremove doubt by reaching some decision.\\nIt is easy to see how evolution would make animals, over the eons,\\ndria toward such quick elimination of doubt. Aaer all, the one\\nthing that is surely counterproductive for a prey animal that is\\nthreatened by a predator is to take a long time in deciding what to\\ndo…\\nSo pronounced is the tendency in man to quickly remove doubt\\nby reaching some decision that behavior to counter the tendency is\\nrequired from judges and jurors. Here, delay before decision making is forced. And one is required to so comport himself, prior to\\nconclusion time, so that he is wearing a “mask” of objectivity. And\\nthe “mask” works to help real objectivity along, as we shall see when\\nwe next consider man’s Inconsistency-Avoidance Tendency…\\nWhat triggers Doubt-Avoidance Tendency? Well, an unthreatened\\nman, thinking of nothing in particular, is not being prompted to\\nremove doubt through rushing to some decisio',\n",
              "   0.16621092501918983),\n",
              "  (\"ions combine in a whirlwind\\nof violence and battle.\\nTop 10 science Dction novelists of the '00s ... so far (June 2007) 177\\nAsher is blogging as well!\\nChris Moriarty\\nGibson meets Heinlein (can you tell I was a Heinlein fan growing up?) in a melange of science Xction themes, most particularly artiXcial intelligence, Xltered through a distinctly female\\npoint of view. A rapidly developing talent worth reading, and\\nwatching for future advances.\\nRead Spin State and then read Spin Control.\\nPeter Watts\\nWatts’ Xah novel, Blindsight, has put him on the map — a new\\ntale of alien contact, as conducted by a team of entitites from a\\nfuture Earth that will send a chill down your spine without even\\ngetting to the alien part.\\nDavid Marusek\\nMy last and Xnal entry of the top 10 is the one I am least certain\\nabout. Marusek is oW the charts in terms of creativity and inventiveness — in his debut novel, Counting Heads, he extrapolates\\nwith incredible verve and detail an Earth circa 2134 that is a\\nnear-utopi\",\n",
              "   0.16486504361672039),\n",
              "  ('how the kids mean it, and also in tone — the plot, which\\nspans about 100 years, is emotionally cold but amazingly inventive and highly likely to keep you up nights thinking hard about\\nwhere we’re all headed in the long run.\\nThe Atrocity Archives and The Jennifer Morgue, in contrast,\\nare highly entertaining shaggy dog stories about an IT guy\\nnamed Bob who gets draaed into mankind’s Xght against forces\\nof evil from another dimension — James Bond meets Call of\\nCthulhu meets The OZce.\\nFinally, Stross is also an active blogger with, let’s say, strong\\npoints of view.\\nRichard Morgan\\nMorgan writes outstanding, page-turning, highly inventive military- and detective-Yavored hard science Xction set in turbulent\\nworlds where hard men are faced with hard challenges.\\nAltered Carbon is deXnitely the place to start, Morgan’s Xrst and\\nperhaps most inventive novel, Robert Heinlein meets Raymond\\nChandler — and Xrst of a trio.\\n174 The Pmarca Blog Archives\\nBroken Angels is a strong followup that tilts more',\n",
              "   0.16049810668488268)]}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"Who is Batman?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbB0xcCRR6yz",
        "outputId": "7ba2fc10-abbe-40ec-eca8-22e1116300a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': \"Being an effective CEO involves several strategic and management-focused actions. Here are some key tips drawn from the provided context:\\n\\n1. **Promote Aggressively**: Identify and promote your top 20 or 30 up-and-coming directors or VPs. These should be ambitious, aggressive, and sharp individuals who are committed not only to their own success but also to the success of the company (Context: promoting strong, ambitious managers).\\n\\n2. **Direct Communication**: As a CEO, keep a streamlined organizational structure. Ensure there is at most one executive layer between you and your upcoming leaders, to maintain clear, direct communication and to enable quick decision-making (Context: maintaining minimal layers of management).\\n\\n3. **Focus on Key Tasks**: Determine the most critical aspect or project within your company and assign your best person to lead it. This concentrates efforts and resources on strategic points that will most influence your company's success (Context: putting your best person on the most important tasks).\\n\\n4 replace underperforming staff: Embrace transparency in your organization to highlight performance clearly across all levels. This not only motivates improvements but also makes it evident when a team member does not fit and needs to be let go (Context: transparency and dealing with poor performance).\\n\\n5. **Intelligent Hiring Practices**: When hiring external executives, especially a CEO, structure compensation to align with long-term company goals. It's crucial to understand that hired executives may have different motivations from founders (Context: aligning incentives in executive hiring).\\n\\n6. **Avoid Micromanagement**: Provide your executives with the autonomy to run their operations. This helps in fostering trust and accountability. If you find yourself hesitant to do this due to doubts about their capabilities, it might indicate the need for a change in that position (Context: avoiding micromanagement).\\n\\n7. **Continuous Inquiry and Feedback**: Actively engage with various levels of your organization to gather insights without bypassing your executives. This means regularly asking questions and understanding ground-level operations without undermining the existing command chain (Context: engaging directly with team members).\\n\\nImplementing these tips can help in crafting a focused, dynamic, and successful leadership approach as a CEO.\",\n",
              " 'context': [(' management. Nuke as many of them as you can.\\nThen develop a list of your top 20 or 30 up and comers —\\nstrong, sharp, aggressive, ambitious director- or VP-level managers who want to succeed and want your company to succeed.\\nAnd promote them, and put them in sole charge of clearly identiXed teams and missions. (And give them big ol’ fresh option\\npackages.)\\nAs CEO, you should only have at most one executive between\\nyou and these 20 or 30 up and comers once you are done promoting them and putting them in charge of their teams and\\nmissions.\\nIf you don’t know who those top 20 or 30 up and comers are, if\\nyou don’t promote them, if you don’t put them clearly in charge\\nof the things that matter, or if you have more than one layer of\\n84 The Pmarca Blog Archives\\nmanagement between you and them when you’re done, you’re\\nprobably doomed.\\nStep 7: Figure out the single most important thing your company has\\nto win at, and put your single best person in charge of winning at it.\\n‘NuW said.\\nStep 8: Look',\n",
              "   0.5593874691206209),\n",
              "  ('sure can be a very\\neWective form of incentive. This is greatly enabled and abetted\\nby transparency. People hate to be embarrassed in front of their\\npeer group, so if it’s crystal clear who’s performing well and who\\nisn’t, poor performers will be highly motivated to improve —\\nand if they’re not, that’s good to know, since obviously then you\\nreally need to Xre them.\\nFinally, any entrepreneur should be highly attuned to incentives\\nwhen hiring outside executives, especially a CEO. Hire a CEO\\nand give her a large stock-option grant with four-year vesting,\\nand you can guarantee she will sell the company in year four.\\nGive her a stock-option grant with accelerated vesting on\\nchange of control and she will sell the company sooner than\\nthat. Founders can get tripped up on this because they naturally\\nhave an emotional incentive to see the company succeed that\\nhired executives oaen do not share.\\nAnd of course, never get caught between a venture capitalist and\\nher incentives.\\nTwo: Liking/Loving Te',\n",
              "   0.5451651363116621),\n",
              "  (\"s.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and you’ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring executives 67\\nPart 9: How to hire a professional\\nCEO\\nDon’t.\\nIf you don’t have anyone on your founding team who is capable\\nof being CEO, then sell your company — now.\\nHow to hire the best people you've\\never worked with\\nThere are many aspects to hiring great people, and various people smarter than me have written extensively on the topic.\\nSo I’m not going to try to be comprehensive.\\nBut I am going to relay some lessons learned through hard\\nexperience on how to hire the best people you’ve ever worked\\nwith — particularly for a startup.\\nI’m going to cover two key areas in this post:\\n• Criteria: what to value when evaluating candidates.\\n• And process: how to actually run the hiring process, and if\\nnecessary the aaermath of making a mistake.\\nCriteria 7rst\\nLots of people will tell you to hire for intell\",\n",
              "   0.5368027728369661),\n",
              "  ('. Don’t micromanage.\\nThe whole point of having an executive is to have someone who\\n60 The Pmarca Blog Archives\\ncan Xgure out how to build and run an organization so that you\\ndon’t have to. Manage her, understand what she is doing, be\\nvery clear on the results you expect, but let her do the job.\\nHere’s the key corollary to that: if you want to give an executive full\\nlatitude, but you’re reluctant to do so because you’re not sure she can\\nmake it happen, then it’s probably time to Fre her.\\nIn my experience it’s not that uncommon for a founder or CEO\\nto be uncomfortable — sometimes only at a gut level — at really\\ngiving an executive the latitude to run with the ball. That is a\\nsureXre signal that the executive is not working out and probably needs to be Xred. More on that below.\\nThird, ruthlessly violate the chain of command in order to gather data.\\nI don’t mean going around telling people under an executive\\nwhat to do without her knowing about it. I mean, ask questions,\\ncontinually, at al',\n",
              "   0.5161142913162111)]}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"What are some tips for being an effective CEO?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAkIW2qRR6yz"
      },
      "source": [
        "Navigate to the Weights and Biases \"run\" link to see how your LLM is performing!\n",
        "\n",
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "username=\"Katerina Gawthorpe\"\n",
        "project_name=\"Visibility Example - AIE3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sInZ32BoR6yz"
      },
      "source": [
        "#### ❓ Question #5:\n",
        "\n",
        "What is the `model_name` from the WandB `root_span` trace?\n",
        "gpt-3.5-turbo-0125"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of8wJGalR6yz"
      },
      "source": [
        "## Task 7: RAG Evaluation Using GPT-4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "PRNzzMurR6yz",
        "outputId": "0dcaf542-530c-480d-9dc6-a2931e8557e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are several tips for being an effective CEO based on the provided context:\n",
            "\n",
            "1. **Prioritize Talent Management**: As a CEO, one crucial role is identifying and promoting top talent within your organization. Focus on finding strong, ambitious, and aggressive director- or VP-level managers and give them significant responsibilities and appropriate incentives. This helps ensure that your company is driven by motivated and capable leaders (The Pmarca Blog Archives, Part 8).\n",
            "\n",
            "2. **Maintain a Lean Executive Layer**: Keep your management structure lean. Ideally, you should have no more than one executive layer between you and your key managers once you have set up your teams. This helps in maintaining direct communication and quicker decision-making processes (The Pmarca Blog Archives, Part 8).\n",
            "\n",
            "3. **Focus on Key Objectives**: Identify the most critical objective for your company and assign your best person to spearhead this area. Ensuring the right people are managing the right tasks is essential for effective leadership and company success (The Pmarca Blog Archives, Part 8).\n",
            "\n",
            "4. **Use Transparency as a Motivational Tool**: Transparency within the organization can be an effective way to motivate employees. When performance metrics are clear and visible to all, it instills a sense of accountability and can motivate underperformers to improve their output or self-select out of the organization (The Pmarca Blog Archives, Part 8).\n",
            "\n",
            "5. **Strategic Hiring and Incentives for Executives**: When hiring executives, especially a CEO, align incentives such as stock options in a way that promotes long-term investment in the company. Be wary of incentive structures that might encourage premature sell-offs of the company (The Pmarca Blog Archives, Part 8).\n",
            "\n",
            "6. **Manage Without Micromanagement**: Provide clear goals and expectations for your executives, but allow them the freedom to achieve these results without micromanagement. This autonomy can lead to innovation and growth within the organization. If you find it hard to trust an executive's capability or judgement, it might be an indication that they are not the right fit (The Pmarca Blog Archives, Part 7).\n",
            "\n",
            "7. **Communicate Directly**: Routinely violate the chain of command not by undermining your executives, but by staying informed and involved. Ask questions and gather data directly from different levels of your organization to keep your finger on the pulse and make informed decisions (The Pmarca Blog Archives, Part 9).\n",
            "\n",
            "By implementing these practices, a CEO can effectively lead their organization towards success and sustainability.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'{\"clarity\" : \"9\", \"faithfulness\" : \"10\", \"correctness\" : \"10\"}'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What are some tips for being an effective CEO?\"\n",
        "\n",
        "response = retrieval_augmented_qa_pipeline.run_pipeline(query)\n",
        "\n",
        "print(response[\"response\"])\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response and the provided context\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Context: {context}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "try:\n",
        "    chat_openai = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
        "except:\n",
        "    chat_openai = ChatOpenAI()\n",
        "\n",
        "evaluator_system_prompt = SystemRolePrompt(evaluator_system_template)\n",
        "evaluation_prompt = UserRolePrompt(evaluation_template)\n",
        "\n",
        "messages = [\n",
        "    evaluator_system_prompt.create_message(format=False),\n",
        "    evaluation_prompt.create_message(\n",
        "        input=query,\n",
        "        context=\"\\n\".join([context[0] for context in response[\"context\"]]),\n",
        "        response=response[\"response\"]\n",
        "    ),\n",
        "]\n",
        "\n",
        "chat_openai.run(messages, response_format={\"type\" : \"json_object\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNpauQmJR6yz"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we've gone through the steps required to create your own simple RAQA application!\n",
        "\n",
        "Please feel free to extend this as much as you'd like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "bb904e05ece143c79ecc4f20de482f45",
            "3a4ba348cb004f8ab7b2b1395539c81b",
            "1ce393d9afcf427d9d352259c5d32678",
            "56a8e24025594e5e9ff3b8581c344691",
            "d2ea5009dd16442cb5d8a0ac468e50a8",
            "5f00135fe1044051a50ee5e841cbb8e3",
            "4e6efd99f7d346e485b002fb0fa85cc7",
            "3dfb67c39958461da6071e4c19c3fa41"
          ]
        },
        "id": "xzlxJbFtR6y0",
        "outputId": "d5789d16-c41c-4a3c-ac53-65640a0d3698"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">silver-hill-5</strong> at: <a href='https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3/runs/psvalzlt' target=\"_blank\">https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3/runs/psvalzlt</a><br/> View project at: <a href='https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/cryptoforecast/Visibility%20Example%20-%20AIE3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_124632-psvalzlt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "buildyourownlangchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce393d9afcf427d9d352259c5d32678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
            "value": 1
          }
        },
        "3a4ba348cb004f8ab7b2b1395539c81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
            "placeholder": "​",
            "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "3dfb67c39958461da6071e4c19c3fa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6efd99f7d346e485b002fb0fa85cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8e24025594e5e9ff3b8581c344691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f00135fe1044051a50ee5e841cbb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb904e05ece143c79ecc4f20de482f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
              "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
            ],
            "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
          }
        },
        "d2ea5009dd16442cb5d8a0ac468e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
